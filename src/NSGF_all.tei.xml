<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,207.94,89.99,196.13,12.90">Neural Sinkhorn Gradient Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-01-25">25 Jan 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,87.76,129.81,78.74,10.75"><forename type="first">Huminhao</forename><surname>Zhu</surname></persName>
							<email>zhuhuminhao@zju.edu.cn</email>
							<affiliation key="aff0" coords="1,147.80,147.82,321.14,10.37">
								<note type="raw_affiliation"><label>1</label> College of Computer Science and Technology , Zhejiang University</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,182.19,129.81,92.93,10.75"><forename type="first">Fangyikang</forename><surname>Wang</surname></persName>
							<email>wangfangyikang@zju.edu.cn</email>
							<affiliation key="aff0" coords="1,147.80,147.82,321.14,10.37">
								<note type="raw_affiliation"><label>1</label> College of Computer Science and Technology , Zhejiang University</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.80,129.81,63.45,10.75"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0" coords="1,147.80,147.82,321.14,10.37">
								<note type="raw_affiliation"><label>1</label> College of Computer Science and Technology , Zhejiang University</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.94,129.81,68.11,10.75"><forename type="first">Hanbin</forename><surname>Zhao</surname></persName>
							<email>zhaohanbin@zju.edu.cn</email>
							<affiliation key="aff0" coords="1,147.80,147.82,321.14,10.37">
								<note type="raw_affiliation"><label>1</label> College of Computer Science and Technology , Zhejiang University</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,472.00,129.81,47.51,10.75"><forename type="first">Hui</forename><surname>Qian</surname></persName>
							<email>qianhui@zju.edu.cn</email>
							<affiliation key="aff0" coords="1,147.80,147.82,321.14,10.37">
								<note type="raw_affiliation"><label>1</label> College of Computer Science and Technology , Zhejiang University</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coords="1,207.94,89.99,196.13,12.90">Neural Sinkhorn Gradient Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-01-25">25 Jan 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">2D9A011F70B7E2C0E7E66A71E32CEF87</idno>
					<idno type="arXiv">arXiv:2401.14069v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-09-04T04:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,73.93,236.90,203.15,8.64;1,73.93,247.86,203.15,8.64;1,73.93,258.82,111.93,8.64">Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature.</s><s coords="1,192.09,258.82,84.98,8.64;1,73.93,269.78,203.15,8.64;1,73.93,280.74,203.15,8.64;1,73.93,291.70,203.15,8.64">Recently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures.</s><s coords="1,73.93,302.66,203.15,8.64;1,73.93,313.62,203.15,8.64;1,73.93,324.57,203.15,8.64;1,73.93,335.53,203.15,8.64;1,73.93,346.49,203.15,8.64;1,73.93,357.45,17.99,8.64">In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.</s><s coords="1,97.91,357.45,179.17,8.64;1,73.93,368.41,203.15,8.64;1,73.93,379.37,203.15,8.64;1,73.93,390.33,166.75,8.64">We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation.</s><s coords="1,243.73,390.33,33.34,8.64;1,73.93,401.29,203.15,8.64;1,73.93,412.25,203.15,8.64;1,73.93,423.20,203.15,8.64;1,73.93,434.16,54.85,8.64">Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field.</s><s coords="1,132.49,434.16,144.59,8.64;1,73.93,445.12,203.15,8.64;1,73.93,456.08,203.15,8.64;1,73.93,466.72,203.15,8.96;1,73.93,478.00,203.15,8.64;1,73.93,488.96,66.54,8.64">To further enhance model efficiency on high-dimensional tasks, a two-phase NSGF++ model is devised, which first follows the Sinkhorn flow to approach the image manifold quickly (≤ 5 NFEs) and then refines the samples along a simple straight flow.</s><s coords="1,143.84,488.96,133.23,8.64;1,73.93,499.92,203.15,8.64;1,73.93,510.88,203.15,8.64;1,73.93,521.83,134.91,8.64">Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed methods.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" coords="1,54.00,547.60,77.51,10.75">Introdution</head><p><s coords="1,54.00,564.01,243.00,8.64;1,54.00,574.65,243.00,8.96;1,54.00,585.93,243.00,8.64;1,54.00,596.88,216.11,8.64">The Wasserstein Gradient Flow (WGF) with respect to certain specific functional objective F (denoted as F Wasserstein gradient flow) is a powerful tool for solving optimization problems over the Wasserstein probability space.</s><s coords="1,274.86,596.88,22.14,8.64;1,54.00,606.95,243.00,9.53;1,54.00,618.80,243.00,8.64;1,54.00,629.76,243.00,8.64;1,54.00,640.72,19.56,8.64">Since the seminal work of <ref type="bibr" coords="1,139.42,606.95,78.80,9.53" target="#b62">[Jordan et al., 1998</ref>] which shows that the Fokker-Plank equation is the Wasserstein gradient flow with respect to the free energy, Wasserstein gradient flow w.r.t.</s><s coords="1,78.18,640.72,218.82,8.64;1,54.00,650.78,243.00,9.53;1,54.00,661.74,243.00,9.53;1,54.00,672.70,198.89,9.53">different functionals have been widely used in various machine learning tasks such as Bayesian inference <ref type="bibr" coords="1,258.46,650.78,38.54,9.30;1,54.00,662.46,40.83,8.82">[Zhang et al., 2021a]</ref>, reinforcement learning <ref type="bibr" coords="1,194.31,661.74,81.66,9.53">[Zhang et al., 2021b]</ref>, and mean-field games <ref type="bibr" coords="1,127.00,672.70,121.61,9.53" target="#b120">[Zhang and Katsoulakis, 2023]</ref>.</s></p><p><s coords="1,63.96,684.56,233.04,8.64;1,54.00,695.51,243.00,8.64;1,315.00,218.58,242.99,9.53;1,315.00,230.26,243.00,8.82;1,315.00,241.22,109.11,8.82">One recent trend in the Wasserstein gradient flow literature is to develop efficient generative modeling methods <ref type="bibr" coords="1,315.00,218.58,76.83,9.53" target="#b42">[Gao et al., 2019;</ref><ref type="bibr" coords="1,396.42,219.30,73.51,8.82" target="#b44">Gao et al., 2022;</ref><ref type="bibr" coords="1,474.52,219.30,83.47,8.82" target="#b10">Ansari et al., 2021;</ref><ref type="bibr" coords="1,315.00,230.26,86.45,8.82" target="#b81">Mokrov et al., 2021;</ref><ref type="bibr" coords="1,405.31,230.26,111.79,8.82" target="#b4">Alvarez-Melis et al., 2022;</ref><ref type="bibr" coords="1,520.95,230.26,37.05,8.82;1,315.00,241.22,38.10,8.82" target="#b14">Bunne et al., 2022;</ref><ref type="bibr" coords="1,355.73,241.22,64.10,8.82" target="#b32">Fan et al., 2022]</ref>.</s><s coords="1,427.63,241.39,130.37,8.64;1,315.00,252.35,243.00,8.64;1,315.00,263.31,243.00,8.64;1,315.00,274.27,63.69,8.64">In general, these methods mimic the Wasserstein gradient flow with respect to a specific distribution metric, driving a source distribution towards a target distribution.</s><s coords="1,384.12,274.27,173.88,8.64;1,315.00,285.23,243.00,8.64;1,315.00,296.19,243.00,8.64;1,315.00,307.15,108.31,8.64">Neural networks are typically employed to approximate the computationally challenging components of the underlying Wasserstein gradient flow such as the timedependent transport maps.</s><s coords="1,431.57,307.15,126.42,8.64;1,315.00,318.11,243.00,8.64;1,315.00,329.07,63.14,8.64">During the training process of these methods, it is common to require samples from the target distribution.</s><s coords="1,381.95,329.07,176.05,8.64;1,315.00,340.02,243.00,8.64;1,315.00,350.98,243.00,8.64;1,315.00,361.94,243.00,8.64;1,315.00,372.90,149.16,8.64">After the training process, an inference procedure is often employed to generate new samples from the target distribution This procedure involves iteratively transporting samples from the source distribution with the assistance of the trained neural network.</s><s coords="1,471.81,372.90,86.19,8.64;1,315.00,383.86,243.00,8.64">Based on the chosen metric, these methods can be categorized into two main types.</s></p><p><s coords="1,324.96,396.83,233.03,8.96;1,315.00,407.79,41.25,8.96">Divergences Between Distributions With Exact Same Supports.</s><s coords="1,363.69,408.18,194.31,8.64;1,315.00,419.14,243.00,8.64;1,315.00,430.10,113.90,8.64">The first class of widely used metrics is the fdivergence, such as the Kullback-Leibler divergence and the Jensen-Shannon divergence.</s><s coords="1,435.11,430.10,122.89,8.64;1,315.00,441.06,243.00,8.64;1,315.00,452.02,243.00,8.64;1,315.00,462.98,103.71,8.64">These divergences are defined based on the density ratio between two distributions and are only well-defined when dealing with distributions that have exactly the same support.</s><s coords="1,424.39,462.98,133.61,8.64;1,315.00,473.93,243.00,8.64;1,315.00,484.89,243.00,8.64;1,315.00,494.96,243.00,9.53;1,315.00,505.91,80.32,9.53">Within the scope of f-divergence Wasserstein gradient flow generative models, neural networks are commonly utilized to formulate density-ratio estimators, as demonstrated by <ref type="bibr" coords="1,393.57,494.96,69.21,9.53" target="#b42">[Gao et al., 2019;</ref><ref type="bibr" coords="1,464.99,495.67,76.41,8.82" target="#b10">Ansari et al., 2021]</ref> and <ref type="bibr" coords="1,315.00,505.91,76.03,9.53" target="#b54">[Heng et al., 2022]</ref>.</s><s coords="1,400.53,506.81,157.47,8.64;1,315.00,517.77,243.00,8.64;1,315.00,528.73,243.00,8.64;1,315.00,539.69,243.00,8.64;1,315.00,549.75,201.80,9.53">However, as one can only access finite samples from target distributions in the training process, the support shift between the sample collections from the compared distributions may cause significant approximation error in the density-ratio estimators <ref type="bibr" coords="1,438.83,549.75,69.38,9.53" target="#b18">[Choi et al., 2022</ref>].</s><s coords="1,521.86,550.65,36.14,8.64;1,315.00,560.71,243.00,9.53;1,315.00,572.56,243.00,8.64;1,315.00,583.52,77.66,8.64">An alternative approach, proposed by <ref type="bibr" coords="1,435.04,560.71,67.44,9.53" target="#b32">[Fan et al., 2022]</ref>, circumvents these limitations by employing a dual variational formulation of the f-divergence.</s><s coords="1,395.74,583.52,162.26,8.64;1,315.00,594.48,243.00,8.64;1,315.00,605.44,76.50,8.64">In this framework, two networks are employed to approximate the optimal variational function and the transport maps.</s><s coords="1,395.21,605.44,162.79,8.64;1,315.00,616.40,36.70,8.64">These two components are optimized alternately.</s><s coords="1,354.76,616.40,203.24,8.64;1,315.00,627.36,243.00,8.64;1,315.00,637.42,234.36,9.53">It's imperative to highlight that the non-convex and non-concave characteristics of their min-max objective can render the training inherently unstable <ref type="bibr" coords="1,469.67,637.42,75.40,9.53" target="#b60">[Hsieh et al., 2021]</ref>.</s></p><p><s coords="1,324.96,651.29,233.03,8.96;1,315.00,662.25,70.80,8.96">Divergences Between Distributions With Possible Different Supports.</s><s coords="1,394.29,662.64,163.71,8.64;1,315.00,673.60,243.00,8.64;1,315.00,684.56,243.00,8.64">Another type of generative Wasserstein gradient flow model employs divergences that are welldefined for distributions with possible different supports.</s><s coords="1,315.00,694.62,243.00,9.53;2,54.00,203.61,243.00,8.82;2,54.00,213.85,242.99,9.53;2,54.00,225.70,243.00,8.64;2,54.00,235.77,154.49,9.53">This includes free energy fuctionals <ref type="bibr" coords="1,467.69,694.62,85.77,9.53" target="#b81">[Mokrov et al., 2021</ref>;  <ref type="bibr" coords="2,54.00,203.61,80.97,8.82" target="#b14">Bunne et al., 2022]</ref>, the kernel-based metrics such as the Maximum-Mean/Sobolev Discrepancy <ref type="bibr" coords="2,211.60,213.85,85.40,9.53" target="#b85">[Mroueh et al., 2019;</ref><ref type="bibr" coords="2,54.00,225.70,110.86,8.64" target="#b83">Mroueh and Rigotti, 2020]</ref> and sliced-Wasserstein distance <ref type="bibr" coords="2,54.00,235.77,85.09,9.53" target="#b75">[Liutkus et al., 2019;</ref><ref type="bibr" coords="2,141.96,236.48,62.24,8.82" target="#b30">Du et al., 2023]</ref>.</s><s coords="2,212.72,236.66,84.28,8.64;2,54.00,247.62,243.00,8.64;2,54.00,258.58,243.00,8.64;2,54.00,269.54,127.07,8.64">As these divergences can be efficiently approximated with samples, neural networks are typically used to directly model the transport maps used in the inference procedure.</s><s coords="2,184.12,269.54,112.88,8.64;2,54.00,279.60,243.00,9.53;2,54.00,291.46,243.00,8.64">In Wasserstein gradient flow methods, input convex neural networks (ICNNs, <ref type="bibr" coords="2,246.06,279.60,50.94,9.30;2,54.00,291.46,22.13,8.64" target="#b8">[Amos et al., 2017]</ref>) are commonly used to approximate the transport map.</s><s coords="2,54.00,301.52,243.00,9.53;2,54.00,313.37,243.00,8.64;2,54.00,324.33,243.00,8.64;2,54.00,335.29,50.64,8.64">However, recently, several works <ref type="bibr" coords="2,191.54,301.52,87.22,9.53" target="#b67">[Korotin et al., 2021]</ref> discuss the poor expressiveness of ICNNs architecture and show that it would result in poor performance in high-dimension applications.</s><s coords="2,108.38,335.29,188.62,8.64;2,54.00,346.25,243.00,8.64;2,54.00,357.21,243.00,8.64;2,54.00,367.27,243.00,9.53;2,54.00,379.13,243.00,8.64;2,54.00,389.19,149.16,9.53">Besides, the Maximum-Mean/Sobolev discrepancy Wasserstein gradient flow models are usually hard to train and are easy to trapped in poor local optima in practice <ref type="bibr" coords="2,71.65,367.27,77.36,9.53" target="#b12">[Arbel et al., 2019]</ref>, since the kernel-based divergences are highly sensitive to the parameters of the kernel function <ref type="bibr" coords="2,54.00,389.19,65.86,9.53" target="#b69">[Li et al., 2017;</ref><ref type="bibr" coords="2,123.44,389.91,75.43,8.82" target="#b118">Wang et al., 2018]</ref>.</s><s coords="2,209.56,389.19,87.45,9.53;2,54.00,400.87,243.00,8.82;2,54.00,412.00,243.00,8.64;2,54.00,422.96,243.00,8.64;2,54.00,433.92,236.22,8.64"><ref type="bibr" coords="2,209.56,389.19,87.45,9.53" target="#b75">[Liutkus et al., 2019;</ref><ref type="bibr" coords="2,54.00,400.87,66.72,8.82" target="#b30">Du et al., 2023]</ref> consider sliced-Wasserstein WGF to build nonparametric generative Models which do not achieve high generation quality, it is an interesting work on how to combine sliced-Wasserstein WGF and neural network methods.</s></p><p><s coords="2,63.96,454.03,58.20,8.96">Contribution.</s><s coords="2,129.11,454.42,167.89,8.64;2,54.00,465.38,243.00,8.64;2,54.00,476.34,243.00,8.64;2,54.00,487.30,184.90,8.64">In this paper, we investigate the Wasserstein gradient flow with respect to the Sinkhorn divergence, which is categorized under the second type of divergence and does not necessitate any kernel functions.</s><s coords="2,243.38,487.30,53.62,8.64;2,54.00,498.08,243.00,8.82;2,54.00,509.21,243.00,8.64;2,54.00,520.17,243.00,8.64;2,54.00,531.13,17.99,8.64">We introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Sinkhorn Wasserstein gradient flow from a specified source distribution.</s><s coords="2,77.73,531.13,219.27,8.64;2,54.00,542.09,243.00,8.64;2,54.00,553.05,201.79,8.64">The NSGF employs a velocity field matching scheme that demands only samples from the target distribution to calculate empirical velocity field approximations.</s><s coords="2,262.37,553.05,34.63,8.64;2,54.00,564.01,243.00,8.64;2,54.00,574.97,243.00,8.64;2,54.00,585.93,243.00,8.64;2,54.00,596.88,108.36,8.64">Our theoretical analyses show that as the sample size approaches infinity, the mean-field limit of the empirical approximation converges to the true velocity field of the Sinkhorn Wasserstein gradient flow.</s><s coords="2,171.57,596.88,125.43,8.64;2,54.00,607.84,243.00,8.64;2,54.00,618.80,243.00,8.64;2,54.00,629.76,243.00,8.64;2,54.00,640.72,61.61,8.64">Given distinct source and target data samples, our NSGF can be harnessed across a wide range of machine learning applications, including unconditional/conditional image generation, style transfer, and audiotext translation.</s><s coords="2,118.62,640.72,178.38,8.64;2,54.00,651.68,243.00,8.64;2,54.00,662.64,243.00,8.64;2,54.00,673.28,243.00,8.96;2,54.00,684.56,136.55,8.64">To further enhance model efficiency on highdimensional image datasets, a two-phase NSGF++ model is devised, which first follows the Sinkhorn flow to approach the image manifold quickly (≤ 5 NFEs) and then refine the samples along a simple straight flow.</s><s coords="2,197.06,684.56,99.94,8.64;2,54.00,695.51,243.00,8.64">A novel phase-transition time predictor is proposed to transfer between the two phases.</s></p><p><s coords="2,315.00,203.79,243.00,8.64;2,315.00,214.74,243.00,8.64">We empirically validate NSGF on low-dimensional 2D data and NSGF++ on benchmark images (MNIST, CIFAR-10).</s></p><p><s coords="2,315.00,225.70,243.00,8.64;2,315.00,236.66,243.00,8.64;2,315.00,247.62,243.00,8.64;2,315.00,258.58,243.00,8.64;2,315.00,269.54,91.06,8.64">Our findings indicate that our models can be trained to yield commendable results in terms of generation cost and sample quality, surpassing the performance of the neural Wasserstein gradient flow methods previously tested on CIFAR-10, to the best of our knowledge.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="2,315.00,293.46,93.74,10.75">Related Works</head><p><s coords="2,315.00,311.12,184.63,8.96">Sinkhorn Divergence in Machine Learning.</s><s coords="2,503.29,311.51,54.70,8.64;2,315.00,322.47,243.00,8.64;2,315.00,333.43,243.00,8.64;2,315.00,343.49,243.00,9.53;2,315.00,355.16,159.49,8.82">Originally introduced in the domain of optimal transport, the Sinkhorn divergence emerged as a more computationally tractable alternative to the classical Wasserstein distance <ref type="bibr" coords="2,500.94,343.49,57.05,9.53">[Cuturi, 2013;</ref><ref type="bibr" coords="2,315.00,355.16,75.43,8.82" target="#b91">Peyré et al., 2017;</ref><ref type="bibr" coords="2,393.79,355.16,76.41,8.82" target="#b38">Feydy et al., 2019]</ref>.</s><s coords="2,480.17,355.34,77.84,8.64;2,315.00,366.30,243.00,8.64;2,315.00,376.36,243.00,9.53;2,315.00,388.04,243.00,8.82;2,315.00,398.28,243.00,9.53;2,315.00,409.96,51.47,8.82">Since its inception, Sinkhorn divergence has found applications across a range of machine learning tasks, including domain adaptation <ref type="bibr" coords="2,530.89,376.36,27.11,8.64;2,315.00,388.04,52.89,8.82" target="#b0">[Alaya et al., 2019;</ref><ref type="bibr" coords="2,372.75,388.04,92.11,8.82" target="#b65">Komatsu et al., 2021]</ref>, Sinkhorn barycenter <ref type="bibr" coords="2,315.00,398.28,76.47,9.53" target="#b77">[Luise et al., 2019;</ref><ref type="bibr" coords="2,394.24,399.00,71.48,8.82" target="#b104">Shen et al., 2020]</ref> and color transfer <ref type="bibr" coords="2,542.10,398.28,15.90,8.64;2,315.00,409.96,47.18,8.82" target="#b87">[Pai et al., 2021]</ref>.</s><s coords="2,370.66,410.14,187.34,8.64;2,315.00,421.10,243.00,8.64;2,315.00,431.16,243.00,9.53;2,315.00,442.84,80.58,8.82">Indeed, it has already been extended to singlestep generative modeling methods, such as the Sinkhorn GAN and VAE <ref type="bibr" coords="2,383.48,431.16,94.74,9.53" target="#b48">[Genevay et al., 2018;</ref><ref type="bibr" coords="2,482.68,431.88,75.32,8.82" target="#b27">Deja et al., 2020;</ref><ref type="bibr" coords="2,315.00,442.84,76.29,8.82" target="#b89">Patrini et al., 2020]</ref>.</s><s coords="2,399.61,443.01,158.39,8.64;2,315.00,453.97,243.00,8.64;2,315.00,464.93,136.20,8.64">However, to the best of our knowledge, it has yet to be employed in developing efficient generative Wasserstein gradient flow models.</s></p><p><s coords="2,324.96,475.95,187.79,8.96">Neural ODE/SDE Based Diffusion Models.</s><s coords="2,520.74,476.34,37.26,8.64;2,315.00,487.30,243.00,8.64;2,315.00,498.25,243.00,8.64;2,315.00,509.21,243.00,8.64;2,315.00,519.28,243.00,9.53;2,315.00,530.95,75.73,8.82">Recently, diffusion models, as a class of Neural ODE/SDE Based generative methods have achieved unprecedented success, which also transforms a simple density to the target distribution, iteratively <ref type="bibr" coords="2,389.48,519.28,100.47,9.53" target="#b108">[Song and Ermon, 2019;</ref><ref type="bibr" coords="2,493.22,519.99,64.78,8.82" target="#b58">Ho et al., 2020;</ref><ref type="bibr" coords="2,315.00,530.95,71.45,8.82" target="#b110">Song et al., 2021]</ref>.</s><s coords="2,395.79,531.13,162.21,8.64;2,315.00,542.09,243.00,8.64;2,315.00,553.05,243.00,8.64;2,315.00,564.01,118.29,8.64">Typically, each step of diffusion models only progresses a little by denoising a simple Gaussian noise, while each step in WGF models follows the most informative direction (in a certain sense).</s><s coords="2,438.56,564.01,119.44,8.64;2,315.00,574.97,142.51,8.64">Hence, diffusion models usually have a long inference trajectory.</s><s coords="2,460.43,574.97,97.56,8.64;2,315.00,585.93,243.00,8.64;2,315.00,596.88,169.71,8.64">In recent research undertakings, there has been a growing interest in exploring more informative steps within diffusion models.</s><s coords="2,488.76,596.88,69.24,8.64;2,315.00,606.95,243.00,9.53;2,315.00,618.80,243.00,8.64;2,315.00,629.76,243.00,8.64;2,315.00,640.72,243.00,8.64;2,315.00,651.68,91.59,8.64">Specifically, flow matching methods <ref type="bibr" coords="2,390.94,606.95,84.58,9.53" target="#b71">[Lipman et al., 2023;</ref><ref type="bibr" coords="2,478.06,607.66,64.12,8.82" target="#b73">Liu et al., 2023;</ref><ref type="bibr" coords="2,544.72,607.84,8.85,8.64;2,315.00,618.80,138.22,8.64" target="#b2">Albergo and Vanden-Eijnden, 2023]</ref> establish correspondence between the source and target via optimal transport, subsequently crafting a probability path by directly linking data points from both ends.</s><s coords="2,411.60,651.68,146.40,8.64;2,315.00,662.64,243.00,8.64;2,315.00,673.60,41.80,8.64">Notably, when the source and target are both Gaussians, their path is actually a Wasserstein gradient flow.</s><s coords="2,360.14,673.60,197.86,8.64;2,315.00,684.56,118.85,8.64">However, this property does not consistently hold for general data probabilities.</s><s coords="2,438.48,683.66,119.52,9.53;2,315.00,695.34,243.00,8.82;3,54.00,57.48,206.47,8.64">Moreover, <ref type="bibr" coords="2,482.69,683.66,75.31,9.53" target="#b114">[Tong et al., 2023;</ref><ref type="bibr" coords="2,315.00,695.34,89.51,8.82" target="#b95">Pooladian et al., 2023]</ref> consider calculating the minibatch op-timal transport map to guide data points connecting.</s><s coords="3,263.51,57.48,33.48,8.64;3,54.00,67.54,243.00,9.53;3,54.00,78.50,243.00,9.53;3,54.00,90.35,243.00,8.64;3,54.00,101.31,125.86,8.64">Besides, <ref type="bibr" coords="3,54.00,67.54,68.61,9.53" target="#b25">[Das et al., 2023]</ref> consider the shortest forward diffusion path for the Fisher metric and <ref type="bibr" coords="3,154.19,78.50,76.20,9.53" target="#b102">[Shaul et al., 2023]</ref> explore the conditional Gaussian probability path based on the principle of minimizing the Kinetic Energy.</s><s coords="3,183.45,101.31,113.54,8.64;3,54.00,112.27,243.00,8.64;3,54.00,123.23,243.00,8.64;3,54.00,134.19,243.00,8.64;3,54.00,145.15,83.72,8.64">Nonetheless, a commonality among many of these methods is their reliance on Gaussian paths for theoretical substantiation, thereby constraining the broader applicability of these techniques within real-world generative modeling.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="3,54.00,166.25,86.78,10.75">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" coords="3,54.00,182.39,69.39,9.81">Notations</head><p><s coords="3,54.00,196.50,142.21,9.68;3,196.21,194.96,4.15,6.12;3,204.03,196.53,49.11,9.30;3,253.13,194.96,4.15,6.12;3,260.95,196.85,36.05,8.64;3,54.00,207.81,135.60,8.98;3,189.60,205.92,4.15,6.12;3,194.25,207.81,54.61,8.64">We denote x = (x 1 , • • • , x d ) ∈ R d and X ⊂ R d as a vector and a compact ground set in R d , respectively.</s><s coords="3,252.08,207.81,44.92,8.64;3,54.00,221.83,98.51,9.68;3,163.03,226.89,2.82,6.12;3,168.01,221.86,5.69,8.74;3,173.70,219.11,4.11,6.12;3,173.70,226.68,2.82,6.12;3,178.31,221.86,3.87,8.74">For a given point x ∈ X , ∥x∥ p := ( i x p i )</s></p><p><s coords="3,183.52,217.27,3.39,4.35;3,183.38,222.33,3.68,4.37;3,191.47,221.86,105.53,8.96;3,54.00,232.82,243.00,9.65;3,54.00,243.75,91.96,8.99">1 p denotes the p-norm on euclidean space, and δ x stands for the Dirac (unit mass) distribution at point x ∈ X .</s><s coords="3,149.83,243.78,147.18,9.65;3,54.00,254.74,243.00,8.96;3,54.00,265.69,165.47,8.96">P 2 (X ) denotes the set of probability measures on X with finite second moment and C(X ) denotes the space of continuous functions on X .</s><s coords="3,225.61,266.01,71.39,8.64;3,54.00,278.91,109.56,9.65;3,167.92,276.12,24.66,6.12;3,175.89,284.39,8.72,6.12;3,193.77,278.91,28.00,9.30;3,221.77,277.34,4.15,6.12;3,230.18,278.91,66.82,9.30;3,54.00,292.17,102.88,9.65">For a given functional F(•) : P 2 (X ) → R, δF (µt)  δµ (•) : R d → R denotes its first variation at µ = µ t .</s><s coords="3,162.77,292.17,134.23,8.96;3,54.00,303.45,243.00,8.64">Besides, we use ∇ and ∇ • () to denote the gradient and the divergence operator, respectively.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" coords="3,54.00,321.28,240.33,9.81">Wasserstein distance and Sinkhorn divergence</head><p><s coords="3,54.00,335.75,243.00,8.64">We first introduce the background of Wasserstein distance.</s><s coords="3,54.00,346.39,243.00,9.65;3,54.00,357.35,242.50,9.65;3,54.00,368.63,52.02,8.64">Given two probability measures µ, ν ∈ P 2 (X ), the p-Wasserstein distance W p (µ, ν) : P 2 (X ) × P 2 (X ) → R + is defined as:</s></p><formula xml:id="formula_0" coords="3,60.68,380.40,236.32,28.48">W p (µ, ν) = inf π∈Π(µ,ν) X ×X ∥x -y∥ p dπ(x, y) 1 p ,<label>(1)</label></formula><p><s coords="3,54.00,414.37,242.64,8.96;3,54.00,425.33,95.36,8.96">where Π(µ, ν) denotes the set of all probability couplings π with marginals µ and ν.</s><s coords="3,152.55,425.33,144.46,9.65;3,54.00,436.26,204.89,8.99;3,258.89,433.32,4.11,6.12;3,265.66,436.61,31.34,8.64;3,54.00,447.25,140.04,8.96">The W p distance aims to find a coupling π so as to minimize the cost function ∥x -y∥ p of moving a probability mass from µ to ν.</s><s coords="3,197.40,447.57,99.60,8.64;3,54.00,458.21,243.00,9.65;3,54.00,469.17,242.99,9.65;3,54.00,479.55,142.45,9.53">It has been demonstrated that the p-Wasserstein distance is a valid metric on P 2 (X ), and (P 2 (X ), W p ) is referred to as the Wasserstein probability space <ref type="bibr" coords="3,91.63,479.55,100.54,9.53" target="#b116">[Villani and others, 2009]</ref>.</s></p><p><s coords="3,63.96,491.09,233.04,9.65;3,54.00,501.47,243.00,9.53;3,54.00,513.32,74.77,8.64">Note that directly calculating W p is computationally expensive, especially for high dimensional problems <ref type="bibr" coords="3,275.42,501.47,17.26,8.64;3,54.00,513.32,70.48,8.64">[Santambrogio, 2015]</ref>.</s><s coords="3,139.30,513.32,157.70,8.64;3,54.00,523.38,243.00,9.53;3,54.00,535.24,243.00,8.64;3,54.00,546.20,85.96,8.64;3,54.00,559.71,53.05,8.96">Consequently, the entropy-regularized Wasserstein distance <ref type="bibr" coords="3,140.73,523.38,58.11,9.53">[Cuturi, 2013]</ref> is proposed to approximate equation equation 1 by regularizing the original problem with an entropy term: Definition 1.</s><s coords="3,112.71,559.92,184.29,8.59;3,54.00,570.88,88.83,8.59">The entropy-regularized Wasserstein distance is formally defined as:</s></p><formula xml:id="formula_1" coords="3,58.30,585.80,69.07,38.57">Wp,ε(µ, ν) = inf π∈Π(µ,ν) X ×X</formula><p><s coords="3,130.28,608.84,32.35,7.89;3,162.63,606.60,3.84,5.24;3,168.51,608.84,33.85,7.89">∥x -y∥ p dπ(x, y)</s></p><formula xml:id="formula_2" coords="3,210.34,598.72,86.66,36.53">1 p + εKL(π|µ ⊗ ν) ,<label>(2)</label></formula><p><s coords="3,54.00,637.46,243.00,8.74;3,54.00,648.39,243.00,8.77;3,54.00,659.38,243.00,8.74">where ε &gt; 0 is a regularization coefficient, µ ⊗ ν denotes the product measure, i.e., µ ⊗ ν(x, y) = µ(x)ν(y), and KL(π|µ⊗ν) denotes the KL-divergence between π and µ⊗ν.</s></p><p><s coords="3,63.96,673.28,233.04,9.65;3,54.00,684.24,243.00,9.65;3,54.00,694.62,99.59,9.53">Generally, the computational cost of W p,ε is much lower than W p , and can be efficiently calculated with Sinkhorn algorithms <ref type="bibr" coords="3,92.89,694.62,56.42,9.53">[Cuturi, 2013]</ref>.</s><s coords="3,159.58,695.51,137.42,8.64;3,315.00,57.16,243.00,9.65;3,315.00,68.44,65.72,8.64">Without loss of generality, we fix p = 2 and abbreviate W 2,ε := W ε for ease of notion in the whole paper.</s><s coords="3,384.03,68.44,173.97,8.64;3,315.00,79.08,243.00,9.65;3,315.00,90.35,243.00,8.64;3,315.00,100.42,79.26,9.53">According to Fenchel-Rockafellar theorem, the entropy-regularized Wasserstein problem W ε equation 2 has an equivalent dual formulation, which is given as follows <ref type="bibr" coords="3,315.00,100.42,74.93,9.53" target="#b91">[Peyré et al., 2017]</ref>:</s></p><formula xml:id="formula_3" coords="3,325.82,117.36,232.18,43.63">W ε (µ, ν) = max f,g∈C(X ) ⟨µ, f ⟩ + ⟨ν, g⟩ -ε µ ⊗ ν, exp 1 ε (f ⊕ g -C) -1 ,<label>(3)</label></formula><p><s coords="3,315.00,168.35,243.00,8.96;3,315.00,179.31,95.99,8.96;3,412.45,177.73,3.97,6.12;3,420.87,179.31,67.02,8.74">where C is the cost function in equation 2 and f ⊕ g is the tensor sum: (x, y) ∈ X 2 → f (x) + g(y).</s><s coords="3,492.90,179.63,65.11,8.64;3,315.00,190.27,243.00,9.65;3,315.00,201.22,40.33,9.65">The maximizers f µ,ν and g µ,ν of equation 3 are called the W ε -potentials of W ε (µ, ν).</s><s coords="3,359.35,201.54,198.64,8.64;3,315.00,212.18,105.70,9.65;3,315.00,225.82,243.00,10.39;3,315.00,237.51,243.00,9.65;3,315.00,248.61,52.67,8.59">The following lemma states the optimality condition for the W ε -potentials: Lemma 1. (Optimality <ref type="bibr" coords="3,414.78,225.82,59.24,9.47">[Cuturi, 2013]</ref>) The W ε -potentials (f µ,ν , g µ,ν ) exist and are unique (µ, ν)-a.e. up to an additive constant (i.e.</s><s coords="3,373.99,248.47,184.01,9.65">∀K ∈ R, (f µ,ν + K, g µ,ν -K) is optimal).</s><s coords="3,315.00,259.57,40.20,8.59">Moreover,</s></p><formula xml:id="formula_4" coords="3,368.81,275.94,185.32,9.65">W ε (µ, ν) = ⟨µ, f µ,ν ⟩ + ⟨ν, g µ,ν ⟩. (<label>4</label></formula><formula xml:id="formula_5" coords="3,554.13,276.25,3.87,8.64">)</formula><p><s coords="3,324.96,292.76,233.03,8.64;3,315.00,303.72,19.65,8.64">We describe such the method in Appendix A for completeness.</s><s coords="3,337.67,303.72,220.32,8.64;3,315.00,314.36,243.00,9.65;3,315.00,325.32,243.00,9.65;3,315.00,336.27,158.51,9.65">Note that, although computationally more efficient than the W p distance, the W ε distance is not a true metric, as there exists µ ∈ P 2 (X ) such that W ε (µ, µ) ̸ = 0 when ε ̸ = 0, which restricts the applicability of W ε .</s><s coords="3,478.57,336.59,79.43,8.64;3,315.00,347.23,243.00,9.65;3,315.00,357.62,127.41,9.53;3,315.00,372.49,142.04,8.96">As a result, the following Sinkhorn divergence S ε (µ, ν) : P 2 (X )×P 2 (X ) → R is proposed <ref type="bibr" coords="3,363.15,357.62,74.93,9.53" target="#b91">[Peyré et al., 2017]</ref>: Definition 2. Sinkhorn divergence:</s></p><formula xml:id="formula_6" coords="3,327.77,387.58,230.23,22.31">S ε (µ, ν) = W ε (µ, ν) - 1 2 (W ε (µ, µ) + W ε (ν, ν)) .<label>(5)</label></formula><p><s coords="3,324.96,415.17,233.04,9.65;3,315.00,426.13,201.57,9.65">S ε (µ, ν) is nonnegative, bi-convex thus a valid metric on P 2 (X ) and metricize the convergence in law.</s><s coords="3,523.69,426.45,34.31,8.64;3,315.00,437.09,243.00,9.65;3,315.00,447.47,243.00,9.53;3,315.00,459.32,25.73,8.64">Actually S ε (µ, ν) interpolates the Wasserstein distance (ϵ → 0) and the Maximum Mean Discrepancy (ϵ → ∞) <ref type="bibr" coords="3,502.14,447.47,55.86,9.31;3,315.00,459.32,21.44,8.64" target="#b38">[Feydy et al., 2019]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" coords="3,315.00,477.64,93.23,9.81">Gradient flows</head><p><s coords="3,315.00,492.02,189.30,9.65">Consider an optimization problem over P 2 (X ):</s></p><formula xml:id="formula_7" coords="3,381.43,506.45,172.70,17.12">min µ∈P2(X ) F(µ) := D(µ|µ * ). (<label>6</label></formula><formula xml:id="formula_8" coords="3,554.13,508.84,3.87,8.64">)</formula><p><s coords="3,315.00,531.56,34.01,8.96;3,349.01,529.98,4.08,6.12;3,357.26,531.56,200.74,8.96;3,315.00,542.83,30.16,8.64">where µ * is the target distribution, D is the divergence we choose.</s><s coords="3,350.31,542.83,207.69,8.64;3,315.00,553.47,235.93,9.65;3,550.93,551.90,4.08,6.12;3,555.51,553.79,2.49,8.64;3,315.00,564.43,243.00,9.65;3,315.00,575.39,117.57,9.65;3,432.57,573.82,4.08,6.12;3,440.87,575.39,100.08,9.65">We consider now the problem of transporting mass from an initial distribution µ 0 to a target distribution µ * , by finding a continuous probability path µ t starting from µ 0 = µ that converges to µ * while decreasing F(µ t ).</s><s coords="3,547.73,575.71,10.27,8.64;3,315.00,586.67,243.00,8.64;3,315.00,597.31,243.00,8.96;3,315.00,608.27,243.00,9.65;3,315.00,619.23,24.71,8.96;3,339.71,617.65,4.08,6.12;3,344.29,619.55,2.49,8.64">To solve this optimization problem, one can consider a descent flow of F(µ) in the Wasserstein space, which transports any initial distribution µ 0 towards the target distribution µ * .</s><s coords="3,352.00,619.23,206.00,8.96;3,315.00,629.61,243.00,9.53;3,315.00,641.46,187.85,8.64">Specifically, the descent flow of F(µ) is described by the following continuity equation <ref type="bibr" coords="3,463.98,629.61,94.02,9.53" target="#b6">[Ambrosio et al., 2005;</ref><ref type="bibr" coords="3,315.00,641.46,98.47,8.64" target="#b116">Villani and others, 2009;</ref><ref type="bibr" coords="3,415.96,641.46,82.56,8.64">Santambrogio, 2017]</ref>:</s></p><formula xml:id="formula_9" coords="3,375.81,657.31,182.20,22.31">∂µ t (x) ∂t = -∇ • (µ t (x)v t (x)).<label>(7)</label></formula><p><s coords="3,315.00,684.21,243.00,9.68;3,315.00,695.51,121.75,8.64">where v µt : X → X is a velocity field that defines the direction of position transportation.</s><s coords="3,440.37,695.20,117.63,9.65;4,54.00,57.13,243.00,9.68;4,54.00,68.26,145.55,8.82">To ensure a descent of F(µ t ) over time t, the velocity field v µt should satisfy the following inequality ( <ref type="bibr" coords="4,100.99,68.26,90.18,8.82" target="#b6">[Ambrosio et al., 2005]</ref>):</s></p><formula xml:id="formula_10" coords="4,99.25,85.26,197.75,22.31">dF(µ t ) dt = ⟨∇ δF(µ t ) δµ , v t ⟩dµ t ≤ 0.<label>(8)</label></formula><p><s coords="4,54.00,118.32,184.95,9.68;4,240.15,115.56,24.66,6.12;4,248.12,123.83,8.72,6.12;4,266.01,118.67,30.99,8.64;4,54.00,131.61,211.53,9.65">A straightforward choice of v t is v t = -∇ δF (µt) δµ , which is actually the steepest descent direction of F(µ t ).</s><s coords="4,273.20,131.93,23.79,8.64;4,54.00,142.54,243.00,9.68;4,54.00,153.53,243.00,8.96;4,54.00,164.81,243.00,8.64;4,54.00,175.77,109.01,8.64">When we select this v t , we refer to the aforementioned continuous equation as the Wasserstein gradient flow of F. We give the definition of the first variation in the appendix for the sake of completeness of the article.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="4,54.00,197.73,85.01,10.75">Methodology</head><p><s coords="4,54.00,214.35,243.00,8.64;4,54.00,225.31,243.00,8.64">In this section, we first introduce the Sinkhorn Wasserstein gradient flow and investigate its convergence properties.</s><s coords="4,54.00,236.27,243.00,8.64;4,54.00,247.23,243.00,8.64;4,54.00,258.18,195.86,8.64">Then, we develop our Neural Sinkhorn Gradient Flow model, which consists of a velocity field matching training procedure and a velocity field guided inference procedure.</s><s coords="4,255.92,258.18,41.08,8.64;4,54.00,269.14,243.00,8.64;4,54.00,280.10,243.00,8.64;4,54.00,291.06,243.00,8.64;4,54.00,302.02,19.31,8.64">Moreover, we theoretically show that the mean-field limit of the empirical approximation used in the training procedure converges to the true velocity field of the Sinkhorn Wasserstein gradient flow.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" coords="4,54.00,320.72,190.55,9.81">Sinkhorn Wasserstein gradient flow</head><p><s coords="4,54.00,335.62,243.00,8.64;4,54.00,346.26,189.55,9.65;4,243.55,344.69,4.08,6.12;4,248.13,346.26,44.29,8.96;4,292.42,344.69,4.08,6.12;4,54.00,357.54,122.27,8.64">Based on the definition of the Sinkhorn divergence, we construct our Sinkhorn objective F ε (•) = S ε (•, µ * ), where µ * denotes the target distribution.</s><s coords="4,180.81,357.54,116.18,8.64;4,54.00,368.50,173.41,8.64">The following theorem gives the first variation of the Sinkhorn objective.</s><s coords="4,54.00,382.20,243.00,9.63;4,54.00,394.04,54.74,8.82">Theorem 1. (First variation of the Sinkhorn objective <ref type="bibr" coords="4,271.54,382.20,25.46,8.59;4,54.00,394.04,54.74,8.82" target="#b77">[Luise et al., 2019]</ref></s></p><formula xml:id="formula_11" coords="4,54.00,393.90,243.00,71.68">) Let ε &gt; 0. Let (f µ,µ * , g µ,µ * ) be the W ε -potentials of W ε (µ, µ * ) and (f µ,µ , g µ,µ ) be the W ε - potentials of W ε (µ, µ). The first variation of the Sinkhorn objective F ε is δF ε δµ = f µ,µ * -f µ,µ .<label>(9)</label></formula><p><s coords="4,63.96,473.41,233.03,8.64;4,54.00,484.02,243.00,9.68;4,54.00,497.24,158.93,8.99;4,213.30,495.69,9.07,6.12;4,212.93,501.77,7.65,6.12;4,227.33,497.27,27.76,8.74;4,256.28,494.47,27.82,6.12;4,264.18,502.74,11.53,6.12;4,289.25,497.27,7.75,8.74;4,54.00,509.97,80.21,9.65">According to Theorem 1, we can construct the Sinkhorn Wasserstein gradient flow by setting the velocity field v t in the continuity equation equation 7 as v Fε µt = -∇ δFε(µt) δµt = ∇f µt,µt -∇f µt,µ * .</s><s coords="4,54.00,524.66,59.68,8.96">Proposition 1.</s><s coords="4,119.08,524.87,177.92,8.59;4,54.00,535.83,207.55,8.59">Consider the Sinkhorn Wasserstein gradient flow described by the following continuity equation:</s></p><formula xml:id="formula_12" coords="4,60.18,552.74,232.67,22.31">∂µ t (x) ∂t = -∇ • (µ t (x)(∇f µt,µt (x) -∇f µt,µ * (x))). (<label>10</label></formula><formula xml:id="formula_13" coords="4,292.85,559.80,4.15,8.64">)</formula><p><s coords="4,54.00,580.65,212.72,9.65">The following local descending property of F ε holds:</s></p><formula xml:id="formula_14" coords="4,60.73,597.69,236.27,22.31">dF ε (µ t ) dt = -∥∇f µt,µt (x) -∇f µt,µ * (x)∥ 2 dµ t ,<label>(11)</label></formula><p><s coords="4,54.00,627.53,60.23,8.59">where the r.h.s.</s><s coords="4,117.32,627.39,117.91,9.65;4,235.23,625.82,4.08,6.12;4,239.81,627.53,2.49,8.59">equals 0 if and only if µ t = µ * .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" coords="4,54.00,646.41,137.08,9.81">Velocity-fields Matching</head><p><s coords="4,54.00,661.31,243.00,8.64;4,54.00,672.27,243.00,8.64;4,54.00,682.88,60.80,8.99;4,115.17,681.33,9.07,6.12;4,114.80,687.41,7.65,6.12;4,127.98,683.23,169.02,8.64;4,54.00,695.51,19.31,8.64">We now present our NSGF method, the core of which lies in training a neural network to approximate the time-varying velocity field v Fε µt induced by Sinkhorn Wasserstein gradient flow.</s><s coords="4,79.30,695.20,217.71,9.65;4,315.00,57.13,120.62,8.99;4,435.99,55.58,8.26,6.12;4,435.63,61.66,7.65,6.12;4,445.25,57.16,112.75,9.65;4,315.00,68.44,217.53,8.64">Given a target probability density path µ t (x) and it's corresponding velocity field v Sε µt , which generates µ t (x), we define the velocity field matching objective as follows:</s></p><formula xml:id="formula_15" coords="4,338.71,86.21,215.14,19.11">min θ E t∼[0,T ],x∼µt v θ (x, t) -v Sε µt (x) 2 . (<label>12</label></formula><formula xml:id="formula_16" coords="4,553.85,90.98,4.15,8.64">)</formula><p><s coords="4,324.96,114.69,233.03,8.64;4,315.00,125.33,218.65,9.65;4,533.65,123.75,4.92,6.12;4,533.65,129.96,12.91,6.12;4,551.36,125.33,6.64,8.74;4,315.00,139.56,7.20,7.11;4,322.20,135.80,4.15,6.12;4,329.13,137.38,224.29,8.96;4,553.42,135.80,4.08,6.12;4,315.00,148.65,216.04,8.64">To construct our algorithm, we utilize independently and identically distributed (i.i.d) samples denoted as {Y i } n i=1 ∈ R d , which are drawn from an unknown target distribution µ * a common practice in the field of generative modeling.</s><s coords="4,534.05,148.65,23.95,8.64;4,315.00,158.14,126.32,11.48;4,437.53,165.29,2.82,6.12;4,441.82,160.66,4.98,8.74;4,446.80,159.09,4.92,6.12;4,446.80,165.29,12.91,6.12;4,464.82,160.66,93.18,9.65;4,315.00,171.62,243.00,9.65;4,315.00,182.58,145.81,9.65">Given the current set of samples { Xt i } n i=1 ∼ µ t , our method calculates the velocity field using the W ε -potentials (Lemma 1) f μt,μ * and f μt,μt based on samples.</s><s coords="4,465.56,182.58,92.44,8.96;4,315.00,193.86,128.75,8.64">Here, μt and μ * represent discrete Dirac distributions.</s></p><p><s coords="4,315.00,209.37,46.08,8.96">Remark 1.</s><s coords="4,366.90,209.44,191.10,9.65;4,315.00,219.66,243.00,9.69;4,315.00,231.50,26.29,8.59">In the discrete case, W ε -potentials equation 1 can be computed by a standard method in <ref type="bibr" coords="4,492.86,219.66,65.14,9.69;4,315.00,231.50,21.91,8.59" target="#b46">[Genevay et al., 2016]</ref>.</s><s coords="4,344.57,231.50,213.43,8.59;4,315.00,242.46,243.00,8.59;4,315.00,252.54,136.35,9.70">In practice, we use the efficient implementation of the Sinkhorn algorithm with GPU acceleration from the Geom-Loss package <ref type="bibr" coords="4,371.05,252.54,75.92,9.70" target="#b38">[Feydy et al., 2019]</ref>.</s></p><p><s coords="4,324.96,269.50,233.03,8.64;4,315.00,280.46,131.19,8.64">The corresponding finite sample velocity field approximation can be computed as follows:</s></p><formula xml:id="formula_17" coords="4,332.79,297.91,225.21,15.58">vFε μt ( Xt i ) = ∇ Xt i f μt,μt ( Xt i ) -∇ Xt i f μt,μ * ( Xt i ).<label>(13)</label></formula><p><s coords="4,315.00,321.77,243.00,8.64;4,315.00,332.72,158.87,8.64">Subsequently, we derive the particle formulation corresponding to the flow formulation equation 10.</s></p><formula xml:id="formula_18" coords="4,363.49,352.43,194.51,13.53">d Xt i = vFϵ μt Xt i dt, i = 1, 2, • • • n.<label>(14)</label></formula><p><s coords="4,315.00,377.47,243.00,8.64;4,315.00,389.80,90.21,8.64">In the following proposition, we investigate the mean-field limit of the particle set</s></p><formula xml:id="formula_19" coords="4,315.00,386.96,152.64,27.31">{ Xt i } i=1,••• ,M . Theorem 2.</formula><p><s coords="4,369.17,405.52,77.33,8.59">(Mean-field limits.)</s><s coords="4,450.81,405.52,107.20,8.59;4,315.00,416.34,243.00,8.74;4,315.00,427.30,100.65,9.65">Suppose the empirical distribution μ0 of M particles weakly converges to a distribution µ 0 when M → ∞.</s><s coords="4,422.09,427.44,135.91,8.59;4,315.00,438.26,243.00,8.74;4,315.00,449.22,243.00,9.65;4,315.00,460.18,39.53,8.74">Then, the path of equation equation 14 starting from μ0 weakly converges to a solution of the following partial differential equation starting from µ 0 when M → ∞:</s></p><formula xml:id="formula_20" coords="4,365.01,476.67,188.84,23.22">∂µ t (x) ∂t = -∇ • (µ t (x)∇ δF ε (µ t ) δµ t ). (<label>15</label></formula><formula xml:id="formula_21" coords="4,553.85,483.73,4.15,8.64">)</formula><p><s coords="4,315.00,507.93,242.50,9.65;4,315.00,519.03,99.56,8.59">which is actually the gradient flow of Sinkhorn divergence F ε in the Wasserstein space.</s></p><p><s coords="4,324.96,535.11,233.03,8.64;4,315.00,546.07,243.00,8.64;4,315.00,557.03,243.00,8.64;4,315.00,567.99,38.91,8.64">The following proposition shows that the goal of the velocity field matching objective equation 12 can be regarded as approximating the steepest local descent direction with neural networks.</s></p><p><s coords="4,315.00,583.51,201.54,8.96">Proposition 2. (Steepest local descent direction.)</s><s coords="4,521.47,583.72,36.53,8.59;4,315.00,594.53,185.53,8.74">Consider the infinitesimal transport T (x) = x + λϕ.</s><s coords="4,508.62,594.67,49.38,8.59;4,315.00,605.63,180.71,8.59">The Fréchet derivative under this particular perturbation,</s></p><formula xml:id="formula_22" coords="4,327.44,623.92,230.56,49.05">d dλ F ε (T # µ)| λ=0 = lim λ→0 F ε (T # µ) -F ε (µ) λ = X ∇f µ,µ * (x)ϕ(x)dµ - X ∇f µ,µ (x)ϕ(x)dµ,<label>(16)</label></formula><p><s coords="4,315.00,680.67,220.27,8.74">and the steepest local descent direction is ϕ</s></p><formula xml:id="formula_23" coords="4,316.20,680.67,241.80,26.31">= ∇f µ,µ * (x)-fµ,µ(x) ∥∇f µ,µ * (x)-fµ,µ(x)∥ .</formula><formula xml:id="formula_24" coords="5,81.62,140.01,191.89,100.15">/ X0 i ∼ µ 0 , Ỹi ∼ µ * , i = 1, 2, • • • n. for t = 0, 1, • • • T do calculatef μt,μt Xt i , f μt,μ * Xt i . vFϵ µt Xt i = ∇f μt,μt Xt i -∇f μt,μ * Xt i . Xt+1 i = Xt i + η vFϵ t Xt i . store all Xt i , vFϵ t Xt i pair into the pool, i = 1, 2, • • • n.</formula><p><s coords="5,66.09,248.51,132.67,7.48;5,263.31,248.51,10.21,7.48;5,66.09,256.74,89.25,7.65;5,79.19,267.23,126.84,9.79;5,202.79,269.26,20.06,9.29">/ * velocity field matching * / while Not convergence do from trajectory pool sample pair Xt i , vFϵ</s></p><formula xml:id="formula_25" coords="5,83.44,267.23,169.74,41.04">t Xt i . L(θ) = v θ ( Xt i , t) -vFε µt Xt i 2 , θ ← θ -γ∇ θ L (θ</formula><formula xml:id="formula_26" coords="5,63.96,369.19,224.68,52.17">i ∼ μ0 for t = 0, 1, • • • T do Xt+1 i = Xt i + ηv θ Xt i , t , i = 1, 2, • • • n. Output: XT</formula><p><s coords="5,107.45,417.26,2.72,5.90;5,116.66,413.10,52.07,8.33">i as the results.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" coords="5,54.00,447.14,210.37,9.81;5,78.55,459.09,87.57,9.81">Minibatch Sinkhorn Gradient Flow and Experience Replay</head><p><s coords="5,54.00,475.77,243.00,8.64;5,54.00,486.73,91.02,8.64">According to Theorem 2, we construct our NSGF method based on minibatches.</s><s coords="5,152.23,486.73,144.77,8.64;5,54.00,497.37,67.17,9.65;5,121.17,495.80,4.92,6.12;5,121.17,502.01,12.91,6.12;5,134.57,497.37,162.43,8.96;5,54.00,508.65,243.00,8.64;5,54.00,519.61,182.86,8.64">We utilize a set of discrete targets, denoted as {Y i } n i=1 , where n represents the batch size, to construct the Sinkhorn Gradient Flow starting from random Gaussian noise or other initial distributions.</s><s coords="5,245.17,519.61,51.82,8.64;5,54.00,530.57,243.00,8.64;5,54.00,541.21,243.00,8.96">As indicated by Theorem 2, the mean-field limit converges to the true Sinkhorn Gradient flow when the batch size approaches ∞.</s><s coords="5,54.00,552.49,243.00,8.64;5,54.00,563.44,243.00,8.64;5,54.00,574.40,221.68,8.64">Note that in practice, we only use a moderate batch size for computation efficiency, and the experimental results demonstrate that this works well for practical generative tasks.</s></p><p><s coords="5,63.96,585.93,233.04,8.64;5,54.00,596.88,243.00,8.64;5,54.00,607.84,243.00,8.64;5,54.00,618.80,151.01,8.64">Considering the balance between expensive training costs and training quality, we opted to first build a trajectory pool of Sinkhorn gradient flow and then sample from it to construct the velocity field matching algorithm.</s><s coords="5,208.77,618.80,88.23,8.64;5,54.00,629.76,243.00,8.64;5,54.00,640.72,243.00,8.64;5,54.00,650.78,203.45,9.53">Our method draws inspiration from experience replay, a common technique in reinforcement learning, adapting it to enhance our model's effectiveness <ref type="bibr" coords="5,100.69,650.78,76.08,9.53" target="#b79">[Mnih et al., 2013;</ref><ref type="bibr" coords="5,179.59,651.50,69.29,8.82" target="#b106">Silver et al., 2016</ref>].</s><s coords="5,261.54,651.68,35.47,8.64;5,54.00,662.18,170.29,9.09;5,220.16,666.82,7.65,6.12;5,228.81,659.80,15.92,11.26;5,240.93,666.95,2.82,6.12;5,245.22,662.32,51.77,8.96;5,54.00,673.60,243.00,8.64;5,54.00,684.56,51.81,8.64">Once we calculate the time-varying velocity field vs µt ( Xt i ), we can parameterize the velocity field using a straightforward regression method.</s><s coords="5,109.86,684.56,187.14,8.64;5,54.00,695.51,106.00,8.64;5,324.96,211.24,225.67,8.99;5,551.00,209.69,3.79,6.12;5,555.51,211.59,2.49,8.64;5,315.00,222.55,243.00,8.64;5,315.00,233.50,243.00,8.64;5,315.00,244.46,119.47,8.64">The velocity field matching training procedure is outlined in Algorithm 1. Once obtained a feasible velocity field approximation v θ , one can generate new samples by iteratively employing the explicit Euler discretization of the Equation equation 14 to drive the samples to the target.</s><s coords="5,437.40,244.46,120.60,8.64;5,315.00,254.53,243.00,9.53;5,315.00,265.48,243.00,9.53;5,315.00,277.34,100.83,8.64">Note that various other numerical schemes, such as the implicit Euler method <ref type="bibr" coords="5,512.24,254.53,45.77,9.53;5,315.00,266.38,86.67,8.64" target="#b93">[Platen and Bruti-Liberati, 2010]</ref> and Runge-Kutta methods <ref type="bibr" coords="5,521.05,265.48,36.95,8.64;5,315.00,277.34,21.44,8.64" target="#b16">[Butcher, 1964]</ref>, can be employed.</s><s coords="5,421.43,277.34,136.57,8.64;5,315.00,287.40,243.00,9.53;5,315.00,299.26,224.85,8.64">In this study, we opt for the firstorder explicit Euler discretization method <ref type="bibr" coords="5,485.88,287.40,72.12,9.53;5,315.00,299.26,23.24,8.64" target="#b112">[Süli and Mayers, 2003]</ref> due to its simplicity and ease of implementation.</s><s coords="5,544.96,299.26,13.03,8.64;5,315.00,310.22,243.00,8.64;5,315.00,321.18,27.94,8.64">We leave the exploration of higher-order algorithms for future research.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4" coords="5,315.00,345.05,66.08,9.81">NSGF++</head><p><s coords="5,315.00,365.03,243.00,8.64;5,315.00,375.99,216.76,8.64">In this subsection, we propose an approach to enhance the performance of NSGF on high-dimensional datasets.</s><s coords="5,538.72,375.99,19.28,8.64;5,315.00,386.95,243.00,8.64;5,315.00,397.91,243.00,8.64;5,315.00,408.87,243.00,8.64;5,315.00,419.83,68.29,8.64">As a trajectory pool should be first constructed in the velocity field matching training procedure of NSGF, the storage and computation costs would greatly hinter the usage of NSGF in large-scale tasks.</s><s coords="5,387.64,419.83,170.36,8.64;5,315.00,430.79,243.00,8.64;5,315.00,441.74,243.00,8.64;5,315.00,452.70,209.84,8.64">To tackle this problem, we propose a twophase NSGF++ algorithm, which first follows the Sinkhorn gradient flow to approach the image manifold quickly and then refine the samples along a simple straight flow.</s><s coords="5,529.23,452.70,28.77,8.64;5,315.00,463.66,243.00,8.64;5,315.00,474.30,243.00,8.96;5,315.00,485.58,243.00,8.64;5,315.00,496.22,243.00,9.65;5,315.00,507.50,243.00,8.64;5,315.00,518.46,243.00,8.64">Specifically, our NSGF++ model consists of three components, (1) a NSGF model trained on T ≤ 5 time steps, (2) a Neural Straight Flow (NSF) model trained via velocity field matching on a straight flow X t ∼ (1 -t)P 0 + tP 1 , t ∈ [0, 1], which has also been used in existing FM models, (3) a phasetransition time predictor to transfer from NSGF to the NSF.</s><s coords="5,315.00,529.10,243.00,9.65;5,315.00,540.37,123.17,8.64">Here, we train the time predictor t ϕ : X → [0, 1] with the following regression objective:</s></p><formula xml:id="formula_27" coords="5,355.75,559.24,161.51,12.03">L(ϕ) = E t∈U (0,1),Xt∼Pt ||t -t ϕ (X t )|| 2 .</formula><p><s coords="5,315.00,582.90,243.00,8.64;5,315.00,593.85,218.27,8.64">Note that the training of the straight NSF model and the time predictor is simulated free and need no extra storage.</s><s coords="5,539.10,593.85,18.90,8.64;5,315.00,604.81,243.00,8.64;5,315.00,615.77,243.00,8.64;5,315.00,626.73,243.00,8.64;5,315.00,637.69,20.75,8.64">As a result, the training cost of NSFG++ is similar to existing FM models, since the computation cost of NSF is nearly the same as FM, and the 5-step NSGF and the time predictor is easy to train.</s></p><p><s coords="5,324.96,650.31,233.03,8.64;5,315.00,658.43,243.00,12.17;5,315.00,671.91,243.00,9.65;5,315.00,683.19,243.00,8.64;5,315.00,692.68,118.64,12.17">In the inference of NSGF++, we first follow the NSGF with less than 5 NFEs form X 0 ∼ P 0 to obtain Xt , then transfer it with the time predictor t ϕ , and obtain our final output by refining the transferred sample with the NSF model from t ϕ ( Xt ), as shown in Figure <ref type="figure" coords="5,426.17,695.51,3.74,8.64" target="#fig_4">5</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="6,54.00,575.92,83.02,10.75">Experiments</head><p><s coords="6,54.00,596.88,243.00,8.64;6,54.00,607.84,243.00,8.64;6,54.00,618.80,186.40,8.64">We conduct an empirical investigation of the NSGF-based generative models (the standard NSGF and its two-phase variant NSGF++) across a range of experiments.</s><s coords="6,247.73,618.80,49.27,8.64;6,54.00,629.76,243.00,8.64;6,54.00,640.72,243.00,8.64;6,54.00,651.68,184.02,8.64">Initially, we demonstrate how NSGF guides the evolution and convergence of particles from the initial distribution toward the target distribution in 2D simulation experiments.</s><s coords="6,241.47,651.68,55.53,8.64;6,54.00,662.64,243.00,8.64;6,54.00,673.60,112.25,8.64">Subsequently, our attention turns to real-world image benchmarks, such as MNIST and CIFAR-10.</s><s coords="6,176.07,673.60,120.93,8.64;6,54.00,684.56,243.00,8.64;6,54.00,695.51,158.66,8.64">To improve the efficiency in those high-dimensional tasks, we adopt the two-phase variant NSGF++ instead of the standard NSGF.</s><s coords="6,215.14,695.51,81.86,8.64;6,315.00,357.97,243.00,8.64;6,315.00,368.93,154.67,8.64">Our method's adapt-ability to high-dimensional spaces is exemplified through experiments conducted on these datasets.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" coords="6,315.00,391.15,113.04,9.81">2D simulation data</head><p><s coords="6,315.00,409.47,243.00,8.64;6,315.00,420.43,108.46,8.64">We assess the performance of various generative modeling models in low dimensions.</s><s coords="6,428.43,420.43,129.56,8.64;6,315.00,431.39,243.00,8.64;6,315.00,442.35,243.00,8.64;6,315.00,452.41,243.00,9.53;6,315.00,463.37,243.00,9.53;6,315.00,474.33,243.00,9.53;6,315.00,485.29,243.00,9.53;6,315.00,497.14,243.00,8.64;6,315.00,507.21,219.90,9.53">Specifically, we conduct a comparative analysis between our method, NSGF, and several neural ODE-based diffusion models, including Flow Matching (FM; <ref type="bibr" coords="6,355.28,452.41,85.84,9.53" target="#b71">[Lipman et al., 2023]</ref>), Rectified Flow (1,2,3-RF; <ref type="bibr" coords="6,315.00,463.37,64.21,9.53" target="#b73">[Liu et al., 2023]</ref>), Optimal Transport Condition Flow Matching (OT-CFM; <ref type="bibr" coords="6,377.32,474.33,77.22,9.53" target="#b114">[Tong et al., 2023;</ref><ref type="bibr" coords="6,458.13,475.05,91.57,8.82" target="#b95">Pooladian et al., 2023]</ref>), Stochastic Interpolant (SI; <ref type="bibr" coords="6,430.54,485.29,127.46,9.53;6,315.00,497.14,20.75,8.64" target="#b2">[Albergo and Vanden-Eijnden, 2023]</ref>), and neural gradient-flow-based models such as JKO-Flow <ref type="bibr" coords="6,339.23,507.21,73.65,9.53" target="#b32">[Fan et al., 2022]</ref> and EPT <ref type="bibr" coords="6,456.96,507.21,73.65,9.53" target="#b44">[Gao et al., 2022]</ref>.</s><s coords="6,542.50,508.10,15.49,8.64;6,315.00,519.06,243.00,8.64;6,315.00,529.12,224.32,9.53">Our evaluation involves learning 2D distributions adapted from <ref type="bibr" coords="6,315.00,529.12,94.67,9.53" target="#b52">[Grathwohl et al., 2018]</ref>, which include multiple modes.</s></p><p><s coords="6,324.96,542.09,233.03,8.64;6,315.00,553.05,243.00,8.64;6,315.00,564.01,78.06,8.64">Table <ref type="table" coords="6,349.61,542.09,4.98,8.64" target="#tab_1">1</ref> provides a comprehensive overview of our 2D experimental results, clearly illustrating the generalization capabilities of NSGF.</s><s coords="6,396.10,564.01,141.41,8.64">Even when employing fewer steps.</s><s coords="6,542.24,564.01,15.76,8.64;6,315.00,574.97,243.00,8.64;6,315.00,585.93,243.00,8.64;6,315.00,596.88,84.14,8.64">It is evident that neural gradient-flow-based models consistently outperform neural ODE-based diffusion models, particularly in low-step settings.</s><s coords="6,408.85,596.88,149.15,8.64;6,315.00,607.84,243.00,8.64;6,315.00,618.80,243.00,8.64;6,315.00,629.76,33.84,8.64">This observation suggests that neural gradient-flow-based models generate more informative paths, enabling effective generation with a reduced number of steps.</s><s coords="6,353.85,629.76,204.14,8.64;6,315.00,640.72,243.00,8.64;6,315.00,651.68,243.00,8.64;6,315.00,662.64,140.93,8.64">Furthermore, our results showcase the best performances among neural gradient-flow-based models, indicating that we have successfully introduced a lower error in approximating Wasserstein gradient flows.</s><s coords="6,458.93,662.64,99.07,8.64;6,315.00,673.60,243.00,8.64;6,315.00,684.56,243.00,8.64;6,315.00,695.51,61.97,8.64">More complete details of the experiment can be found in the appendix E. In the absence of specific additional assertions, we adopted Euler steps as the inference steps.</s><s coords="7,63.96,221.83,233.03,8.64;7,54.00,232.79,243.00,8.64;7,54.00,243.75,243.00,8.64;7,54.00,254.39,187.92,8.96">We present additional comparisons between neural ODEbased diffusion models and neural gradient-flow-based models, represented by NSGF and EPT, in Figure <ref type="figure" coords="7,239.91,243.75,3.74,8.64">3</ref>, 4, which illustrates the flow at different steps from 0 to T .</s><s coords="7,245.00,254.71,52.00,8.64;7,54.00,265.67,243.00,8.64;7,54.00,276.63,195.68,8.64">Our observations reveal that the velocity field induced by NSGF exhibits notably high-speed values right from the outset.</s><s coords="7,255.39,276.63,41.60,8.64;7,54.00,287.59,243.00,8.64;7,54.00,298.55,142.38,8.64">This is attributed to the fact that NSGF follows the steepest descent direction within the probability space.</s><s coords="7,199.36,298.55,97.63,8.64;7,54.00,309.50,243.00,8.64;7,54.00,320.46,243.00,8.64;7,54.00,331.42,51.04,8.64">In contrast, neural ODEbased diffusion models, particularly those based on stochastic interpolation, do not follow the steepest descent path in 2D experiments.</s><s coords="7,110.53,331.42,186.47,8.64;7,54.00,341.49,243.00,9.53;7,54.00,353.34,208.06,8.64">Even with the proposed rectified flow method by <ref type="bibr" coords="7,66.20,341.49,67.07,9.53" target="#b73">[Liu et al., 2023]</ref> to straighten the path, these methods still necessitate more steps to reach the desired outcome.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" coords="7,54.00,372.77,133.03,9.81">Image benchmark data</head><p><s coords="7,54.00,388.30,243.00,8.64;7,54.00,399.26,243.00,8.64;7,54.00,410.22,77.93,8.64">In this section, we illustrate the scalability of our algorithm to the high-dimensional setting by applying our methods to real image datasets.</s><s coords="7,134.96,410.22,162.04,8.64;7,54.00,421.18,243.00,8.64;7,54.00,432.14,150.10,8.64">Notably, we leverage the two-phase variant (NSGF++) instead of the standard NSGF to enhance efficiency in high-dimensional spaces.</s><s coords="7,212.43,432.14,84.57,8.64;7,54.00,443.10,243.00,8.64;7,54.00,454.06,243.00,8.64;7,54.00,465.01,243.00,8.64;7,54.00,475.97,130.56,8.64">It is worth mentioning that we achieve this improvement by constructing a significantly smaller training pool compared with the standard NSGF pool (10% of the usual size), thus requiring only 5% of the typical training duration.</s><s coords="7,192.39,475.97,104.61,8.64;7,54.00,486.93,220.08,8.64">We evaluate NSGF++ on MNIST and CIFAR10 to show our generating ability.</s><s coords="7,280.40,486.93,16.60,8.64;7,54.00,497.89,243.00,8.64;7,54.00,508.85,178.48,8.64">Due to the limit of the space, we defer the generative images and comparison results of MNIST in appendix 3.</s></p><p><s coords="7,63.96,519.99,233.03,8.64;7,54.00,530.05,243.00,9.53;7,54.00,541.01,243.00,9.53;7,54.00,552.87,121.70,8.64">We report sample quality using the standard Fréchet Inception Distance (FID) <ref type="bibr" coords="7,132.81,530.05,78.23,9.53" target="#b56">[Heusel et al., 2017]</ref>, Inception Score (IS) <ref type="bibr" coords="7,54.00,541.01,93.52,9.53" target="#b96">[Salimans et al., 2016]</ref> and compute cost using the number of function evaluations (NFE).</s><s coords="7,178.16,552.87,118.83,8.64;7,54.00,563.83,99.34,8.64">These are all standard metrics throughout the literature.</s></p><p><s coords="7,63.96,574.97,233.04,8.64;7,54.00,585.93,243.00,8.64;7,54.00,596.88,243.00,8.64;7,54.00,607.84,243.00,8.64;7,54.00,618.80,27.76,8.64">Table <ref type="table" coords="7,89.29,574.97,4.98,8.64" target="#tab_2">2</ref> presents the results, including the Fréchet Inception Distance (FID), Inception Score (IS), and the number of function evaluations (NFE), comparing the empirical distribution generated by each algorithm with the target distribution.</s><s coords="7,89.31,618.80,207.69,8.64;7,54.00,629.76,243.00,8.64;7,54.00,640.72,243.00,8.64;7,54.00,650.78,243.00,9.53;7,54.00,661.74,233.75,9.53;7,289.21,660.74,3.97,6.12;7,293.68,662.64,3.32,8.64;7,54.00,672.70,148.17,9.53">While our current implementation may not yet rival state-of-the-art methods, it demonstrates promising outcomes, particularly in terms of generating quality (FID), outperforming neural gradient-flow-based models (EPT, <ref type="bibr" coords="7,267.36,650.78,29.64,9.30;7,54.00,662.46,37.92,8.82" target="#b44">[Gao et al., 2022]</ref>; JKO-Flow, <ref type="bibr" coords="7,146.83,661.74,69.83,9.53" target="#b32">[Fan et al., 2022]</ref>; DGGF,(LSIF-X 2 ) <ref type="bibr" coords="7,54.00,672.70,74.08,9.53" target="#b54">[Heng et al., 2022]</ref>) with fewer steps.</s><s coords="7,205.22,673.60,91.79,8.64;7,54.00,684.56,243.00,8.64;7,54.00,695.51,243.00,8.64">It's essential to emphasize that this work represents an initial exploration of this particular model category and has not undergone optimization</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="7,315.00,481.81,75.07,10.75">Conclusion</head><p><s coords="7,315.00,498.25,243.00,8.64;7,315.00,509.21,243.00,8.64;7,315.00,520.17,36.25,8.64">This paper delves into the realm of Wasserstein gradient flow w.r.t. the Sinkhorn divergence as an alternative to kernel methods.</s><s coords="7,357.51,520.17,200.49,8.64;7,315.00,531.13,243.00,8.64;7,315.00,542.09,243.00,8.64;7,315.00,553.05,93.79,8.64">Our main investigation revolves around the Neural Sinkhorn Gradient Flow (NSGF) model, which introduces a parameterized velocity field that evolves over time in the Sinkhorn gradient flow.</s><s coords="7,412.34,553.05,145.65,8.64;7,315.00,564.01,243.00,8.64;7,315.00,574.97,243.00,8.64;7,315.00,585.93,21.87,8.64">One noteworthy aspect of the NSGF is its efficient velocity field matching, which relies solely on samples from the target distribution for empirical approximations.</s><s coords="7,343.59,585.93,214.41,8.64;7,315.00,596.88,243.00,8.64;7,315.00,607.84,243.00,8.64;7,315.00,618.80,134.76,8.64">The combination of rigorous theoretical foundations and empirical observations demonstrates that our approximations of the velocity field converge toward their true counterparts as the sample sizes grow.</s><s coords="7,454.23,618.80,103.77,8.64;7,315.00,629.76,243.00,8.64;7,315.00,640.72,243.00,8.64;7,315.00,651.68,243.00,8.64;7,315.00,662.64,157.52,8.64">To further enhance model efficiency on high-dimensional tasks, a two-phase NSGF++ model is devised, which first follows the Sinkhorn flow to approach the image manifold quickly and then refine the samples along a simple straight flow.</s><s coords="7,481.99,662.64,76.00,8.64;7,315.00,673.60,243.00,8.64;7,315.00,684.56,243.00,8.64;7,315.00,695.51,36.25,8.64">Through extensive empirical experiments on well-known datasets like MNIST and CIFAR-10, we validate the effectiveness of the proposed methods.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,54.00,55.71,116.08,10.75;10,170.08,60.31,3.91,6.99;10,174.49,55.71,53.80,10.75">A Computation of W ε -potentials</head><p><s coords="10,54.00,71.58,243.00,9.65">The W ε -potentials is the cornerstone to conduct NSGF.</s><s coords="10,54.00,82.86,243.00,8.64;10,54.00,93.82,69.92,8.64">Hence, a key component of our method is to efficiently compute this quantity.</s><s coords="10,126.82,92.92,170.18,9.53;10,54.00,104.46,243.00,8.96;10,54.00,115.41,188.92,9.65"><ref type="bibr" coords="10,126.82,92.92,87.31,9.53" target="#b46">[Genevay et al., 2016]</ref> provided an efficient method when both µ and ν are discrete measures so that we can calculate W ε -potential in terms of samples.</s><s coords="10,246.00,115.73,51.00,8.64;10,54.00,126.37,243.00,8.96;10,54.00,137.33,237.69,8.96">In particular, when µ is discrete, f can be simply represented by a finitedimensional vector since only its values on supp(µ) matter.</s></p><p><s coords="10,63.96,148.61,233.03,8.64;10,54.00,159.25,243.00,9.65;10,54.00,170.53,243.00,8.64;10,54.00,181.17,243.00,9.65;10,54.00,192.45,243.00,8.64;10,54.00,203.40,56.26,8.64">To more clearly explain the relationship between the calculation of W ε -protentials and the composition of our algorithm, we provide the following explanation: In practice, we actually calculate the W ε -potentials for the empirical distribution of discrete minibatches and construct Sinkhorn WGF based on this.</s><s coords="10,115.97,203.09,181.03,8.96;10,54.00,212.67,91.35,11.47;10,141.56,219.82,2.82,6.12;10,145.85,215.19,3.87,8.74;10,149.72,213.62,4.92,6.12;10,149.72,219.82,12.91,6.12;10,166.11,212.67,32.25,11.47;10,193.14,219.82,2.82,6.12;10,198.86,215.19,3.87,8.74;10,202.74,213.62,4.92,6.12;10,202.74,219.82,12.91,6.12;10,219.13,215.51,77.87,8.64">Therefore, in fact, the µ and ν in the subsequent text refer to ( Xt i ) n i=1 and ( Ỹ t i ) n i=1 in the Algorithm 1.</s><s coords="10,54.00,226.47,243.00,8.64;10,54.00,237.43,106.54,8.64">We first introduce another property of the entropy-regularized optimal transport problem.</s><s coords="10,54.00,250.94,177.54,8.96">Lemma 2. Define the Sinkhorn mapping:</s></p><formula xml:id="formula_28" coords="10,54.00,251.01,243.00,50.87">A : C(X ) × M + 1 (X ) → C(X ) A(f, µ)(y) = -ε log X exp((f (x) -c(x, y))/ε)dµ(x).</formula><p><s coords="10,280.40,301.83,16.60,8.64;10,54.00,312.47,243.00,9.65;10,54.00,323.57,218.35,8.59">(17) The pair (f µ,ν , g µ,ν ) are the W ε -potentials of the entropyregularized optimal transport problem 2 if they satisfy:</s></p><formula xml:id="formula_29" coords="10,107.80,338.51,189.20,23.60">f µ,ν = A(g µ,ν , ν), µ -a.e. and g µ,ν = A(f µ,ν , µ), ν -a.e.,<label>(18)</label></formula><p><s coords="10,54.00,368.05,60.04,8.59">or equivalently</s></p><formula xml:id="formula_30" coords="10,113.39,388.85,183.61,43.89">X h(x, y)dν(y) = 1, µ -a.e. , X h(x, y)dµ(x) = 1, ν -a.e. ,<label>(19)</label></formula><p><s coords="10,54.00,438.32,88.48,8.77;10,145.65,436.47,3.97,6.12;10,145.33,443.83,4.19,6.12;10,151.14,438.32,103.58,8.77">where h(x, y) := exp 1 γ (f (x) + g(x) -c(x, y)).</s><s coords="10,63.96,454.19,233.03,8.64;10,54.00,464.83,167.17,9.65">To be more precise, by plugging in the optimality condition on g µ,ν in 1, the dual problem 2 becomes:</s></p><formula xml:id="formula_31" coords="10,95.61,480.15,201.39,14.66">OT ε (µ, ν) = max f ∈C ⟨f, µ⟩ + ⟨A(f, µ), ν⟩<label>(20)</label></formula><p><s coords="10,54.00,501.12,243.00,9.68;10,54.00,512.11,74.47,8.96">Viewing the discrete measure µ as a weight vector w µ on supp(µ), we have:</s></p><formula xml:id="formula_32" coords="10,56.93,526.83,237.13,17.34">OT ε (µ, ν) = max f ∈R d F (f ) := f ⊤ w µ + E y∼ν [A(f , µ)(y)] ,</formula><p><s coords="10,280.40,545.65,16.60,8.64;10,54.00,556.61,243.00,8.64;10,54.00,567.57,243.00,8.64;10,54.00,577.63,101.32,9.53">(21) that is, we get a standard concave stochastic optimization problem, where the randomness of the problem comes from ν <ref type="bibr" coords="10,62.50,577.63,88.54,9.53" target="#b46">[Genevay et al., 2016]</ref>.</s><s coords="10,159.77,578.53,137.23,8.64;10,54.00,589.49,162.99,8.64">Hence, the problem can be solved using stochastic gradient descent (SGD).</s><s coords="10,219.62,589.49,77.38,8.64;10,54.00,600.13,231.53,9.65">In our methods, we can treat the computation of W ε -potentials as a Blackbox.</s><s coords="10,288.70,600.45,8.30,8.64;10,54.00,611.41,243.00,8.64;10,54.00,622.37,243.00,8.64;10,54.00,632.43,97.52,9.53">In practice, we use the efficient implementation of the Sinkhorn algorithm with GPU acceleration from the GeomLoss package <ref type="bibr" coords="10,70.32,632.43,76.92,9.53" target="#b38">[Feydy et al., 2019]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,54.00,654.42,230.85,10.75;10,73.93,668.37,21.14,10.75">B Theory of Sinkhorn Wasserstein gradient flow</head><p><s coords="10,54.00,684.17,243.00,8.96;10,54.00,695.34,15.77,8.59">Definition 3. (First variation of Functionals over Probability).</s><s coords="10,74.28,695.20,144.82,9.30;10,219.10,693.62,6.12,6.12;10,225.72,695.34,71.28,8.59;10,315.00,57.16,243.00,8.74;10,315.00,68.12,118.43,8.74">Given a functional F : P(X ) → R + , we shell perturb measure µ with a perturbation χ so that µ + tχ belongs to P(X ) for small t ( dχ = 0).</s><s coords="10,436.57,68.12,121.43,8.74;10,315.00,79.22,243.00,8.59;10,315.00,90.17,82.48,8.59">We treat F(µ), as a functional over probability in its second argument and compute its first variation as follows:</s></p><formula xml:id="formula_33" coords="10,319.59,107.90,238.41,31.25">d dt F (µ + tχ) t=0 = lim t→0 F (µ + tχ) -F (µ) t := δF δµ (µ) dχ.<label>(22)</label></formula><p><s coords="10,315.00,149.72,117.16,9.81">B.1 Proof of Theorem 1</s></p><p><s coords="10,315.00,164.86,24.74,8.59">Proof.</s><s coords="10,344.72,165.04,133.14,8.64">According to definition 3, given</s></p><formula xml:id="formula_34" coords="10,481.75,163.14,76.25,11.23">F ε (•) = S ε (•, µ * )</formula><p><s coords="10,315.00,175.68,157.16,8.96">and t in a neighborhood of 0, we define</s></p><formula xml:id="formula_35" coords="10,315.00,175.68,247.46,132.83">µ t = µ + tδµ lim t→0 1 t (F ε (µ t ) -F ε (µ)) = lim t→0 1 t (W ε (µ t , µ * ) -W ε (µ, µ * )) ∆ first part t -lim t→0 1 2t (W ε (µ t , µ t ) -W ε (µ, µ)) ∆ second part t We first analysis ∆ first part t := lim t→0 1 t (W ε (µ t , µ * ) -W ε (µ, µ * )).</formula><p><s coords="10,483.66,297.23,74.34,8.64;10,315.00,307.87,243.00,8.96;10,315.00,318.83,78.24,9.65">First, let us remark that (f, g) is the a suboptimal pair of dual potentials W ε,µ * (µ) for short.</s><s coords="10,396.33,319.15,35.42,8.64">Recall 3,</s></p><formula xml:id="formula_36" coords="10,317.43,337.45,238.15,19.25">W ε ≥ ⟨µ t , f ⟩ + ⟨µ * , g⟩ -ε µ t ⊗ µ * , exp 1 ε (f ⊕ g -C) -1 ,</formula><p><s coords="10,315.00,365.80,58.94,8.64">and thus, since</s></p><formula xml:id="formula_37" coords="10,317.43,382.67,238.14,19.75">W ε ≥ ⟨µ, f ⟩ + ⟨µ * , g⟩ -ε µ ⊗ µ * , exp 1 ε (f ⊕ g -C) -1 ,</formula><p><s coords="10,315.00,411.53,30.16,8.64">one has</s></p><formula xml:id="formula_38" coords="10,315.00,427.39,243.55,33.14">∆ first part t ≥ ⟨δµ, f ⟩ -ε δµ ⊗ µ * , exp 1 ε (f ⊕ g -C) + o(1) ≥ ⟨δµ, f -ε⟩ + o(1)</formula><p><s coords="10,324.96,470.75,233.03,9.65;10,315.00,481.70,96.47,9.65;10,411.46,480.13,4.08,6.12;10,416.04,481.70,141.96,9.65;10,315.00,492.66,129.78,9.65">Conversely, let us denote by (f t , g t ) the optimal pair of potentials for W ε (µ t , µ * ) satisfying g t (x o ) = 0 for some arbitrary anchor point x o ∈ X .</s><s coords="10,451.03,492.66,106.97,9.65;10,315.00,503.62,89.76,9.65;10,404.75,502.05,4.08,6.12;10,409.33,503.62,50.08,8.96">As (f t , g t ) are suboptimal potentials for W ε (µ, µ * ) we get that</s></p><formula xml:id="formula_39" coords="10,317.43,521.88,238.14,19.10">W ε ≥ ⟨µ, f t ⟩ + ⟨µ * , g t ⟩ -ε µ t ⊗ µ * , exp 1 ε (f ⊕ g -C) -1 ,</formula><p><s coords="10,315.00,550.07,58.94,8.64">and thus, since</s></p><formula xml:id="formula_40" coords="10,315.00,566.90,248.29,76.79">W ε ≥ ⟨µ t , f t ⟩ + ⟨µ * , g t ⟩ -ε µ t ⊗ µ * , exp 1 ε (f t ⊕ g -C) -1 , one has ∆ first part t ≥ ⟨δµ, ft⟩ -ε δµ ⊗ µ * t , exp 1 ε (ft ⊕ g -C) + o(1) ≥ ⟨δµ, ft -ε⟩ + o(1)</formula><p><s coords="10,315.00,653.24,242.50,9.65;10,315.00,664.20,243.00,8.96;10,315.00,674.58,93.66,9.53">Now, let us remark that as t goes to 0, µ + tδµ ⇀ µ. f t and g t converge uniformly towards f and g according to Proposition 13 <ref type="bibr" coords="10,327.45,674.58,76.92,9.53" target="#b38">[Feydy et al., 2019]</ref>.</s><s coords="10,411.75,675.47,26.28,8.64">we get</s></p><formula xml:id="formula_41" coords="10,399.27,692.52,74.46,13.28">∆ first part t = ⟨δµ, f ⟩ Simular to analysis ∆ first part t := lim t→0 1 t (W ε (µ t , µ * ) -W ε (µ, µ * ))</formula><p><s coords="11,54.00,100.86,38.46,8.64">we define</s></p><formula xml:id="formula_42" coords="11,73.64,114.32,203.72,22.31">∆ second part t := lim t→0 1 2t (W ε (µ t , µ t ) -W ε (µ, µ))</formula><p><s coords="11,54.00,142.22,40.32,8.64">, we have:</s></p><formula xml:id="formula_43" coords="11,54.00,151.32,240.94,55.97">∆ second part t = ⟨δµ, f ′ ⟩ to be more clearly, we denote f = f µ,µ * and f ′ = f µ,µ thus, lim t→0 1 t (F ε (µ t ) -F ε (µ)) = ⟨δµ, f µ,µ * -f µ,µ ⟩.</formula><p><s coords="11,54.00,212.90,117.44,9.65">So the first variation of F ε is:</s></p><formula xml:id="formula_44" coords="11,133.76,228.53,84.68,22.31">δF ε δµ = f µ,µ * -f µ,µ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="11,54.00,276.88,128.69,9.81">B.2 Proof of Proposition 2</head><p><s coords="11,54.00,291.72,243.00,8.64;11,54.00,302.68,65.06,8.64">Following the lines of our proof in Theorem 1, we give the following proof.</s><s coords="11,54.00,316.94,243.00,8.96;11,54.00,327.97,243.00,8.74;11,54.00,338.92,50.60,8.74;11,104.60,337.35,4.08,6.12;11,112.30,338.92,184.70,9.65;11,54.00,349.88,34.42,9.65;11,88.41,348.31,4.08,6.12;11,92.99,349.88,96.68,8.74">Lemma 3. (Fréchet derivative of entropy-regularized Wasserstein distance) Let ε &gt; 0. We shall fix in the following a measure µ * and let (f µ,µ * , g µ,µ * ) be the W ε potentials of W ε (µ, µ * ) according to lemma 1.</s><s coords="11,193.53,350.02,103.47,8.59;11,54.00,360.84,110.90,8.74">Consider the infinitesimal transport T (x) = x + λϕ.</s><s coords="11,170.61,360.98,126.39,8.59;11,54.00,371.94,139.21,8.59">We have the Fréchet derivative under this particular perturbation:</s></p><formula xml:id="formula_45" coords="11,59.06,386.97,237.94,59.18">d dλ W ε (T # µ, µ * )| λ=0 = lim λ→0 W ε (T # µ, µ * ) -W ε (µ, µ * ) λ = X ∇f µ,µ * (x)ϕ(x)dµ(x).<label>(23)</label></formula><p><s coords="11,54.00,455.20,24.74,8.59">Proof.</s><s coords="11,83.72,455.06,213.28,9.65;11,54.00,466.02,44.74,9.65;11,98.74,464.44,4.08,6.12;11,103.32,466.02,43.06,8.96">let f = f µ,µ * and g = g µ,µ * be the W ε -potentials 1 to W ε (µ, µ * ) for short.</s><s coords="11,149.72,466.02,147.28,8.96;11,54.00,477.30,53.36,8.64">By 3 and the optimality of (f, g), we have follows:</s></p><formula xml:id="formula_46" coords="11,114.23,492.08,122.54,11.72">W ε (µ, µ * ) = ⟨f, µ⟩ + ⟨g, µ * ⟩.</formula><p><s coords="11,54.00,511.34,243.00,8.96;11,54.00,522.30,61.39,9.65;11,115.38,520.72,4.08,6.12;11,119.96,522.30,85.50,8.96">However, (f, g) are not necessarily the optimal dual variables for W ε (T # µ, µ * ), recall the lemma 2:</s></p><formula xml:id="formula_47" coords="11,54.60,537.40,241.79,11.72">W ε (T # µ, µ * ) ≥ ⟨f, T # µ⟩ + ⟨g, µ * ⟩ -ε⟨h -1, T # µ ⊗ µ * ⟩,</formula><p><s coords="11,54.00,556.98,24.34,8.64;11,85.02,562.24,5.71,6.12;11,93.91,556.63,43.10,8.77;11,137.01,555.08,4.08,6.12;11,141.59,556.66,136.44,9.65;11,278.03,555.08,4.08,6.12;11,282.61,556.66,14.39,8.74;11,54.00,567.62,33.25,8.96">where X h(x, y)dµ * (y) = 1 and hence ⟨h-1, T # µ⊗µ * ⟩ = 0. Thus:</s></p><formula xml:id="formula_48" coords="11,84.78,582.72,181.44,11.72">W ε (T # µ, µ * ) -W ε (µ, µ * ) ≥ ⟨f, T # µ -µ⟩.</formula><p><s coords="11,54.00,602.30,243.00,8.64;11,54.00,613.26,73.60,8.64">Use the change-of-variables formula of the push-forward measure to obtain:</s></p><formula xml:id="formula_49" coords="11,71.58,627.09,209.04,50.60">1 λ ⟨f, T # µ -µ⟩ = 1 λ X ((f • T )(x) -f (x))dµ(x) = X ∇f (x + λ ′ ϕ(x))ϕ(x)dµ(x),</formula><p><s coords="11,54.00,684.24,33.11,8.96;11,87.11,682.66,2.30,6.12;11,93.54,684.24,165.03,8.96">where λ ′ ∈ [0, λ] is from the mean value theorem.</s><s coords="11,263.07,684.56,33.93,8.64;11,54.00,695.20,243.00,8.96;11,315.00,56.58,243.00,9.53">Here we assume ∇f is Lipschitz continuous follow Proposition 12 in <ref type="bibr" coords="11,315.00,56.58,77.82,9.53" target="#b38">[Feydy et al., 2019]</ref> and Lemma A.4 form <ref type="bibr" coords="11,482.52,56.58,71.19,9.53" target="#b104">[Shen et al., 2020]</ref>.</s></p><p><s coords="11,315.00,68.44,36.75,8.64">We have:</s></p><formula xml:id="formula_50" coords="11,342.95,83.40,187.10,23.97">lim λ→0 1 λ ⟨f, T # µ -µ⟩ = X ∇f (x)ϕ(x)dµ(x).</formula><p><s coords="11,315.00,114.51,28.21,8.64">Hence:</s></p><formula xml:id="formula_51" coords="11,317.43,129.45,238.15,22.60">lim λ→0 1 λ (W ε (T # µ, µ * ) -W ε (µ, µ * )) ≥ X ∇f (x)ϕ(x)dµ(x).</formula><p><s coords="11,315.00,159.78,58.34,8.96;11,374.42,158.21,2.30,6.12;11,379.47,159.78,21.40,8.96;11,401.23,158.21,2.30,6.12;11,406.29,159.78,140.77,9.65;11,547.06,158.21,4.08,6.12;11,551.64,159.78,6.36,8.74;11,315.00,171.06,35.34,8.64">Similarly, let f ′ and g ′ be the W ε potentials to W ε (T # µ, µ * ), we have:</s></p><formula xml:id="formula_52" coords="11,333.23,186.95,206.55,11.72">W ε (µ, µ * ) ≥ ⟨f ′ , µ⟩ + ⟨g, µ * ⟩ -ε⟨h -1, µ ⊗ µ * ⟩,</formula><p><s coords="11,315.00,207.63,24.34,8.64;11,346.16,212.90,5.71,6.12;11,355.05,207.28,43.10,8.77;11,398.15,205.74,4.08,6.12;11,402.73,207.31,126.06,8.96;11,528.79,205.74,4.08,6.12;11,533.37,207.31,24.63,8.74;11,315.00,218.59,22.69,8.64">where X h(x, y)dµ * (y) = 1 and hence ⟨h-1, µ⊗µ * ⟩ = 0. Thus:</s></p><formula xml:id="formula_53" coords="11,344.10,234.48,184.79,11.72">W ε (T # µ, µ * ) -W ε (µ, µ * ) ≤ ⟨f ′ , T # µ -µ⟩.</formula><p><s coords="11,315.00,255.16,243.00,8.64;11,315.00,266.12,83.31,8.64">Same as above, use the change-of-variables formula and the mean value theorem:</s></p><formula xml:id="formula_54" coords="11,328.73,281.08,216.75,23.97">1 λ ⟨f ′ , T # µ -µ * ⟩ = X ∇f ′ (x + λ ′ ϕ(x))ϕ(x)dµ(x),</formula><p><s coords="11,315.00,312.34,22.69,8.64">Thus:</s></p><formula xml:id="formula_55" coords="11,358.73,320.08,155.54,49.02">lim λ→0 1 λ (W ε (T # µ, µ * ) -W ε (µ, µ * )) ≤ X lim λ→0 ∇f ′ (x + λ ′ ϕ(x))ϕ(x)dµ(x).</formula><p><s coords="11,315.00,373.94,68.39,8.96;11,384.47,372.37,2.30,6.12;11,391.34,373.94,123.05,8.96;11,515.47,372.37,2.30,6.12;11,523.99,373.94,34.01,8.96;11,315.00,384.90,182.72,9.65;11,498.79,383.33,2.30,6.12;11,501.59,384.90,29.49,8.74;11,531.08,383.33,2.30,6.12;11,533.88,384.90,24.12,8.74;11,315.00,396.18,42.88,8.64">Assume that ∇f ′ is Lipschitz continuous and f ′ → f as λ → 0. Consequently we have lim λ→0 ∇f ′ (x + λ ′ ϕ(x)) and hence:</s></p><formula xml:id="formula_56" coords="11,315.00,411.12,252.66,23.97">lim λ→0 1 λ (W ε (T # µ, µ * ) -W ε (µ, µ * )) = X ∇f (x)ϕ(x)dµ(x).</formula><p><s coords="11,324.96,442.55,129.69,8.64">According to lemma 3, we have:</s></p><formula xml:id="formula_57" coords="11,317.39,460.59,254.15,70.92">d dλ Fε(T # µ) λ=0 = X ∇fµ,µ * (x)ϕ(x)dµ(x) - 1 2 • X 2∇fµ,µ(x)ϕ(x)dµ(x) = X ∇fµ,µ * (x)ϕ(x)dµ - X ∇fµ,µ(x)ϕ(x)dµ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="11,315.00,558.32,113.52,9.81">B.3 Proof of theorem 2</head><p><s coords="11,315.00,573.66,24.74,8.59">Proof.</s><s coords="11,344.72,573.52,176.35,9.30;11,521.07,571.95,4.15,6.12;11,528.66,573.52,29.34,9.30;11,315.00,587.94,202.16,8.64;11,521.43,584.83,21.06,6.12;11,527.60,593.10,8.72,6.12;11,543.69,587.62,14.31,8.74;11,315.00,600.22,226.85,8.96">First, we define Ψ(µ) = hdµ where h : R d → R is an arbitrary bounded and continuous function and δΨ(µ) δµ (x) denotes the first variation of functional Ψ at µ satisfying:</s></p><formula xml:id="formula_58" coords="11,352.17,618.17,180.29,22.31">δΨ(µ) δµ (x)ξ(x)dx = lim ϵ→0 Ψ(µ + ϵξ) -Ψ(µ) ϵ</formula><p><s coords="11,315.00,650.03,243.00,8.96;11,315.00,661.31,30.20,8.64">for all signed measure ξ(x)dx = 0. We also have the following:</s></p><formula xml:id="formula_59" coords="11,374.20,679.49,125.79,22.50">δΨ(µ) δµ (•) = δ hdµ δµ (•) = h(•)</formula><p><s coords="12,63.96,57.16,172.57,9.65">Assume µ t is a flow satisfies the following:</s></p><formula xml:id="formula_60" coords="12,132.97,77.21,85.06,9.65">∂ t Ψ[µ t ] = (LΨ)[µ t ],</formula><p><s coords="12,54.00,97.58,26.83,8.64">where,</s></p><formula xml:id="formula_61" coords="12,60.81,116.22,236.19,33.14">LΨ[µ t ] = -⟨∇ δF ε (µ t ) δµ (x), ∇ x δΨ(µ t ) δµ (x)⟩µ t (x)dx<label>(24)</label></formula><p><s coords="12,54.00,160.57,154.15,9.65">Notably, µ t is a solution of equation 2.</s></p><p><s coords="12,63.96,172.59,51.05,8.96;12,107.41,177.09,3.01,6.12;12,119.10,172.90,177.90,8.64;12,54.00,183.54,49.46,8.96">Next, let μM t be the distribution produced by the equation 14 at time t.</s><s coords="12,107.59,183.54,121.79,8.96;12,221.78,188.06,3.97,6.12;12,234.03,183.54,62.97,9.65;12,54.00,194.50,157.43,8.96;12,203.83,199.01,3.01,6.12;12,215.31,194.50,81.69,9.65;12,54.00,205.46,111.29,9.65;12,165.29,203.89,7.60,6.12;12,165.29,209.97,3.01,6.12;12,174.13,204.88,108.98,10.23">Under mild assumption of μM 0 ⇀ µ 0 , we want to show that the mean-field limit of μM t as M → ∞ is µ t by showing that lim M →∞ Ψ(µ M t ) = Ψ(µ t ) <ref type="bibr" coords="12,218.79,204.88,60.04,9.53">[Folland, 1999]</ref>.</s><s coords="12,63.96,217.48,128.08,8.96;12,184.44,221.98,3.01,6.12;12,195.62,217.80,101.38,8.64;12,54.00,228.44,100.90,8.96">For the measure valued flow μM t equation 14, the infinitesimal generator of Ψ w.r.t.</s><s coords="12,158.78,228.44,12.81,8.74;12,163.99,232.94,3.01,6.12;12,175.32,228.76,84.15,8.64">μM t is defined as follows:</s></p><formula xml:id="formula_62" coords="12,93.78,247.79,163.44,24.32">(LΨ)[μ M t ] := lim ϵ→0 + Ψ(μ M t+ϵ ) -Ψ(μ M t ) ϵ ,</formula><p><s coords="12,54.00,281.72,243.00,8.64;12,54.00,292.68,36.80,8.64">According to the definition of first variation, it can be calculated that</s></p><formula xml:id="formula_63" coords="12,69.05,309.37,211.71,56.80">(LΨ)[μ M t ] = lim ϵ→0 + Ψ[ M i=1 1 M δ x i t+ϵ ] -Ψ( M i=1 1 M δ x i t ) ϵ = δΨ(μ M t ) δµ (x) M i=1 1 M ∂tρ(x i t )dx</formula><p><s coords="12,63.96,377.30,233.04,8.82;12,54.00,388.12,54.43,9.65;12,108.43,386.55,7.60,6.12;12,108.43,392.63,3.01,6.12;12,117.28,388.12,96.19,9.65">Then we adopt the Induction over the Continuum to prove lim n→∞ Ψ(μ M t ) = Ψ(µ t ) for all t &gt; 0.</s><s coords="12,216.73,388.12,45.10,9.30;12,261.82,386.55,6.12,6.12;12,270.99,388.44,26.01,8.64;12,54.00,399.40,243.00,8.64;12,54.00,410.18,243.00,8.82;12,54.00,420.42,172.10,9.53">Here t ∈ R + satisfy the requirement of well ordering and the existence of a greatest lower bound for non-empty subsets, so Induction over the Continuum is reasonable <ref type="bibr" coords="12,155.00,420.42,66.81,9.53" target="#b64">[Kalantari, 2007]</ref>.</s></p><p><s coords="12,63.96,433.34,7.47,8.64">1.</s><s coords="12,74.52,433.02,142.55,8.96;12,209.48,437.53,3.97,6.12;12,221.09,433.02,54.49,9.65">As for t = 0, our assumption of μM 0 ⇀ µ 0 suffice.</s><s coords="12,63.96,445.04,90.13,8.96;12,154.10,443.46,4.08,6.12;12,158.68,445.04,131.26,8.96;12,289.93,443.46,4.08,6.12;12,294.51,445.35,2.49,8.64;12,54.79,455.99,12.81,8.74;12,60.00,460.50,3.01,6.12;12,71.61,455.99,74.22,9.65">2. For the case of t = t * , we first hypothesis that for t &lt; t * , μM t ⇀ µ t as M → ∞.</s><s coords="12,148.93,455.99,57.55,8.96;12,206.47,454.42,4.08,6.12;12,213.55,456.31,35.34,8.64;12,116.85,617.74,180.15,8.64;12,54.00,628.38,76.67,9.65;12,130.67,626.81,7.60,6.12;12,130.67,632.89,3.01,6.12;12,139.52,628.38,157.48,9.65;12,54.00,639.34,32.43,8.96;12,78.83,643.85,3.01,6.12;12,93.02,639.34,52.28,9.65;12,137.70,643.86,3.97,6.12;12,151.89,639.34,28.27,9.65">Then for t &lt; t * we have: <ref type="formula" coords="12,116.85,617.74,3.87,8.64" target="#formula_0">1</ref>) and (2), we can reach to the conclusion that lim M →∞ Ψ(µ M t ) = Ψ(µ t ) for all t. which indicates that μM t ⇀ µ t if μM 0 ⇀ µ 0 .</s><s coords="12,187.44,639.34,109.57,9.65;12,54.00,650.62,243.00,8.64;12,54.00,661.26,101.29,8.96;12,147.69,665.77,3.97,6.12;12,159.78,661.58,137.22,8.64;12,54.00,672.22,242.50,9.65;12,54.00,683.18,49.49,8.96">Since µ t solves the partial differential equation 10, we conclude that the path of equation 14 starting from μM 0 weakly converges to a solution of the partial differential equation equation 10 starting from µ 0 as M → ∞.</s></p><formula xml:id="formula_64" coords="12,54.00,475.35,243.00,151.03">lim M →∞ (LΨ)[μ M t ] = lim M →∞ δΨ(μ M t ) δµ (x) M i=1 1 M ∂tρ(x i t )dx = -lim M →∞ ⟨∇ δFε(µt) δµ (x), ∇x δΨ(μ M t ) δµ (x)⟩μ M t (x)dx = -⟨∇ δFε(µt) δµ (x), ∇x δΨ(µt) δµ (x)⟩µt(x)dx. Because lim M →∞ Ψ(μ M 0 ) = Ψ(µ 0 ) at t = 0 and lim M →∞ (∂ t Ψ)[μ M t ] = (∂ t Ψ)[µ t ] for all t &lt; t * , we have lim M →∞ Ψ(μ M t * ) = Ψ(µ t * ). Combining (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,315.00,56.44,123.44,9.81">B.4 Descending property</head><p><s coords="12,315.00,70.82,243.00,8.96;12,315.00,81.85,236.43,9.65">Proposition 3. Consider the Sinkhorn gradient flow 10, the differentiation of F ε (µ t ) with respect to the time t satisfies:</s></p><formula xml:id="formula_65" coords="12,338.43,97.77,219.57,26.43">dF ε (µ t ) dt = - ∇ δF ε (µ t ) δµ t 2 dµ t ≤ 0<label>(25)</label></formula><p><s coords="12,315.00,131.68,24.74,8.59">Proof.</s><s coords="12,344.72,131.54,213.28,9.65;12,315.00,142.81,134.07,8.64">By substituting Ψ(•) = F ε (•) in equation 24, we directly reach to the above equality.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,315.00,175.49,173.80,10.75">C Minibatch Optimal Transport</head><p><s coords="12,315.00,191.98,243.00,8.64;12,315.00,202.94,243.00,8.64;12,315.00,213.90,243.00,8.64;12,315.00,223.96,243.00,9.53;12,315.00,235.82,25.73,8.64">For large datasets, the computation and storage of the optimal transport plan can be challenging due to OT's cubic time and quadratic memory complexities relative to the number of samples <ref type="bibr" coords="12,361.09,223.96,56.91,9.53">[Cuturi, 2013;</ref><ref type="bibr" coords="12,420.84,224.68,86.16,8.82" target="#b46">Genevay et al., 2016;</ref><ref type="bibr" coords="12,509.84,224.68,48.16,8.82;12,315.00,235.82,21.44,8.64" target="#b91">Peyré et al., 2017]</ref>.</s><s coords="12,344.46,235.82,213.54,8.64;12,315.00,246.77,146.23,8.64">The minibatch approximation offers a viable solution for enhancing calculation efficiency.</s><s coords="12,466.08,246.77,91.91,8.64;12,315.00,257.73,243.00,8.64;12,315.00,267.80,223.27,9.53">Theoretical analysis of using the minibatch approximation for transportation plans is provided by <ref type="bibr" coords="12,367.45,267.80,81.25,9.53" target="#b34">[Fatras et al., 2019;</ref><ref type="bibr" coords="12,452.31,268.51,81.57,8.82">Fatras et al., 2021b]</ref>.</s><s coords="12,544.72,268.69,13.28,8.64;12,315.00,279.65,243.00,8.64;12,315.00,290.61,243.00,8.64;12,315.00,301.57,243.00,8.64;12,315.00,311.63,243.00,9.53;12,315.00,322.59,243.00,9.53;12,315.00,334.45,25.73,8.64">Although minibatch OT introduces some errors compared to the exact OT solution, its efficiency in computing approximate OT is clear, and it has seen successful applications in domains like domain adaptation <ref type="bibr" coords="12,452.22,311.63,105.78,9.53" target="#b23">[Damodaran et al., 2018;</ref><ref type="bibr" coords="12,315.00,323.31,78.47,8.82">Fatras et al., 2021a]</ref> and generative modeling <ref type="bibr" coords="12,495.43,322.59,62.57,9.31;12,315.00,334.45,21.44,8.64" target="#b48">[Genevay et al., 2018]</ref>.</s><s coords="12,324.96,344.51,233.04,9.53;12,315.00,356.36,243.00,8.64;12,315.00,367.32,243.00,8.64;12,315.00,377.39,243.00,9.53;12,315.00,389.24,243.00,8.64;12,315.00,400.20,18.54,8.64">More recently, <ref type="bibr" coords="12,386.82,344.51,95.23,9.53" target="#b95">[Pooladian et al., 2023;</ref><ref type="bibr" coords="12,485.15,345.23,68.20,8.82" target="#b114">Tong et al., 2023</ref>] introduced OT-CFM and empirically demonstrated that using minibatch approximation of optimal transport in flow matching methods <ref type="bibr" coords="12,390.72,377.39,67.10,9.53" target="#b73">[Liu et al., 2023;</ref><ref type="bibr" coords="12,460.25,378.10,81.49,8.82" target="#b71">Lipman et al., 2023]</ref> can straighten the flow's trajectory and yield more consistent samples.</s><s coords="12,339.09,400.20,218.91,8.64;12,315.00,411.16,243.00,8.64;12,315.00,422.12,23.52,8.64">OT-CFM specifically focuses on minibatch initial and target samples, continuing to use random linear interpolation paths.</s><s coords="12,342.67,421.80,215.33,9.65;12,315.00,433.08,218.99,8.64">In contrast, NSGF leverages minibatch W ε -potentials to construct Sinkhorn gradient flows in minibatches.</s><s coords="12,542.51,433.08,15.49,8.64;12,315.00,444.04,243.00,8.64;12,315.00,454.99,243.00,8.64;12,315.00,465.95,83.02,8.64">Our method also involves performing velocity field matching on the flow's discretized form, marking a separate and innovative direction in the field.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,315.00,487.67,66.10,10.75">D NSGF++</head><p><s coords="12,315.00,504.16,243.00,8.64;12,315.00,515.12,243.00,8.64;12,315.00,526.08,243.00,8.64;12,315.00,537.04,115.29,8.64">We introduce a two-phase NSGF++ algorithm that initially employs the Sinkhorn gradient flow for rapid approximation to the image manifold, followed by sample refinement using a straightforward straight flow.</s><s coords="12,433.35,537.04,124.65,8.64;12,315.00,548.00,89.95,8.64">The NSGF++ model comprises three key components:</s></p><p><s coords="12,326.46,562.19,193.33,8.96">• An NSGF model trained for T ≤ 5 time steps.</s></p><p><s coords="12,326.46,576.92,231.55,9.65;12,334.93,587.88,223.07,8.96">• A phase-transition time predictor, denoted as t ϕ : X → [0, 1], which facilitates the transition from NSGF to NSF.</s></p><p><s coords="12,326.46,602.92,231.54,8.64;12,334.93,613.88,223.07,8.64;12,334.93,624.52,153.63,9.65;12,315.00,639.35,151.93,8.64">• A Neural Straight Flow (NSF) model, trained through velocity field matching on a linear interpolation straight flow X t ∼ (1 -t)P 0 + tP 1 , t ∈ [0, 1]. the detailed algorithm is outlined in 3.</s></p><p><s coords="12,324.96,650.31,233.03,8.64;12,315.00,660.95,243.00,9.65;12,315.00,670.76,125.20,11.47">In the inference process of NSGF++, we initially apply the NSGF with fewer than 5 NFE, starting from X 0 ∼ P 0 , to obtain an intermediate output XT .</s><s coords="12,443.17,673.60,114.83,8.64;12,315.00,684.24,111.58,9.65">This output is then processed using the time predictor t ϕ .</s></p><formula xml:id="formula_66" coords="13,63.96,327.85,476.45,143.88">i ∼ µ 0 , Ỹi ∼ µ * , i = 1, 2, • • • n. t ∼ U(0, 1). X t = t Ỹi + (1 -t) X0 i L(ϕ) = E t∈U (0,1),Xt∼Pt ||t -t ϕ (X t )|| 2 . ϕ ← ϕ -γ ′ ∇ ϕ L (ϕ) . / * NSF model * / while Training do X0 i ∼ µ 0 , Ỹi ∼ µ * , i = 1, 2, • • • n. t ∼ U(0, 1). X t = t Ỹi + (1 -t) X0 i . L NSF (δ) ← u δ (t, X t ) -Ỹi -X0 i 2 . δ ← δ -γ ′′ ∇ δ L NSF (δ).</formula><p><s coords="13,63.96,475.01,475.74,9.19;13,98.76,485.73,108.02,8.17;13,54.00,518.25,89.92,12.17">Output: v θ parameterize the time-varying velocity field of NSGF, t ϕ parameterize the phase trainsition time predictor, u δ parameterize the NSF model from the state t ϕ ( Xt ).</s><s coords="13,148.78,521.09,148.22,8.64;13,54.00,532.05,7.47,8.64">The detailed algorithm is outlined in 4.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,54.00,554.23,84.83,10.75;13,54.00,570.91,111.22,9.81">E Experimrnts E.1 2D simulated data</head><p><s coords="13,54.00,585.03,243.00,9.53;13,54.00,596.71,209.56,8.82">For the 2D experiments, we closely follow <ref type="bibr" coords="13,273.45,585.03,23.55,8.64;13,54.00,596.71,67.94,8.82" target="#b114">[Tong et al., 2023]</ref> and the released code at <ref type="url" coords="13,274.85,596.88,22.15,8.64;13,54.00,607.84,193.90,8.64" target="https://github.com/atong01/conditional-flow-matching">https: //github.com/atong01/conditional-flow-matching</ref></s><s coords="13,274.87,607.84,22.13,8.64;13,54.00,618.80,243.00,8.64;13,54.00,629.76,243.00,8.64;13,54.00,640.72,243.00,8.64;13,54.00,651.68,27.95,8.64">(code released under MIT license), and use the same synthetic datasets and the 2-Wasserstein distance between the test set and samples simulated using NSGF as the evaluation metric.</s><s coords="13,88.80,651.68,208.20,8.64;13,54.00,662.64,243.00,8.64;13,54.00,673.60,243.00,8.64">We use 1024 samples in the test set since we find the We use a simple MLP with 3 hidden layers and 256 hidden units to parameterize the velocity matching networks.</s><s coords="13,54.00,684.56,243.00,8.64;13,54.00,695.51,109.90,8.64">We use batch size 256 and 10/100 steps with a uniform schedule at sampling time.</s><s coords="13,170.63,695.51,126.36,8.64;13,315.00,521.09,243.00,8.64;13,315.00,532.05,83.84,8.64">For both Nerual gradient-flow-based models and Nerual ODE-based Models, we train for 20000 steps in total.</s><s coords="13,405.50,532.05,152.50,8.64;13,315.00,543.01,243.00,8.64;13,315.00,553.97,243.00,8.64;13,315.00,564.93,165.55,8.64">Note that FM cannot be used for the 8gaussians-moons task since it requires a Gaussian source, but we still conducted experiments with the algorithm and found competitive experimental results.</s><s coords="13,491.46,564.93,66.54,8.64;13,315.00,575.89,243.00,8.64;13,315.00,586.85,243.00,8.64;13,315.00,597.80,243.00,8.64;13,315.00,607.87,77.45,9.53">We believe that this is because FM is essentially very close to 1-RF in its algorithmic design, and that the Gaussian source condition can be meaningfully relaxed in practice, as confirmed in <ref type="bibr" coords="13,315.00,607.87,73.16,9.53" target="#b114">[Tong et al., 2023]</ref>.</s><s coords="13,396.25,608.76,161.75,8.64;13,315.00,619.72,243.00,8.64;13,315.00,630.68,82.74,8.64">The experiments are run using one 3090 GPU and take approximately less than 60 minutes (for both training and testing).</s></p><p><s coords="13,324.96,651.68,233.04,8.64;13,315.00,662.64,243.00,8.64;13,315.00,672.70,243.00,9.53;13,315.00,684.38,66.89,8.82">For the neural gradient-flow-based models, we solely implemented the EPT without the outer loop, as the outer loop can be likened to a GAN-like distillation approach <ref type="bibr" coords="13,518.72,672.70,34.92,8.64;13,315.00,684.38,62.60,8.82" target="#b50">[Goodfellow et al., 2014]</ref>.</s><s coords="13,384.87,683.66,173.13,9.53;13,315.00,695.20,243.00,8.96">Notably, the original EPT <ref type="bibr" coords="13,488.38,683.66,69.62,9.53" target="#b44">[Gao et al., 2022]</ref> recommends iterating for 20, 000 rounds with an exceedingly</s></p><formula xml:id="formula_67" coords="14,63.96,150.69,234.48,43.14">i = Xt i + ηv θ Xt i , t , i = 1, 2, • • • n. / * phase trainsition time predict * / t = t ϕ ( XT i ).</formula><p><s coords="14,63.96,205.81,109.59,8.45;14,286.91,205.81,11.54,8.45;14,65.84,215.41,59.21,10.86">/ * NSF refine phase * / T = (1 -t)/ω.</s><s coords="14,71.53,227.88,85.56,10.86;14,151.79,234.78,2.72,5.90;14,158.60,227.88,49.50,11.74">X = ODEsolver( XT i , u δ , t, 1, T ).</s><s coords="14,63.96,240.19,98.71,11.07">Output: X as the results.</s><s coords="14,54.00,276.73,243.00,8.64;14,54.00,287.69,243.00,8.64;14,54.00,298.64,136.52,8.64">small step size; however, to ensure a fair comparison, we employed the same number of steps as the other methods while adapting the step size accordingly.</s><s coords="14,193.58,298.64,103.42,8.64;14,54.00,309.60,243.00,8.64;14,54.00,319.67,243.00,9.53;14,54.00,331.20,243.00,8.96">It's worth mentioning that for the JKO-Flow, we used the recommended parameter setting of 10 steps, as suggested in <ref type="bibr" coords="14,190.73,319.67,70.44,9.53" target="#b32">[Fan et al., 2022]</ref>, but we also provide results for 100 steps for comparative purposes.</s><s coords="14,54.00,342.48,243.00,8.64;14,54.00,353.44,243.00,8.64;14,54.00,364.40,124.38,8.64">All the results for Neural Gradient flow-based models were trained and sampled following the standard procedures outlined in their respective papers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="14,54.00,383.86,134.85,9.81">E.2 Image benchmark data</head><p><s coords="14,54.00,399.43,243.00,8.64;14,54.00,410.39,243.00,8.64;14,54.00,421.35,185.97,8.64">For the MNIST/CIFAR-10 experiments, we summarize the setup of our NSGF++ model here, where the exact parameter choices can be seen in the source code.</s><s coords="14,247.95,421.35,49.05,8.64;14,54.00,431.99,243.00,9.65;14,54.00,442.37,243.00,9.53;14,54.00,453.91,243.00,8.96;14,54.00,465.18,42.26,8.64">For the calculation of W ε -potentials, we use the GeomLoss package <ref type="bibr" coords="14,54.00,442.37,83.09,9.53" target="#b38">[Feydy et al., 2019]</ref> with blur = 0.5, 1.0 or 2.0, scaling = 0.80, 0.85 or 0.95 depends on learning rate of Sinkhorn gradient flow.</s><s coords="14,100.94,465.18,196.05,8.64;14,54.00,476.14,113.18,8.64">We find using an incremental lr scheme will improve training performance.</s><s coords="14,173.26,476.14,123.74,8.64;14,54.00,487.10,104.12,8.64">More detailed experiments we will leave for future work.</s><s coords="14,161.19,487.10,135.81,8.64;14,54.00,497.74,186.09,9.65">We used the Adam optimizer with β 1 = 0.9, β 2 = 0.999 and no weight decay.</s><s coords="14,246.59,498.06,50.41,8.64;14,54.00,509.02,189.44,8.64">Here we list different part of our NSGF++ model separately.</s><s coords="14,246.50,509.02,50.50,8.64;14,54.00,519.08,226.29,9.53">First, we use the UNet architecture from <ref type="bibr" coords="14,163.34,519.08,112.66,9.53" target="#b29">[Dhariwal and Nichol, 2021]</ref>.</s><s coords="14,283.32,519.98,13.69,8.64;14,54.00,530.94,243.00,8.64;14,54.00,541.90,243.00,8.64">For MNIST, we use channels = 32, depth = 1, channels multiple = [1, 2, 2], heads = 1, attention resolution = 16, dropout = 0.0.</s><s coords="14,54.00,552.86,243.00,8.64;14,54.00,563.81,243.00,8.64;14,54.00,574.77,144.40,8.64">For CIFAR-10, we use channels = 128, depth = 2, channels multiple = [1, 2, 2, 2], heads = 4, heads channels = 64, attention resolution = 16, dropout = 0.0.</s><s coords="14,203.64,574.77,93.36,8.64;14,54.00,585.73,182.64,8.64">We use the same UNet architecture in our neural straight flow model.</s></p><p><s coords="14,63.96,596.88,233.03,8.64;14,54.00,607.84,243.00,8.64;14,54.00,618.80,243.00,8.64;14,54.00,629.76,36.79,8.64">For the phase transition time predictor, we employed an efficiently designed convolutional neural network (CNN) capable of achieving satisfactory results while optimizing training time.</s><s coords="14,97.97,629.76,199.03,8.64;14,54.00,640.72,243.00,8.64;14,54.00,651.68,135.84,8.64">The CNN used in our study consists of a structured architecture featuring four convolutional layers with filter depths of 32, 64, 128, and 256.</s><s coords="14,192.90,651.68,104.10,8.64;14,54.00,662.64,243.00,8.64;14,54.00,673.60,243.00,8.64;14,54.00,684.56,22.97,8.64">Each layer uses a 3x3 kernel, a stride of 1, and padding of 1, coupled with ReLU activation and 2x2 average pooling for effective feature downsampling.</s><s coords="14,80.04,684.56,216.95,8.64;14,54.00,695.51,243.00,8.64;14,324.96,436.00,233.04,8.64;14,315.00,446.96,243.00,8.64;14,315.00,457.92,222.86,8.64">The network culminates in a fully connected layer that transforms the flattened features into a single value, further For the MNIST/CIFAR-10 experiments, a considerable amount of storage space is required when establishing the trajectory pool during the first phase of the algorithm.</s><s coords="14,544.31,457.92,13.69,8.64;14,315.00,468.88,243.00,8.64;14,315.00,479.84,243.00,8.64;14,315.00,490.80,243.00,8.64;14,315.00,501.76,243.00,8.64;14,315.00,512.72,243.00,8.64;14,315.00,523.68,94.62,8.64">For MNIST dataset, setting the batch size to 256 and saving 1500 batches 5-step minibatch Sinkhorn gradient flow trajectories requires less than 20GB of storage space and with CIFAR-10, setting the batch size to 128 and saving 2500 batches 5-step minibatch Sinkhorn gradient flow trajectories requires about 45GB of storage space.</s><s coords="14,414.39,523.68,143.61,8.64;14,315.00,534.63,243.00,8.64;14,315.00,545.59,243.00,8.64">In situations where storage space is limited, we suggest dynamically adding and removing trajectories in the trajectory pool to meet the training requirements.</s><s coords="14,315.00,556.55,243.00,8.64;14,315.00,567.51,243.00,8.64;14,315.00,578.47,22.42,8.64">Identifying a more effective trade-off between training time and storage space utilization is a direction for future improvement.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="14,315.00,599.40,195.25,9.81">E.3 Supplementary experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="14,315.00,616.05,77.49,8.96">2D simulated data</head><p><s coords="14,315.00,629.76,243.00,8.64;14,315.00,640.72,243.00,8.64;14,315.00,651.68,170.56,8.64">In our supplementary materials, we present additional results from 2D simulated data to demonstrate the efficiency of the NSGF++ model in Figure <ref type="figure" coords="14,423.13,651.68,4.98,8.64" target="#fig_6">6</ref> and Figure <ref type="figure" coords="14,478.09,651.68,3.74,8.64" target="#fig_7">7</ref>.</s><s coords="14,490.75,651.68,67.25,8.64;14,315.00,662.64,243.00,8.64;14,315.00,673.60,243.00,8.64;14,315.00,684.56,243.00,8.64;14,315.00,695.51,216.35,8.64">These results indicate that NSGF++ achieves competitive performance with a more direct path and fewer steps compared to other neural Wasserstein gradient flow and flow-matching methods, highlighting its effectiveness and computational efficiency.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="15,54.00,415.77,32.66,8.96">MNIST</head><p><s coords="15,54.00,428.12,243.00,8.64;15,54.00,439.08,192.29,8.64">In our study, we include results from the MNIST dataset to showcase the efficiency of the NSGF++ model.</s><s coords="15,251.27,439.08,45.73,8.64;15,54.00,450.04,243.00,8.64;15,54.00,461.00,243.00,8.64;15,54.00,471.95,204.84,8.64">As detailed in Table <ref type="table" coords="15,89.64,450.04,3.74,8.64" target="#tab_4">3</ref>, NSGF++ achieves competitive Fréchet Inception Distances (FIDs) while utilizing only 60% of the Number of Function Evaluations (NFEs) typically required.</s><s coords="15,263.12,471.95,33.88,8.64;15,54.00,482.91,243.00,8.64;15,54.00,493.87,119.69,8.64">This underscores the model's effectiveness in balancing performance with computational efficiency.</s><s coords="15,176.66,493.87,120.34,8.64;15,54.00,504.83,243.00,8.64;15,54.00,515.79,113.61,8.64">To evaluate our results, we use the Fréchet inception distance (FID) between 10K generated samples and the test dataset.</s><s coords="15,171.44,515.79,125.56,8.64;15,54.00,526.75,242.65,8.64">Here, a smaller FID value indicates a higher similarity between generated and test samples.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="15,54.00,543.30,43.93,8.96">CIFAR-10</head><p><s coords="15,54.00,555.64,243.00,8.64;15,54.00,566.60,243.00,8.64">In our work, we present further results of the NSGF++ model on the CIFAR-10 dataset, illustrated in Figures <ref type="figure" coords="15,256.50,566.60,4.98,8.64">9</ref> and<ref type="figure" coords="15,284.54,566.60,8.30,8.64" target="#fig_1">11</ref>.</s><s coords="15,54.00,577.56,243.00,8.64;15,54.00,588.52,243.00,8.64;15,54.00,599.48,60.10,8.64">These experimental findings demonstrate that NSGF++ attains competitive performance in generation tasks, highlighting its efficacy.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="15,353.64,93.03,48.37,10.06">Algorithm</head><p><s coords="15,468.14,86.26,36.11,10.06;15,439.12,99.81,33.85,10.06;15,496.19,99.81,37.08,10.06;15,340.90,113.67,73.85,10.44">MNIST FID(↓) NFE(↓) NSGF++(ours)</s></p><p><s coords="15,448.79,114.12,14.51,10.06;15,508.92,114.12,11.61,10.06;15,346.23,126.62,63.19,11.11;15,442.99,127.66,26.12,10.06;15,506.02,127.66,17.41,10.06;15,353.00,140.16,49.65,11.11;15,448.79,141.21,14.51,10.06;15,513.11,141.21,3.23,10.06;15,352.51,154.48,50.62,11.11;15,448.79,155.52,14.51,10.06;15,506.02,155.52,17.41,10.06;15,339.74,168.02,76.17,11.11;15,448.79,169.07,14.51,10.06;15,506.02,169.07,17.41,10.06;15,353.80,181.56,48.05,11.11;15,448.79,182.61,14.51,10.06;15,506.02,182.61,17.41,10.06;15,350.14,196.93,55.37,10.06;15,445.89,196.93,20.31,10.06;15,513.11,196.93,3.23,10.06">3.8 60 SWGF <ref type="bibr" coords="15,378.47,126.62,30.95,11.11">[ 2019 ]</ref> 225.1 500 SIG <ref type="bibr" coords="15,371.70,140.16,30.95,11.11">[ 2020 ]</ref> 4.5 / FM <ref type="bibr" coords="15,372.19,154.48,30.95,11.11">[ 2023 ]</ref> 3.4 100 OT-CFM <ref type="bibr" coords="15,384.96,168.02,30.95,11.11">[ 2023 ]</ref> 3.3 100 RF <ref type="bibr" coords="15,370.90,181.56,30.95,11.11">[ 2023 ]</ref> 3.1 100 Training set 2.27 /</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,54.00,163.59,504.00,7.77;2,54.00,173.55,312.24,7.77"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s coords="2,54.00,163.59,374.19,7.77">Figure 1: Tajectories comparison between the Flow matching and the NSGF++ model in CIFAR-10 task.</s><s coords="2,430.91,163.59,127.09,7.77;2,54.00,173.55,312.24,7.77">we can see NSGF++ model quickly recovers the target structure and progressively optimizes the details in subsequent steps</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,58.98,59.77,191.33,8.96;5,66.09,73.40,207.43,7.70;5,97.51,82.81,176.00,7.65;5,97.51,90.82,176.01,9.58;5,97.51,101.52,51.64,7.65;5,66.09,111.95,122.46,7.48;5,263.31,111.95,10.21,7.48;5,66.09,120.17,61.90,7.65;5,79.19,129.58,185.76,8.55;5,94.49,140.01,61.23,5.99;5,263.31,141.50,5.10,5.99"><head>Algorithm 1 :</head><label>1</label><figDesc><div><p><s coords="5,116.53,59.77,133.78,8.96;5,66.09,73.40,207.43,7.70;5,97.51,82.81,176.00,7.65;5,97.51,90.82,176.01,9.58;5,97.51,101.52,51.64,7.65;5,66.09,111.95,122.46,7.48;5,263.31,111.95,10.21,7.48;5,66.09,120.17,61.90,7.65;5,79.19,129.58,160.25,8.55">Velocity field matching training Input : number of time steps T , batch size n, gradient flow step size η &gt; 0, empirical or samplable distribution µ 0 and µ * , neural network parameters θ, optimizer step size γ &gt; 0 / * Build trajectory pool * / while Building do / * Sample batches of size n i.i.d.</s><s coords="5,244.54,130.66,20.41,5.99;5,94.49,140.01,61.23,5.99;5,263.31,141.50,5.10,5.99">from the datasets *</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,381.65,188.52,109.70,7.77"><head/><label/><figDesc><div><p><s coords="5,381.65,188.52,109.70,7.77">Figure 2: NSGF++ framework</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,54.00,331.95,504.00,7.77;6,54.00,341.62,486.17,8.06;6,60.34,183.95,120.96,120.96"><head>Figure 3 :Figure 4</head><label>34</label><figDesc><div><p><s coords="6,54.00,331.95,196.96,7.77">Figure 3: Visualization results for 2D generated paths.</s><s coords="6,254.57,331.95,303.43,7.77;6,54.00,341.91,111.98,7.77">We show different methods that drive the particle from the prior distribution (black) to the target distribution (blue).</s><s coords="6,168.76,341.62,371.41,8.06">The color change of the flow shows the different number of steps (from blue to red means from 0 to T ).</s></p></div></figDesc><graphic coords="6,60.34,183.95,120.96,120.96" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,315.00,325.42,243.00,7.77;7,315.00,335.39,243.00,7.77;7,315.00,345.35,43.84,7.77;7,333.22,184.66,206.56,129.66"><head>Figure 5 :</head><label>5</label><figDesc><div><p><s coords="7,315.00,325.42,193.79,7.77">Figure 5: The inference result of our NSGF++ model.</s><s coords="7,511.77,325.42,46.23,7.77;7,315.00,335.39,243.00,7.77;7,315.00,345.35,43.84,7.77">The first row shows the result after 5 NSGF steps and the second row shows the final results.</s></p></div></figDesc><graphic coords="7,333.22,184.66,206.56,129.66" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,58.98,59.77,138.18,8.96;14,63.96,73.39,234.48,8.71;14,99.48,84.00,191.85,8.67;14,291.69,82.51,3.66,5.90;14,296.04,84.34,2.40,8.33;14,99.48,94.60,198.95,9.31;14,99.48,107.14,97.95,8.67;14,197.43,105.65,3.49,5.90;14,201.65,104.74,70.98,11.07;14,268.05,111.64,2.72,5.90;14,275.79,107.17,22.66,8.64;14,99.48,117.74,205.76,8.43;14,63.96,129.53,74.98,8.45;14,286.91,129.53,11.54,8.45;14,63.96,138.82,81.21,8.64;14,81.52,150.69,18.60,8.43"><head/><label/><figDesc><div><p><s coords="14,58.98,59.77,138.18,8.96;14,63.96,73.39,234.48,8.71;14,99.48,84.00,191.85,8.67;14,291.69,82.51,3.66,5.90;14,296.04,84.34,2.40,8.33;14,99.48,94.60,198.95,9.31;14,99.48,107.14,97.95,8.67;14,197.43,105.65,3.49,5.90;14,201.65,104.74,70.98,11.07;14,268.05,111.64,2.72,5.90;14,275.79,107.17,22.66,8.64;14,99.48,117.74,205.76,8.43;14,63.96,129.53,74.98,8.45;14,286.91,129.53,11.54,8.45;14,63.96,138.82,81.21,8.64;14,81.52,150.69,18.60,8.43">Algorithm 4: NSGF++ Inference Input : number of NSGF time steps T ≤ 5, NSGF++ inference step size η, NSGF velocity field v θ , phase trainsition time predictor t ϕ , NSF inference step size ω, NSF model u δ , prior samples X0 i ∼ μ0 , ODEsolver(X, model, starttime, endtime, steps) / * NSGF phase * / for t = 0, 1, • • • T do Xt+1</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="14,315.00,237.57,243.00,7.77;14,315.00,247.53,243.00,7.77;14,315.00,257.49,243.00,7.77;14,315.00,267.17,243.00,8.06;14,315.00,277.13,206.03,8.06;14,315.00,292.86,243.00,8.64;14,315.00,303.82,243.00,8.64;14,315.00,314.77,243.00,8.64;14,315.00,325.73,243.00,8.64;14,315.00,336.37,170.81,8.96;14,485.82,334.80,10.20,6.12;14,496.51,336.69,61.49,8.64;14,315.00,347.65,243.00,8.64;14,315.00,358.61,243.00,8.64;14,315.00,369.57,243.00,8.64;14,315.00,380.53,243.00,8.64;14,315.00,391.49,243.00,8.64;14,315.00,402.45,243.00,8.64;14,315.00,413.40,243.00,8.64;14,315.00,424.36,43.99,8.64;14,324.96,436.00,233.04,8.64;14,315.00,446.96,243.00,8.64;14,315.00,457.92,243.00,8.64;14,315.00,468.88,243.00,8.64;14,315.00,479.84,243.00,8.64;14,315.00,490.80,243.00,8.64;14,315.00,501.76,243.00,8.64;14,315.00,512.72,243.00,8.64;14,315.00,523.68,243.00,8.64;14,315.00,534.63,243.00,8.64;14,315.00,545.59,243.00,8.64;14,315.00,556.55,243.00,8.64;14,315.00,567.51,243.00,8.64;14,315.00,578.47,22.42,8.64;14,324.66,143.83,72.90,72.90"><head>Figure 6 :</head><label>6</label><figDesc><div><p><s coords="14,315.00,237.57,203.28,7.77">Figure6: Visualization results for 2D generated paths.</s><s coords="14,524.26,237.57,33.74,7.77;14,315.00,247.53,243.00,7.77;14,315.00,257.49,138.41,7.77">We show different methods that drive the particle from the prior distribution (black) to the target distribution (blue).</s><s coords="14,456.10,257.49,101.89,7.77;14,315.00,267.17,243.00,8.06;14,315.00,277.13,21.10,8.06">The color change of the flow shows the different number of steps (from blue to red means from 0 to T ).</s><s coords="14,338.88,277.42,182.15,7.77;14,315.00,292.86,243.00,8.64;14,315.00,303.82,140.71,8.64">We can see NSGF using fewer steps than OT-CFM refined through a sigmoid activation function for regression tasks targeting time value outputs.</s><s coords="14,462.34,303.82,95.66,8.64;14,315.00,314.77,243.00,8.64;14,315.00,325.73,119.72,8.64">This architecture is tailored for processing image inputs to predict continuous time values within a specific range.</s><s coords="14,437.77,325.73,120.23,8.64;14,315.00,336.37,170.81,8.96;14,485.82,334.80,10.20,6.12;14,496.51,336.69,2.49,8.64">The training parameters are as follows: Batch size: 128 Learning rate: 10 -4 .</s><s coords="14,502.62,336.69,55.38,8.64;14,315.00,347.65,243.00,8.64;14,315.00,358.61,131.89,8.64">For sampling, a 5-step Euler integration is applied in the NSGF phase on MNIST and CIFAR-10 datasets.</s><s coords="14,453.12,358.61,104.88,8.64;14,315.00,369.57,243.00,8.64">Training the phase transition time predictor is efficient and methodically streamlined.</s><s coords="14,315.00,380.53,243.00,8.64;14,315.00,391.49,243.00,8.64;14,315.00,402.45,69.72,8.64">Utilizing a well-structured CNN as its backbone, the model reaches peak performance in merely 20 minutes, covering 40,000 iterations.</s><s coords="14,388.59,402.45,169.40,8.64;14,315.00,413.40,243.00,8.64;14,315.00,424.36,43.99,8.64;14,324.96,436.00,233.04,8.64;14,315.00,446.96,243.00,8.64;14,315.00,457.92,222.86,8.64">This training efficiency is a significant advantage, especially for applications that demand rapid model adaptation.For the MNIST/CIFAR-10 experiments, a considerable amount of storage space is required when establishing the trajectory pool during the first phase of the algorithm.</s><s coords="14,544.31,457.92,13.69,8.64;14,315.00,468.88,243.00,8.64;14,315.00,479.84,243.00,8.64;14,315.00,490.80,243.00,8.64;14,315.00,501.76,243.00,8.64;14,315.00,512.72,243.00,8.64;14,315.00,523.68,94.62,8.64">For MNIST dataset, setting the batch size to 256 and saving 1500 batches 5-step minibatch Sinkhorn gradient flow trajectories requires less than 20GB of storage space and with CIFAR-10, setting the batch size to 128 and saving 2500 batches 5-step minibatch Sinkhorn gradient flow trajectories requires about 45GB of storage space.</s><s coords="14,414.39,523.68,143.61,8.64;14,315.00,534.63,243.00,8.64;14,315.00,545.59,243.00,8.64">In situations where storage space is limited, we suggest dynamically adding and removing trajectories in the trajectory pool to meet the training requirements.</s><s coords="14,315.00,556.55,243.00,8.64;14,315.00,567.51,243.00,8.64;14,315.00,578.47,22.42,8.64">Identifying a more effective trade-off between training time and storage space utilization is a direction for future improvement.</s></p></div></figDesc><graphic coords="14,324.66,143.83,72.90,72.90" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="15,54.00,348.07,243.00,7.77;15,54.00,358.03,243.00,7.77;15,54.00,367.99,243.00,7.77;15,54.00,377.96,243.00,7.77;15,54.00,387.92,20.17,7.77"><head>Figure 7</head><label>7</label><figDesc><div><p><s coords="15,54.00,348.07,243.00,7.77;15,54.00,358.03,169.41,7.77">Figure 7: 2-Wasserstein Distance of the generated process utilizing neural ODE-based diffusion models and NSGF.</s><s coords="15,225.36,358.03,71.64,7.77;15,54.00,367.99,243.00,7.77;15,54.00,377.96,243.00,7.77;15,54.00,387.92,20.17,7.77">The FM/SI methods reduce noise roughly linearly, while NSGF quickly recovers the target structure and progressively optimizes the details in subsequent steps.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="15,315.00,431.93,243.00,8.06;15,315.00,442.18,243.00,7.77;15,315.00,452.14,243.00,7.77;15,315.00,462.10,243.00,7.77;15,315.00,472.07,23.66,7.77;15,319.66,308.00,113.12,113.12"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc><div><p><s coords="15,315.00,431.93,243.00,8.06;15,315.00,442.18,243.00,7.77;15,315.00,452.14,111.02,7.77">Figure 8: Uncurated samples on MNIST and L2-nearest neighbors from the training set (top: Samples, bottom: real)We observe that they are significantly different.</s><s coords="15,429.62,452.14,128.38,7.77;15,315.00,462.10,243.00,7.77;15,315.00,472.07,23.66,7.77">Hence, our method generates really new samples and is not just reproducing the samples from the training set</s></p></div></figDesc><graphic coords="15,319.66,308.00,113.12,113.12" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="16,154.80,60.16,302.40,302.40"><head/><label/><figDesc><div><p/></div></figDesc><graphic coords="16,154.80,60.16,302.40,302.40" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="16,154.80,363.55,302.40,302.40"><head/><label/><figDesc><div><p/></div></figDesc><graphic coords="16,154.80,363.55,302.40,302.40" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="17,154.80,60.15,302.41,302.41"><head/><label/><figDesc><div><p/></div></figDesc><graphic coords="17,154.80,60.15,302.41,302.41" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="17,154.80,363.55,302.41,302.41"><head/><label/><figDesc><div><p/></div></figDesc><graphic coords="17,154.80,363.55,302.41,302.41" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,54.00,57.82,504.00,127.30"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="6,86.99,167.38,471.01,7.77;6,54.00,177.34,20.17,7.77">Comparison of neural gradient-flow-based methods and neural ODE-based diffusion models over five data sets with 10/100 Euler steps.</s><s coords="6,76.95,177.34,315.16,7.77">The principle of steps in JKO-flow means backward Eulerian method steps (JKO steps).</s></p></div></figDesc><table coords="6,60.78,57.82,492.44,95.54"><row><cell>Algorithm</cell><cell cols="10">2-Wasserstein distance (10 steps) 8gaussians 8gaussians-moons moons scurve checkerboard 8gaussians 8gaussians-moons moons scurve checkerboard 2-Wasserstein distance (100 steps)</cell></row><row><cell>NSGF (ours)</cell><cell>0.285</cell><cell>0.144</cell><cell>0.077</cell><cell>0.117</cell><cell>0.252</cell><cell>0.278</cell><cell>0.144</cell><cell>0.067</cell><cell>0.110</cell><cell>0.147</cell></row><row><cell>JKO-Flow</cell><cell>0.290</cell><cell>0.177</cell><cell>0.085</cell><cell>0.135</cell><cell>0.269</cell><cell>0.274</cell><cell>0.167</cell><cell>0.085</cell><cell>0.123</cell><cell>0.160</cell></row><row><cell>EPT</cell><cell>0.295</cell><cell>0.180</cell><cell>0.082</cell><cell>0.138</cell><cell>0.277</cell><cell>0.289</cell><cell>0.176</cell><cell>0.080</cell><cell>0.118</cell><cell>0.163</cell></row><row><cell>OT-CFM</cell><cell>0.289</cell><cell>0.173</cell><cell>0.088</cell><cell>0.149</cell><cell>0.253</cell><cell>0.269</cell><cell>0.165</cell><cell>0.078</cell><cell>0.127</cell><cell>0.159</cell></row><row><cell>1-RF</cell><cell>0.427</cell><cell>0.294</cell><cell>0.107</cell><cell>0.169</cell><cell>0.396</cell><cell>0.415</cell><cell>0.293</cell><cell>0.099</cell><cell>0.136</cell><cell>0.166</cell></row><row><cell>2-RF</cell><cell>0.428</cell><cell>0.311</cell><cell>0.125</cell><cell>0.171</cell><cell>0.421</cell><cell>0.430</cell><cell>0.311</cell><cell>0.121</cell><cell>0.136</cell><cell>0.170</cell></row><row><cell>3-RF</cell><cell>0.421</cell><cell>0.298</cell><cell>0.110</cell><cell>0.170</cell><cell>0.413</cell><cell>0.414</cell><cell>0.297</cell><cell>0.103</cell><cell>0.140</cell><cell>0.170</cell></row><row><cell>SI</cell><cell>0.435</cell><cell>0.324</cell><cell>0.134</cell><cell>0.187</cell><cell>0.427</cell><cell>0.411</cell><cell>0.294</cell><cell>0.096</cell><cell>0.139</cell><cell>0.166</cell></row><row><cell>FM</cell><cell>0.423</cell><cell>0.292</cell><cell>0.111</cell><cell>0.171</cell><cell>0.417</cell><cell>0.415</cell><cell>0.290</cell><cell>0.097</cell><cell>0.135</cell><cell>0.165</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,54.00,59.22,243.00,141.78"><head>Table 2 :</head><label>2</label><figDesc><div><p><s coords="7,87.05,183.26,209.95,7.77;7,54.00,193.23,204.04,7.77">Comparison of Neural Wasserstein gradient flow methods and Neural ODE-based diffusion models over CIFAR-10</s></p></div></figDesc><table coords="7,83.78,59.22,185.92,109.01"><row><cell>Algorithm</cell><cell cols="3">CIFAR 10 IS(↑) FID(↓) NFE(↓)</cell></row><row><cell>NSGF++ (ours)</cell><cell>8.86</cell><cell>5.55</cell><cell>59</cell></row><row><cell>EPT[2022]</cell><cell>/</cell><cell>46.63</cell><cell>10k</cell></row><row><cell cols="2">JKO-Flow[2022] 7.48</cell><cell>23.7</cell><cell>&gt;150</cell></row><row><cell>DGGF[2022]</cell><cell>/</cell><cell>28.12</cell><cell>110</cell></row><row><cell>OT-CFM[2023]</cell><cell>/</cell><cell>11.14</cell><cell>100</cell></row><row><cell>FM[2023]</cell><cell>/</cell><cell>6.35</cell><cell>142</cell></row><row><cell>RF[2023]</cell><cell>9.20</cell><cell>4.88</cell><cell>100</cell></row><row><cell>SI[2023]</cell><cell>/</cell><cell>10.27</cell><cell>/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,315.00,684.56,243.00,19.60"><head/><label/><figDesc><div><p><s coords="12,431.77,684.56,126.24,8.64;12,315.00,695.51,243.00,8.64;13,58.98,59.77,134.88,8.96;13,63.96,72.02,473.62,10.61;13,538.06,73.81,2.35,8.17;13,98.76,83.86,207.72,8.47">The final output is achieved by refining this intermediate result with the NSF model, starting Algorithm 3: NSGF++ Training Input : number of time steps T , batch size n, gradient flow step size η &gt; 0, empirical or samplable distribution µ 0 and µ * , neural network parameters θ, optimizer step size γ &gt; 0</s></p></div></figDesc><table coords="13,63.96,95.41,476.45,240.70"><row><cell cols="2">/ * NSGF model</cell><cell/><cell/><cell>* /</cell></row><row><cell cols="5">/ * Build trajectory pool</cell><cell>* /</cell></row><row><cell cols="2">while Building do</cell><cell/><cell/></row><row><cell cols="5">/ * Sample batches of size n i.i.d. from the datasets</cell><cell>* /</cell></row><row><cell cols="5">X0 i ∼ µ 0 , Ỹi ∼ µ  *  , i = 1, 2, • • • n.</cell></row><row><cell cols="3">for t = 0, 1, • • • T do</cell><cell/></row><row><cell cols="2">calculatef μt,μt</cell><cell cols="3">Xt i , f μt,μ  *  Xt i .</cell></row><row><cell>vFϵ µt</cell><cell cols="3">Xt i = ∇f μt,μt</cell><cell>Xt i -∇f μt,μ  *  Xt i .</cell></row><row><cell>Xt+1 i</cell><cell cols="2">= Xt i + η vFϵ t</cell><cell cols="2">Xt i .</cell></row><row><cell cols="3">store all Xt i , vFϵ t</cell><cell>Xt i</cell><cell>pair into the pool, i = 1, 2, • • • n.</cell></row><row><cell cols="5">/ * velocity field matching</cell><cell>* /</cell></row><row><cell cols="3">while Not convergence do</cell><cell/></row><row><cell cols="5">from trajectory pool sample pair Xt i , vFϵ t</cell><cell>Xt i</cell><cell>.</cell></row><row><cell cols="4">L(θ) = v θ ( Xt i , t) -vFε µt</cell><cell>Xt i</cell><cell>2</cell><cell>,</cell></row><row><cell cols="3">θ ← θ -γ∇ θ L (θ) .</cell><cell/></row><row><cell cols="5">/ * phase trainsition time predictor</cell><cell>* /</cell></row><row><cell cols="2">while Training do</cell><cell/><cell/></row><row><cell>X0</cell><cell/><cell/><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="15,315.00,224.52,243.00,27.70"><head>Table 3 :</head><label>3</label><figDesc><div><p><s coords="15,348.01,224.52,209.98,7.77;15,315.00,234.49,243.00,7.77;15,315.00,244.45,115.28,7.77">Comparison of NSGF++ and other methods over MNIST, The last row states statistics of the FID scores between 10k training examples and 10k test examples</s></p></div></figDesc><table/></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,58.07,70.82,65.91,8.58" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Alaya</surname></persName>
		</author>
		<idno type="DOI">10.36567/aly.v15i2</idno>
		<imprint>
			<date type="published" when="2019-11-29">2019</date>
			<publisher>Balai Bahasa Jawa Tengah</publisher>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Alaya et al., 2019]</note>
</biblStruct>

<biblStruct coords="8,128.95,71.63,168.05,7.77;8,64.95,81.59,232.05,7.77;8,64.95,91.39,178.41,7.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coords="8,173.75,81.59,123.25,7.77;8,64.95,91.55,102.79,7.77">Screening sinkhorn algorithm for regularized optimal transport</title>
		<author>
			<persName coords=""><forename type="first">Maxime</forename><surname>Mokhtar Z Alaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilles</forename><surname>Berar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alain</forename><surname>Gasso</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,174.38,91.39,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mokhtar Z Alaya, Maxime Berar, Gilles Gasso, and Alain Rakotomamonjy. Screening sinkhorn algorithm for regularized optimal transport. NeurIPS, 32, 2019.</note>
</biblStruct>

<biblStruct coords="8,58.03,104.25,128.78,8.58" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Vanden-Eijnden</forename><surname>Albergo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Albergo and Vanden-Eijnden, 2023]</note>
</biblStruct>

<biblStruct coords="8,191.79,105.06,105.21,7.77;8,64.95,115.02,232.05,7.77;8,64.95,124.82,150.78,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main" coords="8,144.23,115.02,152.77,7.77;8,64.95,124.98,41.15,7.77">Building normalizing flows with stochastic interpolants</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albergo</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,122.47,124.82,66.51,7.73">The Eleventh ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh ICLR, 2023.</note>
</biblStruct>

<biblStruct coords="8,57.90,137.68,96.23,8.58" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Alvarez-Melis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alvarez-Melis et al., 2022]</note>
</biblStruct>

<biblStruct coords="8,159.11,138.49,137.89,7.77;8,64.95,148.45,232.05,7.77;8,64.95,158.25,232.04,7.93;8,64.95,168.21,114.20,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coords="8,130.56,148.45,166.44,7.77;8,64.95,158.41,149.56,7.77">Optimizing functionals on the space of probabilities with input convex neural networks</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yair</forename><surname>Schiff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,222.11,158.25,74.89,7.73;8,64.95,168.21,87.88,7.73">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">David Alvarez-Melis, Yair Schiff, and Youssef Mroueh. Optimizing functionals on the space of proba- bilities with input convex neural networks. Transactions on Ma- chine Learning Research, 2022.</note>
</biblStruct>

<biblStruct coords="8,58.32,181.07,80.11,8.58" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Ambrosio</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ambrosio et al., 2005]</note>
</biblStruct>

<biblStruct coords="8,143.41,181.88,153.59,7.77;8,64.95,191.68,232.05,7.93;8,64.95,201.64,232.05,7.93;8,64.95,211.77,47.57,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coords="8,134.45,191.68,162.55,7.73;8,64.95,201.64,110.30,7.73">Gradient Flows of Probability Measures</title>
		<author>
			<persName coords=""><forename type="first">Luigi</forename><surname>Ambrosio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giuseppe</forename><surname>Savaré</surname></persName>
		</author>
		<idno type="DOI">10.1016/s1874-5717(07)80004-1</idno>
	</analytic>
	<monogr>
		<title level="m">Handbook of Differential Equations: Evolutionary Equations</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="136"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient flows: in metric spaces and in the space of probability measures. Springer Science &amp; Business Media, 2005.</note>
</biblStruct>

<biblStruct coords="8,58.88,224.46,65.10,8.58" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Amos</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Amos et al., 2017]</note>
</biblStruct>

<biblStruct coords="8,128.96,225.27,168.04,7.77;8,64.95,235.07,232.04,7.93;8,64.95,245.20,20.17,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coords="8,286.55,225.27,10.45,7.77;8,64.95,235.23,99.09,7.77">Input convex neural networks</title>
		<author>
			<persName coords=""><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,183.82,235.07,18.93,7.73">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="146" to="155"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Brandon Amos, Lei Xu, and J Zico Kolter. In- put convex neural networks. In ICML, pages 146-155. PMLR, 2017.</note>
</biblStruct>

<biblStruct coords="8,57.84,257.89,68.62,8.58" xml:id="b10">
	<monogr>
		<title level="m" type="main">Figure 2: Forest plot demonstrating the incidence of dental trauma in Arab children’s permanent teeth (Hamdan &amp; Rock, 1995; Marcenes et al., 1999; Al-Majed, Murray &amp; Maguire, 2001; Al-Jundi, 2002; Artun et al., 2005; Sgan-Cohen, Yassin &amp; Livny, 2008; Al-Malik, 2009; Noori &amp; Al-Obaidi, 2009; Navabazam &amp; Farahani, 2010; Ajlouni, Jaradat &amp; Rihani, 2010; ElKarmi et al., 2015; El-Kenany, Awad &amp; Hegazy, 2016; Muhamad, Nezar &amp; Azzaldeen, 2016; Rajab &amp; Abu Al Huda, 2019; Al-Ansari &amp; Nazir, 2020; Arheiam et al., 2020; Shehri et al., 2021; Basha et al., 2021; Alshammary et al., 2022; Hashim et al., 2022).</title>
		<author>
			<persName coords=""><surname>Ansari</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.18366/fig-2</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Ansari et al., 2021]</note>
</biblStruct>

<biblStruct coords="8,131.45,258.70,165.56,7.77;8,64.95,268.66,232.05,7.77;8,64.95,278.47,106.03,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main" coords="8,113.70,268.66,183.30,7.77;8,64.95,278.63,45.53,7.77">A Characteristic Function Approach to Deep Implicit Generative Modeling</title>
		<author>
			<persName coords=""><forename type="first">Abdul</forename><surname>Fatir Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Scarlett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harold</forename><surname>Soh</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00750</idno>
	</analytic>
	<monogr>
		<title level="m" coords="8,126.89,278.47,17.34,7.73">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7476" to="7484"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Abdul Fatir Ansari, Ming Liang Ang, and Harold Soh. Refining deep generative models via discriminator gradient flow. In ICLR, 2021.</note>
</biblStruct>

<biblStruct coords="8,57.90,291.32,65.08,8.58" xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Arbel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Arbel et al., 2019]</note>
</biblStruct>

<biblStruct coords="8,127.96,292.13,169.04,7.77;8,64.95,302.09,232.05,7.77;8,64.95,311.89,68.99,7.93" xml:id="b13">
	<analytic>
		<title level="a" type="main" coords="8,133.10,302.09,160.43,7.77">Maximum mean discrepancy gradient flow</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Korba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adil</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,64.95,311.89,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient flow. NeurIPS, 32, 2019.</note>
</biblStruct>

<biblStruct coords="8,58.40,324.75,67.57,8.58" xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Bunne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bunne et al., 2022]</note>
</biblStruct>

<biblStruct coords="8,130.95,325.56,166.05,7.77;8,64.95,335.52,232.05,7.77;8,64.95,345.32,232.04,7.93;8,64.95,355.29,232.04,7.93;8,64.95,365.41,49.07,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coords="8,197.15,335.52,99.85,7.77;8,64.95,345.49,125.85,7.77">Proximal optimal transport modeling of population dynamics</title>
		<author>
			<persName coords=""><forename type="first">Charlotte</forename><surname>Bunne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laetitia</forename><surname>Papaxanthos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,217.43,345.32,79.57,7.73;8,64.95,355.29,159.62,7.73">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6511" to="6528"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Charlotte Bunne, Laetitia Papaxanthos, An- dreas Krause, and Marco Cuturi. Proximal optimal transport modeling of population dynamics. In International Confer- ence on Artificial Intelligence and Statistics, pages 6511-6528. PMLR, 2022.</note>
</biblStruct>

<biblStruct coords="8,57.70,378.11,52.72,8.58" xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Butcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Butcher, 1964]</note>
</biblStruct>

<biblStruct coords="8,115.40,378.91,181.60,7.77;8,64.95,388.72,180.56,7.93" xml:id="b17">
	<analytic>
		<title level="a" type="main" coords="8,183.40,378.91,109.94,7.77">Implicit runge-kutta processes</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Butcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,64.95,388.72,100.93,7.73">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">85</biblScope>
			<biblScope unit="page" from="50" to="64"/>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
	<note type="raw_reference">John C Butcher. Implicit runge-kutta processes. Mathematics of computation, 18(85):50-64, 1964.</note>
</biblStruct>

<biblStruct coords="8,58.09,401.57,61.92,8.58" xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Choi et al., 2022]</note>
</biblStruct>

<biblStruct coords="8,124.98,402.38,172.02,7.77;8,64.95,412.34,232.05,7.77;8,64.95,422.15,232.05,7.93;8,64.95,432.11,169.12,7.93" xml:id="b19">
	<analytic>
		<title level="a" type="main" coords="8,127.57,412.34,169.43,7.77;8,64.95,422.31,31.93,7.77">Density ratio estimation via infinitesimal classification</title>
		<author>
			<persName coords=""><forename type="first">Kristy</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,114.86,422.15,182.14,7.73;8,64.95,432.11,47.17,7.73">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2552" to="2573"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Kristy Choi, Chenlin Meng, Yang Song, and Stefano Ermon. Density ratio estimation via infinitesimal clas- sification. In International Conference on Artificial Intelligence and Statistics, pages 2552-2573. PMLR, 2022.</note>
</biblStruct>

<biblStruct coords="8,57.52,444.97,47.79,8.58" xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cuturi, 2013]</note>
</biblStruct>

<biblStruct coords="8,110.29,445.77,186.72,7.77;8,64.95,455.57,177.32,7.93" xml:id="b21">
	<analytic>
		<title level="a" type="main" coords="8,163.71,445.77,133.30,7.77;8,64.95,455.74,101.69,7.77">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,173.28,455.57,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marco Cuturi. Sinkhorn distances: Lightspeed com- putation of optimal transport. NeurIPS, 26, 2013.</note>
</biblStruct>

<biblStruct coords="8,57.98,468.43,239.02,8.58;8,64.95,479.04,213.87,7.93" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Seljak</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00674</idno>
		<title level="m" coords="8,140.16,469.24,156.84,7.77;8,64.95,479.20,63.45,7.77">Sliced iterative normalizing flows</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Biwei Dai and Uros Seljak</note>
	<note type="raw_reference">Dai and Seljak, 2020] Biwei Dai and Uros Seljak. Sliced iterative normalizing flows. arXiv preprint arXiv:2007.00674, 2020.</note>
</biblStruct>

<biblStruct coords="8,58.48,491.90,85.91,8.58" xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Damodaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Damodaran et al., 2018]</note>
</biblStruct>

<biblStruct coords="8,149.37,492.71,147.63,7.77;8,64.95,502.67,232.05,7.77;8,64.95,512.63,232.05,7.77;8,64.95,522.43,232.05,7.93;8,64.95,532.40,211.34,7.93" xml:id="b24">
	<analytic>
		<title level="a" type="main" coords="8,64.95,512.63,232.05,7.77;8,64.95,522.59,87.68,7.77">Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Bharath Bhushan Damodaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rémi</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devis</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Courty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,172.05,522.43,124.96,7.73;8,64.95,532.40,125.96,7.73">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="447" to="463"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Bharath Bhushan Damodaran, Benjamin Kellenberger, Rémi Flamary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsuper- vised domain adaptation. In Proceedings of the European con- ference on computer vision (ECCV), pages 447-463, 2018.</note>
</biblStruct>

<biblStruct coords="8,58.23,545.25,58.27,8.58" xml:id="b25">
	<monogr>
		<title level="m" type="main">Figure 11: Visual representation of comparison with existing studies (Genovese et al., 2021; Das &amp; Meher, 2021; Khan Tusar et al., 2024; Bhuvaneswari et al., 2023; Najjar et al., 2023; Keerthivasan &amp; Saranya, 2023; Dangore et al., 2024).</title>
		<author>
			<persName coords=""><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.2997/fig-11</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Das et al., 2023]</note>
</biblStruct>

<biblStruct coords="8,121.49,546.06,175.52,7.77;8,64.95,556.02,232.05,7.77;8,64.95,565.83,232.05,7.93;8,64.95,575.79,121.46,7.93" xml:id="b26">
	<monogr>
		<title level="m" type="main" coords="8,110.28,565.99,160.60,7.77">Image generation with shortest path diffusion</title>
		<author>
			<persName coords=""><forename type="first">Ayan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stathi</forename><surname>Fotiadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anil</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Farhang</forename><surname>Nabiei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fengting</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sattar</forename><surname>Vakili</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Da-Shan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Shiu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bernacchia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.00501</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Ayan Das, Stathi Fotiadis, Anil Batra, Farhang Nabiei, FengTing Liao, Sattar Vakili, Da-shan Shiu, and Alberto Bernacchia. Image generation with shortest path diffusion. arXiv preprint arXiv:2306.00501, 2023.</note>
</biblStruct>

<biblStruct coords="8,57.98,588.65,61.51,8.58" xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Deja</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Deja et al., 2020]</note>
</biblStruct>

<biblStruct coords="8,124.47,589.45,172.52,7.77;8,64.95,599.42,232.05,7.77;8,64.95,609.22,232.05,7.93;8,64.95,619.34,71.98,7.77" xml:id="b28">
	<analytic>
		<title level="a" type="main" coords="8,279.57,599.42,17.43,7.77;8,64.95,609.38,176.63,7.77">Endto-end sinkhorn autoencoder with noise generator</title>
		<author>
			<persName coords=""><forename type="first">Kamil</forename><surname>Deja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Dubiński</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandro</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Przemysław</forename><surname>Spurek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomasz</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,248.62,609.22,44.57,7.73">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="7211" to="7219"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kamil Deja, Jan Dubiński, Piotr Nowak, San- dro Wenzel, Przemysław Spurek, and Tomasz Trzcinski. End- to-end sinkhorn autoencoder with noise generator. IEEE Access, 9:7211-7219, 2020.</note>
</biblStruct>

<biblStruct coords="8,58.03,632.04,238.97,8.58;8,64.95,642.65,232.05,7.93;8,64.95,652.61,232.05,7.93;8,64.95,662.73,20.17,7.77" xml:id="b29">
	<analytic>
		<title level="a" type="main" coords="8,98.75,642.81,174.65,7.77">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName coords=""><forename type="first">Nichol</forename><forename type="middle">;</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,284.05,642.65,12.95,7.73;8,64.95,652.61,172.13,7.73">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794"/>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dhariwal and Nichol, 2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Ad- vances in neural information processing systems, 34:8780-8794, 2021.</note>
</biblStruct>

<biblStruct coords="8,58.65,675.43,54.87,8.58" xml:id="b30">
	<monogr>
		<title level="m" type="main">Figure 4: Forest plot showing impact of paternal HBV infection on clinical pregnancy rate (per cycle) (Chen et al., 2013; Cito et al., 2021; Du et al., 2014; He et al., 2018; Lam et al., 2010; Lee et al., 2010; Ma et al., 2023; Oger et al., 2011; Sun et al., 2024; Zhou et al., 2011).</title>
		<author>
			<persName coords=""><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.19824/fig-4</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Du et al., 2023]</note>
</biblStruct>

<biblStruct coords="8,118.50,676.24,178.50,7.77;8,64.95,686.20,232.05,7.77;8,64.95,696.16,173.02,7.77" xml:id="b31">
	<monogr>
		<title level="m" type="main" coords="8,165.44,686.20,131.56,7.77;8,64.95,696.16,146.04,7.77">Nonparametric generative modeling with conditional sliced-wasserstein flows</title>
		<author>
			<persName coords=""><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Yan Shuicheng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chao Du, Tianbo Li, Tianyu Pang, YAN Shuicheng, and Min Lin. Nonparametric generative modeling with conditional sliced-wasserstein flows. 2023.</note>
</biblStruct>

<biblStruct coords="8,319.08,57.32,57.80,8.58" xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Fan</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fan et al., 2022]</note>
</biblStruct>

<biblStruct coords="8,381.86,58.13,176.14,7.77;8,325.95,68.09,232.05,7.77;8,325.95,77.89,172.78,7.94" xml:id="b33">
	<analytic>
		<title level="a" type="main" coords="8,441.54,68.09,116.46,7.77;8,325.95,78.05,13.90,7.77">Variational wasserstein gradient flow</title>
		<author>
			<persName coords=""><forename type="first">Jiaojiao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amirhossein</forename><surname>Taghvaei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,356.27,77.89,18.93,7.73">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6185" to="6215"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jiaojiao Fan, Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin Chen. Variational wasserstein gradient flow. In ICML, pages 6185-6215. PMLR, 2022.</note>
</biblStruct>

<biblStruct coords="8,318.54,90.56,66.80,8.58" xml:id="b34">
	<analytic>
		<title level="a" type="main">L'Institut dans la presse locale et régionale</title>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Fatras</surname></persName>
		</author>
		<idno type="DOI">10.3406/casec.1999.2294</idno>
	</analytic>
	<monogr>
		<title level="j">Cahiers de sociologie économique et culturelle</title>
		<title level="j" type="abbrev">casec</title>
		<idno type="ISSN">0761-9871</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="114"/>
			<date type="published" when="2019">2019</date>
			<publisher>PERSEE Program</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Fatras et al., 2019]</note>
</biblStruct>

<biblStruct coords="8,390.32,91.36,167.67,7.77;8,325.95,101.33,232.05,7.77;8,325.95,111.13,232.04,7.93;8,325.95,121.09,90.16,7.93" xml:id="b35">
	<monogr>
		<title level="m" type="main" coords="8,467.89,101.33,90.11,7.77;8,325.95,111.29,172.04,7.77">Learning with minibatch wasserstein: asymptotic and gradient properties</title>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><surname>Fatras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Younes</forename><surname>Zine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rémi</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Courty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04091</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Kilian Fatras, Younes Zine, Rémi Flamary, Rémi Gribonval, and Nicolas Courty. Learning with minibatch wasserstein: asymptotic and gradient properties. arXiv preprint arXiv:1910.04091, 2019.</note>
</biblStruct>

<biblStruct coords="8,318.54,133.76,239.47,8.58;8,325.95,144.53,232.05,7.77;8,325.95,154.33,232.05,7.93;8,325.95,164.45,71.49,7.77" xml:id="b36">
	<analytic>
		<title level="a" type="main" coords="8,424.18,144.53,133.82,7.77;8,325.95,154.49,141.39,7.77">Unbalanced minibatch optimal transport; applications to domain adaptation</title>
		<author>
			<persName coords=""><surname>Fatras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,486.01,154.33,18.93,7.73">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="3186" to="3197"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Fatras et al., 2021a] Kilian Fatras, Thibault Séjourné, Rémi Fla- mary, and Nicolas Courty. Unbalanced minibatch optimal trans- port; applications to domain adaptation. In ICML, pages 3186- 3197. PMLR, 2021.</note>
</biblStruct>

<biblStruct coords="8,318.54,176.96,239.46,8.58;8,325.95,187.73,232.04,7.77;8,325.95,197.69,232.05,7.77;8,325.95,207.49,143.62,7.93" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><surname>Fatras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01792</idno>
		<title level="m" coords="8,537.57,187.73,20.43,7.77;8,325.95,197.69,228.54,7.77">Minibatch optimal transport distances; analysis and applications</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Fatras et al., 2021b] Kilian Fatras, Younes Zine, Szymon Majew- ski, Rémi Flamary, Rémi Gribonval, and Nicolas Courty. Mini- batch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021.</note>
</biblStruct>

<biblStruct coords="8,319.21,220.16,66.63,8.58" xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Feydy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Feydy et al., 2019]</note>
</biblStruct>

<biblStruct coords="8,390.82,220.96,167.18,7.77;8,325.95,230.93,232.05,7.77;8,325.95,240.89,232.05,7.77;8,325.95,250.69,232.05,7.93;8,325.95,260.65,232.04,7.93;8,325.95,270.78,20.17,7.77" xml:id="b39">
	<analytic>
		<title level="a" type="main" coords="8,353.99,240.89,204.01,7.77;8,325.95,250.85,74.82,7.77">Interpolating between optimal transport and mmd using sinkhorn divergences</title>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Feydy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thibault</forename><surname>Séjourné</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Franc ¸ois-Xavier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shun-Ichi</forename><surname>Vialard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alain</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Trouvé</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,418.92,250.69,139.08,7.73;8,325.95,260.65,129.01,7.73">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2681" to="2690"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jean Feydy, Thibault Séjourné, Franc ¸ois- Xavier Vialard, Shun-ichi Amari, Alain Trouvé, and Gabriel Peyré. Interpolating between optimal transport and mmd using sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2681-2690. PMLR, 2019.</note>
</biblStruct>

<biblStruct coords="8,318.61,283.28,52.04,8.58" xml:id="b40">
	<analytic>
		<title level="a" type="main">Management of Atrial Fibrillation: Out-of-Hospital Approach</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><forename type="middle">D</forename><surname>Folland</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1008877302411</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Thrombosis and Thrombolysis</title>
		<title level="j" type="abbrev">J Thromb Thrombolysis</title>
		<idno type="ISSN">0929-5305</idno>
		<idno type="ISSNe">1573-742X</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="135"/>
			<date type="published" when="1999-04">1999</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Folland, 1999]</note>
</biblStruct>

<biblStruct coords="8,375.64,283.93,182.36,7.93;8,325.95,293.89,232.05,7.93;8,325.95,304.01,20.17,7.77" xml:id="b41">
	<monogr>
		<title level="m" type="main" coords="8,450.87,283.93,107.13,7.73;8,325.95,293.89,105.50,7.73">Real analysis: modern techniques and their applications</title>
		<author>
			<persName coords=""><surname>Gerald B Folland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Gerald B Folland. Real analysis: modern tech- niques and their applications, volume 40. John Wiley &amp; Sons, 1999.</note>
</biblStruct>

<biblStruct coords="8,319.48,316.52,59.02,8.58" xml:id="b42">
	<monogr>
		<title level="m" type="main">Review of Gao et al.</title>
		<author>
			<persName coords=""><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.5194/amt-2019-67-rc1</idno>
		<imprint>
			<date type="published" when="2019-05-23">2019</date>
			<publisher>Copernicus GmbH</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Gao et al., 2019]</note>
</biblStruct>

<biblStruct coords="8,383.48,317.33,174.52,7.77;8,325.95,327.29,232.05,7.77;8,325.95,337.09,232.04,7.93;8,325.95,347.21,20.17,7.77" xml:id="b43">
	<analytic>
		<title level="a" type="main" coords="8,452.82,327.29,105.18,7.77;8,325.95,337.25,88.00,7.77">Deep generative learning via variational gradient flow</title>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuling</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Can</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shunkang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,434.79,337.09,18.93,7.73">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2093" to="2101"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, and Shunkang Zhang. Deep generative learning via variational gradient flow. In ICML, pages 2093-2101. PMLR, 2019.</note>
</biblStruct>

<biblStruct coords="8,319.48,359.72,59.02,8.58" xml:id="b44">
	<monogr>
		<title level="m" type="main">Figure 2: Summary graph of risk of bias of research (Gao et al., 2022; Turrado et al., 2021; Feyzioğlu et al., 2020; Basha et al., 2022; Mohammad &amp; Ahmad, 2018; Villumsen et al., 2019; Zhang et al., 2022).</title>
		<author>
			<persName coords=""><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.18701/fig-2</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Gao et al., 2022]</note>
</biblStruct>

<biblStruct coords="8,383.48,360.53,174.52,7.77;8,325.95,370.49,232.05,7.77;8,325.95,380.29,232.05,7.93;8,325.95,390.25,144.96,7.93" xml:id="b45">
	<analytic>
		<title level="a" type="main" coords="8,435.77,370.49,122.23,7.77;8,325.95,380.45,73.73,7.77">Deep generative learning via euler particle transport</title>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuling</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiliang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhijian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,420.63,380.29,137.37,7.73;8,325.95,390.25,31.22,7.73">Mathematical and Scientific Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="336" to="368"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Yuan Gao, Jian Huang, Yuling Jiao, Jin Liu, Xil- iang Lu, and Zhijian Yang. Deep generative learning via eu- ler particle transport. In Mathematical and Scientific Machine Learning, pages 336-368. PMLR, 2022.</note>
</biblStruct>

<biblStruct coords="8,319.30,402.92,75.68,8.58" xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Genevay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Genevay et al., 2016]</note>
</biblStruct>

<biblStruct coords="8,399.96,403.73,158.04,7.77;8,325.95,413.69,232.05,7.77;8,325.95,423.49,135.97,7.93" xml:id="b47">
	<analytic>
		<title level="a" type="main" coords="8,419.47,413.69,138.53,7.77;8,325.95,423.65,60.35,7.77">Stochastic optimization for large-scale optimal transport</title>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,392.94,423.49,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Aude Genevay, Marco Cuturi, Gabriel Peyré, and Francis Bach. Stochastic optimization for large-scale optimal transport. NeurIPS, 29, 2016.</note>
</biblStruct>

<biblStruct coords="8,319.30,436.16,75.68,8.58" xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Genevay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Genevay et al., 2018]</note>
</biblStruct>

<biblStruct coords="8,399.96,436.96,158.04,7.77;8,325.95,446.93,232.05,7.77;8,325.95,456.73,232.05,7.93;8,325.95,466.69,133.50,7.93" xml:id="b49">
	<analytic>
		<title level="a" type="main" coords="8,356.42,446.93,197.87,7.77">Learning generative models with sinkhorn divergences</title>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,335.82,456.73,222.18,7.73;8,325.95,466.69,11.76,7.73">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1608" to="1617"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with sinkhorn divergences. In International Conference on Artificial Intelligence and Statis- tics, pages 1608-1617. PMLR, 2018.</note>
</biblStruct>

<biblStruct coords="8,319.14,479.36,87.02,8.58" xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Goodfellow et al., 2014]</note>
</biblStruct>

<biblStruct coords="8,411.15,480.16,146.85,7.77;8,325.95,490.13,232.05,7.77;8,325.95,500.09,232.05,7.77;8,325.95,509.89,88.90,7.93" xml:id="b51">
	<analytic>
		<title level="a" type="main" coords="8,476.00,500.09,82.01,7.77;8,325.95,510.05,13.35,7.77">Generative adversarial nets</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,345.87,509.89,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 27, 2014.</note>
</biblStruct>

<biblStruct coords="8,319.12,522.56,82.70,8.58" xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Grathwohl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grathwohl et al., 2018]</note>
</biblStruct>

<biblStruct coords="8,406.80,523.36,151.20,7.77;8,325.95,533.33,232.05,7.77;8,325.95,543.29,232.05,7.77;8,325.95,553.09,175.00,7.93" xml:id="b53">
	<monogr>
		<title level="m" type="main" coords="8,510.92,533.33,47.08,7.77;8,325.95,543.29,232.05,7.77;8,325.95,553.25,24.12,7.77">Ffjord: Freeform continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricky Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free- form continuous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367, 2018.</note>
</biblStruct>

<biblStruct coords="8,319.48,565.76,63.50,8.58" xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Heng et al., 2022]</note>
</biblStruct>

<biblStruct coords="8,387.96,566.56,170.04,7.77;8,325.95,576.53,199.77,7.77" xml:id="b55">
	<monogr>
		<title level="m" type="main" coords="8,345.37,576.53,153.37,7.77">Deep generative wasserstein gradient flows</title>
		<author>
			<persName coords=""><forename type="first">Alvin</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abdul</forename><surname>Fatir Ansari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harold</forename><surname>Soh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alvin Heng, Abdul Fatir Ansari, and Harold Soh. Deep generative wasserstein gradient flows. 2022.</note>
</biblStruct>

<biblStruct coords="8,318.98,589.03,69.48,8.58" xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Heusel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Heusel et al., 2017]</note>
</biblStruct>

<biblStruct coords="8,393.44,589.84,164.56,7.77;8,325.95,599.80,232.05,7.77;8,325.95,609.76,232.05,7.77;8,325.95,619.56,116.30,7.93" xml:id="b57">
	<analytic>
		<title level="a" type="main" coords="8,539.58,599.80,18.43,7.77;8,325.95,609.76,232.05,7.77;8,325.95,619.73,40.41,7.77">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,373.27,619.56,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30, 2017.</note>
</biblStruct>

<biblStruct coords="8,319.65,632.23,54.87,8.58" xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Ho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ho et al., 2020]</note>
</biblStruct>

<biblStruct coords="8,379.50,633.04,178.50,7.77;8,325.95,642.84,232.05,7.93;8,325.95,652.96,20.17,7.77" xml:id="b59">
	<analytic>
		<title level="a" type="main" coords="8,544.56,633.04,13.44,7.77;8,325.95,643.00,134.72,7.77">Denoising diffusion probabilistic models</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="8,468.43,642.84,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jonathan Ho, Ajay Jain, and Pieter Abbeel. De- noising diffusion probabilistic models. NeurIPS, 33:6840-6851, 2020.</note>
</biblStruct>

<biblStruct coords="8,318.98,665.47,65.50,8.58" xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hsieh et al., 2021]</note>
</biblStruct>

<biblStruct coords="8,389.46,666.27,168.54,7.77;8,325.95,676.24,232.05,7.77;8,325.95,686.04,232.04,7.93;8,325.95,696.16,71.49,7.77" xml:id="b61">
	<analytic>
		<title level="a" type="main" coords="8,385.13,676.24,172.87,7.77;8,325.95,686.20,146.05,7.77">The limits of min-max optimization algorithms: Convergence to spurious non-critical sets</title>
		<author>
			<persName coords=""><forename type="first">Ya-Ping</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Panayotis</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="8,487.43,686.04,18.93,7.73">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4337" to="4348"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization algorithms: Convergence to spurious non-critical sets. In ICML, pages 4337- 4348. PMLR, 2021.</note>
</biblStruct>

<biblStruct coords="9,57.84,57.32,68.62,8.58" xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Jordan</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jordan et al., 1998]</note>
</biblStruct>

<biblStruct coords="9,131.45,58.13,165.55,7.77;9,64.95,68.09,232.05,7.77;9,64.95,77.89,211.95,7.94" xml:id="b63">
	<analytic>
		<title level="a" type="main" coords="9,86.28,68.09,207.04,7.77">The variational formulation of the fokker-planck equation</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Kinderlehrer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,64.95,77.89,141.68,7.73">SIAM journal on mathematical analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17"/>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-planck equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998.</note>
</biblStruct>

<biblStruct coords="9,57.51,90.97,239.49,8.58;9,64.95,101.74,201.45,7.77" xml:id="b64">
	<monogr>
		<author>
			<persName coords=""><surname>Kalantari</surname></persName>
		</author>
		<title level="m" coords="9,120.73,91.61,172.06,7.94">Iraj Kalantari. Induction over the Continuum</title>
		<meeting><address><addrLine>Netherlands; Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="145" to="154"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Kalantari, 2007] Iraj Kalantari. Induction over the Continuum, pages 145-154. Springer Netherlands, Dordrecht, 2007.</note>
</biblStruct>

<biblStruct coords="9,58.38,114.66,76.24,8.58" xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Komatsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Komatsu et al., 2021]</note>
</biblStruct>

<biblStruct coords="9,139.61,115.46,157.40,7.77;9,64.95,125.43,232.05,7.77;9,64.95,135.23,232.05,7.93;9,64.95,145.19,144.44,7.93" xml:id="b66">
	<analytic>
		<title level="a" type="main" coords="9,98.08,125.43,198.92,7.77;9,64.95,135.39,8.41,7.77">Multi-source domain adaptation with sinkhorn barycenter</title>
		<author>
			<persName coords=""><forename type="first">Tatsuya</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomoko</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,90.88,135.23,206.12,7.73;9,64.95,145.19,26.25,7.73">2021 29th European Signal Processing Conference (EU-SIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1371" to="1375"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Tatsuya Komatsu, Tomoko Matsui, and Jun- bin Gao. Multi-source domain adaptation with sinkhorn barycen- ter. In 2021 29th European Signal Processing Conference (EU- SIPCO), pages 1371-1375. IEEE, 2021.</note>
</biblStruct>

<biblStruct coords="9,57.82,158.27,72.32,8.58" xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Korotin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Korotin et al., 2021]</note>
</biblStruct>

<biblStruct coords="9,135.12,159.08,161.88,7.77;9,64.95,169.04,232.05,7.77;9,64.95,179.00,232.05,7.77;9,64.95,188.80,231.75,7.93" xml:id="b68">
	<analytic>
		<title level="a" type="main" coords="9,101.75,179.00,195.25,7.77;9,64.95,188.97,103.52,7.77">Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Korotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lingxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Filippov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,175.91,188.80,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14593" to="14605"/>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, Alexander Filippov, and Evgeny Burnaev. Do neural optimal transport solvers work? a continu- ous wasserstein-2 benchmark. NeurIPS, 34:14593-14605, 2021.</note>
</biblStruct>

<biblStruct coords="9,57.65,201.88,52.88,8.58" xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li et al., 2017]</note>
</biblStruct>

<biblStruct coords="9,115.51,202.69,181.49,7.77;9,64.95,212.65,232.05,7.77;9,64.95,222.45,232.05,7.93" xml:id="b70">
	<analytic>
		<title level="a" type="main" coords="9,200.01,212.65,96.99,7.77;9,64.95,222.62,156.74,7.77">Mmd gan: Towards deeper understanding of moment matching network</title>
		<author>
			<persName coords=""><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,228.40,222.45,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. Mmd gan: Towards deeper understanding of moment matching network. NeurIPS, 30, 2017.</note>
</biblStruct>

<biblStruct coords="9,58.41,235.53,72.04,8.58" xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Lipman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lipman et al., 2023]</note>
</biblStruct>

<biblStruct coords="9,135.44,236.34,161.56,7.77;9,64.95,246.30,232.05,7.77;9,64.95,256.11,181.54,7.93" xml:id="b72">
	<analytic>
		<title level="a" type="main" coords="9,229.97,246.30,67.03,7.77;9,64.95,256.27,71.33,7.77">Flow matching for generative modeling</title>
		<author>
			<persName coords=""><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,153.24,256.11,66.51,7.73">The Eleventh ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yaron Lipman, Ricky T. Q. Chen, Heli Ben- Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh ICLR, 2023.</note>
</biblStruct>

<biblStruct coords="9,57.86,269.19,57.16,8.58" xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu et al., 2023]</note>
</biblStruct>

<biblStruct coords="9,120.00,269.99,177.01,7.77;9,64.95,279.95,232.05,7.77;9,64.95,289.76,173.38,7.93" xml:id="b74">
	<analytic>
		<title level="a" type="main" coords="9,64.95,279.95,232.05,7.77;9,64.95,289.92,63.71,7.77">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName coords=""><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,145.08,289.76,66.51,7.73">The Eleventh ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh ICLR, 2023.</note>
</biblStruct>

<biblStruct coords="9,57.80,302.84,72.16,8.58" xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Liutkus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liutkus et al., 2019]</note>
</biblStruct>

<biblStruct coords="9,134.94,303.64,162.06,7.77;9,64.95,313.61,232.05,7.77;9,64.95,323.57,232.05,7.77;9,64.95,333.37,232.04,7.93;9,64.95,343.49,20.17,7.77" xml:id="b76">
	<analytic>
		<title level="a" type="main" coords="9,271.60,313.61,25.40,7.77;9,64.95,323.57,232.05,7.77;9,64.95,333.53,98.18,7.77">Slicedwasserstein flows: Nonparametric generative modeling via optimal transport and diffusions</title>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Umut</forename><surname>Simsekli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Szymon</forename><surname>Majewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alain</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,178.20,333.37,18.93,7.73">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4104" to="4113"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Stöter. Sliced- wasserstein flows: Nonparametric generative modeling via opti- mal transport and diffusions. In ICML, pages 4104-4113. PMLR, 2019.</note>
</biblStruct>

<biblStruct coords="9,57.82,356.41,64.67,8.58" xml:id="b77">
	<monogr>
		<title level="m" type="main">Figure 3: Risk of bias summary (Abreu et al., 2017; Afshar et al., 2010; Ai, 2020; Bolasco et al., 2011; Cai et al., 2022; Chen, Zhao &amp; Huang, 2019; Dai &amp; Ma, 2021; Deng, 2011; Dong et al., 2011; Fakhrpour et al., 2020; Fang et al., 2023; Feng et al., 2020; Frih et al., 2017; Hristea et al., 2016; Jeong et al., 2019; Kozlowska et al., 2023; Leng, 2012; Li et al., 2008; Li &amp; Feng, 2020; Liao et al., 2016; Limwannata et al., 2021; Lu, 2022; Martin-Alemañy et al., 2020, 2016, 2022; Sezer et al., 2014; Shi et al., 2021; Su et al., 2022; Sun, Sun &amp; Yang, 2022a; Tabibi et al., 2023; Tan et al., 2015; Tayebi, Ramezani &amp; Kashef, 2018; Vijaya et al., 2019; Wang &amp; Liu, 2021; Wang, 2018; Wang et al., 2019; Wang, 2019; Wang et al., 2023; Wei, 2020; Wen et al., 2022; Wilund et al., 2010; Xu et al., 2022; Xu &amp; Fang, 2016; Yan, Zhao &amp; Peng, 2022; Yang et al., 2021; Yao et al., 2020; Yu &amp; Cao, 2018; Zeng et al., 2020; Zhou, 2020; Zhou et al., 2016; Zhu et al., 2020).</title>
		<author>
			<persName coords=""><forename type="first">Luise</forename></persName>
		</author>
		<idno type="DOI">10.7717/peerj.19053/fig-3</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Luise et al., 2019]</note>
</biblStruct>

<biblStruct coords="9,127.47,357.22,169.54,7.77;9,64.95,367.18,232.05,7.77;9,64.95,376.98,167.00,7.93" xml:id="b78">
	<analytic>
		<title level="a" type="main" coords="9,154.98,367.18,142.03,7.77;9,64.95,377.14,91.07,7.77">Sinkhorn barycenters with free support via frank-wolfe algorithm</title>
		<author>
			<persName coords=""><forename type="first">Giulia</forename><surname>Luise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saverio</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlo</forename><surname>Ciliberto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,162.96,376.98,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Giulia Luise, Saverio Salzo, Massimiliano Pon- til, and Carlo Ciliberto. Sinkhorn barycenters with free support via frank-wolfe algorithm. NeurIPS, 32, 2019.</note>
</biblStruct>

<biblStruct coords="9,58.48,390.06,63.51,8.58" xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mnih et al., 2013]</note>
</biblStruct>

<biblStruct coords="9,126.97,390.87,170.03,7.77;9,64.95,400.83,232.05,7.77;9,64.95,410.79,232.05,7.77;9,64.95,420.60,156.07,7.93" xml:id="b80">
	<analytic>
		<title level="a" type="main" coords="9,136.51,410.79,160.49,7.77;9,64.95,420.76,10.28,7.77">Human-level control through deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
		<idno type="arXiv">arXiv:1312.5602</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<title level="j" type="abbrev">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<idno type="ISSNe">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533"/>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learn- ing. arXiv preprint arXiv:1312.5602, 2013.</note>
</biblStruct>

<biblStruct coords="9,58.53,433.68,72.78,8.58" xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Mokrov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mokrov et al., 2021]</note>
</biblStruct>

<biblStruct coords="9,136.30,434.48,160.70,7.77;9,64.95,444.45,232.05,7.77;9,64.95,454.25,232.05,7.93;9,64.95,464.37,47.07,7.77" xml:id="b82">
	<analytic>
		<title level="a" type="main" coords="9,64.95,454.41,142.98,7.77">Large-scale wasserstein gradient flows</title>
		<author>
			<persName coords=""><forename type="first">Petr</forename><surname>Mokrov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Korotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lingxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,221.25,454.25,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15243" to="15256"/>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Petr Mokrov, Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, and Evgeny Burnaev. Large-scale wasserstein gradient flows. NeurIPS, 34:15243- 15256, 2021.</note>
</biblStruct>

<biblStruct coords="9,58.48,477.29,94.64,8.58" xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Rigotti</forename><surname>Mroueh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mroueh and Rigotti, 2020]</note>
</biblStruct>

<biblStruct coords="9,158.10,478.10,138.90,7.77;9,64.95,487.90,228.13,7.93" xml:id="b84">
	<analytic>
		<title level="a" type="main" coords="9,64.95,488.06,100.47,7.77">Unbalanced sobolev descent</title>
		<author>
			<persName coords=""><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mattia</forename><surname>Rigotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,172.29,487.90,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17034" to="17043"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Youssef Mroueh and Mattia Rigotti. Unbalanced sobolev descent. NeurIPS, 33:17034-17043, 2020.</note>
</biblStruct>

<biblStruct coords="9,58.48,500.98,72.47,8.58" xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Mroueh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mroueh et al., 2019]</note>
</biblStruct>

<biblStruct coords="9,135.93,501.78,161.07,7.77;9,64.95,511.59,232.04,7.93;9,64.95,521.55,232.05,7.93" xml:id="b86">
	<analytic>
		<title level="a" type="main" coords="9,64.95,511.75,57.07,7.77">Sobolev descent</title>
		<author>
			<persName coords=""><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anant</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,139.32,511.59,157.67,7.73;9,64.95,521.55,111.06,7.73">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2976" to="2985"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Youssef Mroueh, Tom Sercu, and Anant Raj. Sobolev descent. In The 22nd International Conference on Arti- ficial Intelligence and Statistics, pages 2976-2985. PMLR, 2019.</note>
</biblStruct>

<biblStruct coords="9,57.58,534.63,56.31,8.58" xml:id="b87">
	<monogr>
		<title level="m" type="main">Review of Pai et al. (2019)</title>
		<author>
			<persName coords=""><surname>Pai</surname></persName>
		</author>
		<idno type="DOI">10.5194/acp-2019-331-rc2</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Copernicus GmbH</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Pai et al., 2021]</note>
</biblStruct>

<biblStruct coords="9,118.87,535.43,178.14,7.77;9,64.95,545.40,232.05,7.77;9,64.95,555.36,232.05,7.77;9,64.95,565.16,232.05,7.93;9,64.95,575.12,195.03,7.93" xml:id="b88">
	<analytic>
		<title level="a" type="main" coords="9,180.82,545.40,116.19,7.77;9,64.95,555.36,232.05,7.77;9,64.95,565.32,16.94,7.77">Fast sinkhorn filters: Using matrix scaling for non-rigid shape correspondence with functional maps</title>
		<author>
			<persName coords=""><forename type="first">Gautam</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,98.18,565.16,198.82,7.73;9,64.95,575.12,110.30,7.73">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="384" to="393"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Gautam Pai, Jing Ren, Simone Melzi, Peter Wonka, and Maks Ovsjanikov. Fast sinkhorn filters: Using ma- trix scaling for non-rigid shape correspondence with functional maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 384-393, 2021.</note>
</biblStruct>

<biblStruct coords="9,57.35,588.20,68.99,8.58" xml:id="b89">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Patrini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Patrini et al., 2020]</note>
</biblStruct>

<biblStruct coords="9,131.32,589.01,165.68,7.77;9,64.95,598.97,232.05,7.77;9,64.95,608.78,232.05,7.93;9,64.95,618.74,231.78,7.93" xml:id="b90">
	<analytic>
		<title level="a" type="main" coords="9,181.58,608.94,81.60,7.77">Sinkhorn autoencoders</title>
		<author>
			<persName coords=""><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcello</forename><surname>Forre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samarth</forename><surname>Carioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,283.06,608.78,13.94,7.73;9,64.95,618.74,118.52,7.73">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="733" to="743"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Giorgio Patrini, Rianne Van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Un- certainty in Artificial Intelligence, pages 733-743. PMLR, 2020.</note>
</biblStruct>

<biblStruct coords="9,57.88,631.82,64.97,8.58" xml:id="b91">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Peyré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peyré et al., 2017]</note>
</biblStruct>

<biblStruct coords="9,127.83,632.62,169.18,7.77;9,64.95,642.43,232.04,7.93;9,64.95,652.39,156.52,7.93" xml:id="b92">
	<analytic>
		<title level="a" type="main" coords="9,261.14,632.62,35.87,7.77;9,64.95,642.59,83.73,7.77">Computational optimal transport</title>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,156.47,642.43,140.53,7.73;9,64.95,652.39,89.93,7.73">Center for Research in Economics and Statistics Working Papers</title>
		<imprint>
			<date type="published" when="2017">2017-86. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gabriel Peyré, Marco Cuturi, et al. Computa- tional optimal transport. Center for Research in Economics and Statistics Working Papers, (2017-86), 2017.</note>
</biblStruct>

<biblStruct coords="9,57.63,665.47,114.41,8.58" xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Bruti-Liberati</forename><surname>Platen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Platen and Bruti-Liberati, 2010]</note>
</biblStruct>

<biblStruct coords="9,177.02,666.27,119.98,7.77;9,64.95,676.08,232.05,7.93;9,64.95,686.04,232.05,7.93;9,64.95,696.16,47.57,7.77" xml:id="b94">
	<monogr>
		<title level="m" type="main" coords="9,100.09,676.08,196.91,7.73;9,64.95,686.04,76.07,7.73">Numerical solution of stochastic differential equations with jumps in finance</title>
		<author>
			<persName coords=""><forename type="first">Eckhard</forename><surname>Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Bruti-Liberati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Eckhard Platen and Nicola Bruti- Liberati. Numerical solution of stochastic differential equations with jumps in finance, volume 64. Springer Science &amp; Business Media, 2010.</note>
</biblStruct>

<biblStruct coords="9,318.89,57.32,239.12,8.58;9,325.95,68.09,232.05,7.77;9,325.95,78.05,232.05,7.77;9,325.95,88.01,156.81,7.77;9,501.83,87.85,56.17,7.73;9,325.95,97.82,90.16,7.94" xml:id="b95">
	<monogr>
		<author>
			<persName coords=""><surname>Pooladian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14772</idno>
		<title level="m" coords="9,417.29,78.05,140.71,7.77;9,325.95,88.01,153.10,7.77">Multisample flow matching: Straightening flows with minibatch couplings</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Pooladian et al., 2023] Aram-Alexandre Pooladian, Heli Ben- Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lip- man, and Ricky Chen. Multisample flow matching: Straight- ening flows with minibatch couplings. arXiv preprint arXiv:2304.14772, 2023.</note>
</biblStruct>

<biblStruct coords="9,318.99,113.80,77.46,8.58" xml:id="b96">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Salimans et al., 2016]</note>
</biblStruct>

<biblStruct coords="9,401.42,114.61,156.58,7.77;9,325.95,124.57,232.05,7.77;9,325.95,134.37,174.27,7.93" xml:id="b97">
	<analytic>
		<title level="a" type="main" coords="9,523.41,124.57,34.59,7.77;9,325.95,134.53,98.33,7.77">Improved techniques for training gans</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,431.24,134.37,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29, 2016.</note>
</biblStruct>

<biblStruct coords="9,318.97,150.36,74.73,8.58" xml:id="b98">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Santambrogio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Santambrogio, 2015]</note>
</biblStruct>

<biblStruct coords="9,398.68,151.17,159.32,7.77;9,325.95,160.97,216.66,7.93" xml:id="b99">
	<analytic>
		<title level="a" type="main" coords="9,482.11,151.17,75.89,7.77;9,325.95,161.13,83.26,7.77">Optimal transport for applied mathematicians</title>
		<author>
			<persName coords=""><forename type="first">Filippo</forename><surname>Santambrogio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,416.34,160.97,32.95,7.73">Birkäuser</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">58-63</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2015">2015</date>
			<pubPlace>NY</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-63):94, 2015.</note>
</biblStruct>

<biblStruct coords="9,318.97,176.96,74.73,8.58" xml:id="b100">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Santambrogio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Santambrogio, 2017]</note>
</biblStruct>

<biblStruct coords="9,398.68,177.48,159.32,8.06;9,325.95,187.56,232.04,7.93;9,325.95,197.53,127.01,7.93" xml:id="b101">
	<analytic>
		<title level="a" type="main" coords="9,486.69,177.48,71.31,8.06;9,325.95,187.73,162.71,7.77">{Euclidean, metric, and Wasserstein} gradient flows: an overview</title>
		<author>
			<persName coords=""><forename type="first">Filippo</forename><surname>Santambrogio</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13373-017-0101-1</idno>
	</analytic>
	<monogr>
		<title level="j" coords="9,496.04,187.56,61.96,7.73;9,325.95,197.53,62.55,7.73">Bulletin of Mathematical Sciences</title>
		<title level="j" type="abbrev">Bull. Math. Sci.</title>
		<idno type="ISSN">1664-3607</idno>
		<idno type="ISSNe">1664-3615</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="154"/>
			<date type="published" when="2017-03-14">2017</date>
			<publisher>World Scientific Pub Co Pte Ltd</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. Bulletin of Math- ematical Sciences, 7:87-154, 2017.</note>
</biblStruct>

<biblStruct coords="9,318.90,213.51,65.08,8.58" xml:id="b102">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Shaul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shaul et al., 2023]</note>
</biblStruct>

<biblStruct coords="9,388.97,214.32,169.03,7.77;9,325.95,224.28,232.05,7.77;9,325.95,234.09,232.05,7.93;9,325.95,244.21,75.97,7.77" xml:id="b103">
	<analytic>
		<title level="a" type="main" coords="9,487.70,224.28,70.31,7.77;9,325.95,234.25,139.13,7.77">On kinetic optimal probability paths for generative models</title>
		<author>
			<persName coords=""><forename type="first">Neta</forename><surname>Shaul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricky Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,482.38,234.09,18.93,7.73">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="30883" to="30907"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Neta Shaul, Ricky TQ Chen, Maximilian Nickel, Matthew Le, and Yaron Lipman. On kinetic optimal probability paths for generative models. In ICML, pages 30883- 30907. PMLR, 2023.</note>
</biblStruct>

<biblStruct coords="9,319.18,260.04,62.31,8.58" xml:id="b104">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shen et al., 2020]</note>
</biblStruct>

<biblStruct coords="9,386.48,260.84,171.53,7.77;9,325.95,270.80,232.05,7.77;9,325.95,280.61,135.22,7.93" xml:id="b105">
	<analytic>
		<title level="a" type="main" coords="9,402.36,270.80,155.64,7.77;9,325.95,280.77,25.49,7.77">Sinkhorn barycenter via functional gradient descent</title>
		<author>
			<persName coords=""><forename type="first">Zebang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenfu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Hassani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,358.31,280.61,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="986" to="996"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zebang Shen, Zhenfu Wang, Alejandro Ribeiro, and Hamed Hassani. Sinkhorn barycenter via functional gradient descent. NeurIPS, 33:986-996, 2020.</note>
</biblStruct>

<biblStruct coords="9,318.47,296.59,66.38,8.58" xml:id="b106">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Silver et al., 2016]</note>
</biblStruct>

<biblStruct coords="9,389.83,297.40,168.17,7.77;9,325.95,307.36,232.05,7.77;9,325.95,317.33,232.05,7.77;9,325.95,327.29,232.05,7.77;9,325.95,337.09,206.75,7.93" xml:id="b107">
	<analytic>
		<title level="a" type="main" coords="9,382.23,327.29,175.78,7.77;9,325.95,337.25,76.49,7.77">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,409.26,337.09,21.70,7.73">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural net- works and tree search. nature, 529(7587):484-489, 2016.</note>
</biblStruct>

<biblStruct coords="9,319.28,353.08,84.38,8.58" xml:id="b108">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Ermon</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Song and Ermon, 2019]</note>
</biblStruct>

<biblStruct coords="9,408.64,353.88,149.36,7.77;9,325.95,363.85,232.05,7.77;9,325.95,373.65,68.99,7.93" xml:id="b109">
	<analytic>
		<title level="a" type="main" coords="9,533.29,353.88,24.71,7.77;9,325.95,363.85,228.71,7.77">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,325.95,373.65,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yang Song and Stefano Ermon. Gener- ative modeling by estimating gradients of the data distribution. NeurIPS, 32, 2019.</note>
</biblStruct>

<biblStruct coords="9,319.28,389.64,62.71,8.58" xml:id="b110">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Song et al., 2021]</note>
</biblStruct>

<biblStruct coords="9,386.98,390.44,171.02,7.77;9,325.95,400.40,232.05,7.77;9,325.95,410.37,232.05,7.77;9,325.95,420.17,93.64,7.93" xml:id="b111">
	<analytic>
		<title level="a" type="main" coords="9,325.95,410.37,232.05,7.77;9,325.95,420.33,32.95,7.77">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,375.50,420.17,17.34,7.73">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021.</note>
</biblStruct>

<biblStruct coords="9,318.49,436.16,83.67,8.58" xml:id="b112">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Mayers</forename><surname>Süli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Süli and Mayers, 2003]</note>
</biblStruct>

<biblStruct coords="9,407.14,436.80,150.86,7.93;9,325.95,446.76,232.05,7.93" xml:id="b113">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Endre</forename><surname>Süli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">F</forename><surname>Mayers</surname></persName>
		</author>
		<title level="m" coords="9,525.83,436.80,32.17,7.73;9,325.95,446.76,104.06,7.73">An introduction to numerical analysis</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Endre Süli and David F Mayers. An intro- duction to numerical analysis. Cambridge university press, 2003.</note>
</biblStruct>

<biblStruct coords="9,319.24,462.75,62.53,8.58" xml:id="b114">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tong et al., 2023]</note>
</biblStruct>

<biblStruct coords="9,386.75,463.56,171.25,7.77;9,325.95,473.52,232.05,7.77;9,325.95,483.48,232.05,7.77;9,325.95,493.45,232.05,7.77;9,325.95,503.25,232.05,7.73;9,325.95,513.21,84.18,7.93" xml:id="b115">
	<analytic>
		<title level="a" type="main" coords="9,438.74,483.48,119.26,7.77;9,325.95,493.45,214.02,7.77">Improving and generalizing flowbased generative models with minibatch optimal transport</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jarrid</forename><surname>Rector-Brooks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fatras</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,325.95,503.25,232.05,7.73;9,325.95,513.21,57.93,7.73">ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian FATRAS, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow- based generative models with minibatch optimal transport. In ICML Workshop on New Frontiers in Learning, Control, and Dy- namical Systems, 2023.</note>
</biblStruct>

<biblStruct coords="9,318.42,529.20,88.68,8.58" xml:id="b116">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Others</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Villani and others, 2009]</note>
</biblStruct>

<biblStruct coords="9,412.08,529.84,145.92,7.93;9,325.95,539.81,151.24,7.93" xml:id="b117">
	<monogr>
		<title level="m" type="main" coords="9,489.49,529.84,68.51,7.73;9,325.95,539.81,41.80,7.73">Optimal transport: old and new</title>
		<author>
			<persName coords=""><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.</note>
</biblStruct>

<biblStruct coords="9,319.74,555.79,64.52,8.58" xml:id="b118">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang et al., 2018]</note>
</biblStruct>

<biblStruct coords="9,389.24,556.60,168.76,7.77;9,325.95,566.40,232.05,7.93;9,325.95,576.36,121.46,7.93" xml:id="b119">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saman</forename><surname>Halgamuge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09916</idno>
		<title level="m" coords="9,325.95,566.56,204.99,7.77">Improving mmd-gan training with repulsive loss function</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Wei Wang, Yuan Sun, and Saman Halgamuge. Improving mmd-gan training with repulsive loss function. arXiv preprint arXiv:1812.09916, 2018.</note>
</biblStruct>

<biblStruct coords="9,319.32,592.35,106.75,8.58" xml:id="b120">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Katsoulakis</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang and Katsoulakis, 2023]</note>
</biblStruct>

<biblStruct coords="9,431.05,593.16,126.95,7.77;9,325.95,603.12,232.05,7.77;9,325.95,612.92,167.02,7.93" xml:id="b121">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Markos</forename><forename type="middle">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Katsoulakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13534</idno>
		<title level="m" coords="9,373.73,603.12,184.28,7.77;9,325.95,613.08,16.81,7.77">A mean-field games laboratory for generative modeling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Benjamin J Zhang and Markos A Katsoulakis. A mean-field games laboratory for generative mod- eling. arXiv preprint arXiv:2304.13534, 2023.</note>
</biblStruct>

<biblStruct coords="9,319.32,628.91,238.69,8.58;9,325.95,639.68,232.05,7.77;9,325.95,649.48,188.11,7.93" xml:id="b122">
	<monogr>
		<title level="m" type="main" coords="9,341.38,639.68,216.62,7.77;9,325.95,649.64,37.14,7.77">Dpvi: A dynamic-weight particle-based variational inference framework</title>
		<author>
			<persName coords=""><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00945</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Zhang et al., 2021a] Chao Zhang, Zhijian Li, Hui Qian, and Xin Du. Dpvi: A dynamic-weight particle-based variational inference framework. arXiv preprint arXiv:2112.00945, 2021.</note>
</biblStruct>

<biblStruct coords="9,319.32,665.47,238.69,8.58;9,325.95,676.24,232.05,7.77;9,325.95,686.20,232.05,7.77;9,325.95,696.00,206.26,7.93" xml:id="b123">
	<analytic>
		<title level="a" type="main" coords="9,471.14,676.24,86.87,7.77;9,325.95,686.20,232.05,7.77;9,325.95,696.16,79.06,7.77">Wasserstein flow meets replicator dynamics: A mean-field analysis of representation learning in actor-critic</title>
		<author>
			<persName coords=""><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,411.42,696.00,28.98,7.73">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15993" to="16006"/>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang et al., 2021b] Yufeng Zhang, Siyu Chen, Zhuoran Yang, Michael Jordan, and Zhaoran Wang. Wasserstein flow meets replicator dynamics: A mean-field analysis of representation learning in actor-critic. NeurIPS, 34:15993-16006, 2021.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
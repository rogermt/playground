{
    "paper_id": "NSGF",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2025-10-31T18:33:26.668973Z"
    },
    "title": "Neural Sinkhorn Gradient Flow",
    "authors": [
        {
            "first": "Huminhao",
            "middle": [],
            "last": "Zhu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Zhejiang University",
                "location": {}
            },
            "email": "zhuhuminhao@zju.edu.cn"
        },
        {
            "first": "Fangyikang",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Zhejiang University",
                "location": {}
            },
            "email": "wangfangyikang@zju.edu.cn"
        },
        {
            "first": "Chao",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Zhejiang University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Hanbin",
            "middle": [],
            "last": "Zhao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Zhejiang University",
                "location": {}
            },
            "email": "zhaohanbin@zju.edu.cn"
        },
        {
            "first": "Hui",
            "middle": [],
            "last": "Qian",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Zhejiang University",
                "location": {}
            },
            "email": "qianhui@zju.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature.Recently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures.In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation.Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field.To further enhance model efficiency on high-dimensional tasks, a two-phase NSGF++ model is devised, which first follows the Sinkhorn flow to approach the image manifold quickly (\u2264 5 NFEs) and then refines the samples along a simple straight flow.Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed methods.",
    "pdf_parse": {
        "paper_id": "NSGF",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature.Recently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures.In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation.Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field.To further enhance model efficiency on high-dimensional tasks, a two-phase NSGF++ model is devised, which first follows the Sinkhorn flow to approach the image manifold quickly (\u2264 5 NFEs) and then refines the samples along a simple straight flow.Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The Wasserstein Gradient Flow (WGF) with respect to certain specific functional objective F (denoted as F Wasserstein gradient flow) is a powerful tool for solving optimization problems over the Wasserstein probability space.Since the seminal work of [Jordan et al., 1998 ] which shows that the Fokker-Plank equation is the Wasserstein gradient flow with respect to the free energy, Wasserstein gradient flow w.r.t.different functionals have been widely used in various machine learning tasks such as Bayesian inference [Zhang et al., 2021a] , reinforcement learning [Zhang et al., 2021b] , and mean-field games [Zhang and Katsoulakis, 2023] .",
                "cite_spans": [
                    {
                        "start": 251,
                        "end": 271,
                        "text": "[Jordan et al., 1998",
                        "ref_id": null
                    },
                    {
                        "start": 520,
                        "end": 541,
                        "text": "[Zhang et al., 2021a]",
                        "ref_id": null
                    },
                    {
                        "start": 567,
                        "end": 588,
                        "text": "[Zhang et al., 2021b]",
                        "ref_id": null
                    },
                    {
                        "start": 612,
                        "end": 641,
                        "text": "[Zhang and Katsoulakis, 2023]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introdution",
                "sec_num": "1"
            },
            {
                "text": "One recent trend in the Wasserstein gradient flow literature is to develop efficient generative modeling methods [Gao et al., 2019; Gao et al., 2022; Ansari et al., 2021; Mokrov et al., 2021; Alvarez-Melis et al., 2022; Bunne et al., 2022; Fan et al., 2022] .In general, these methods mimic the Wasserstein gradient flow with respect to a specific distribution metric, driving a source distribution towards a target distribution.Neural networks are typically employed to approximate the computationally challenging components of the underlying Wasserstein gradient flow such as the timedependent transport maps.During the training process of these methods, it is common to require samples from the target distribution.After the training process, an inference procedure is often employed to generate new samples from the target distribution This procedure involves iteratively transporting samples from the source distribution with the assistance of the trained neural network.Based on the chosen metric, these methods can be categorized into two main types.",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 131,
                        "text": "[Gao et al., 2019;",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 132,
                        "end": 149,
                        "text": "Gao et al., 2022;",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 150,
                        "end": 170,
                        "text": "Ansari et al., 2021;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 171,
                        "end": 191,
                        "text": "Mokrov et al., 2021;",
                        "ref_id": null
                    },
                    {
                        "start": 192,
                        "end": 219,
                        "text": "Alvarez-Melis et al., 2022;",
                        "ref_id": null
                    },
                    {
                        "start": 220,
                        "end": 239,
                        "text": "Bunne et al., 2022;",
                        "ref_id": null
                    },
                    {
                        "start": 240,
                        "end": 257,
                        "text": "Fan et al., 2022]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introdution",
                "sec_num": "1"
            },
            {
                "text": "Divergences Between Distributions With Exact Same Supports.The first class of widely used metrics is the fdivergence, such as the Kullback-Leibler divergence and the Jensen-Shannon divergence.These divergences are defined based on the density ratio between two distributions and are only well-defined when dealing with distributions that have exactly the same support.Within the scope of f-divergence Wasserstein gradient flow generative models, neural networks are commonly utilized to formulate density-ratio estimators, as demonstrated by [Gao et al., 2019; Ansari et al., 2021] and [Heng et al., 2022] .However, as one can only access finite samples from target distributions in the training process, the support shift between the sample collections from the compared distributions may cause significant approximation error in the density-ratio estimators [Choi et al., 2022 ].An alternative approach, proposed by [Fan et al., 2022] , circumvents these limitations by employing a dual variational formulation of the f-divergence.In this framework, two networks are employed to approximate the optimal variational function and the transport maps.These two components are optimized alternately.It's imperative to highlight that the non-convex and non-concave characteristics of their min-max objective can render the training inherently unstable [Hsieh et al., 2021] .",
                "cite_spans": [
                    {
                        "start": 542,
                        "end": 560,
                        "text": "[Gao et al., 2019;",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 561,
                        "end": 581,
                        "text": "Ansari et al., 2021]",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 586,
                        "end": 605,
                        "text": "[Heng et al., 2022]",
                        "ref_id": null
                    },
                    {
                        "start": 860,
                        "end": 878,
                        "text": "[Choi et al., 2022",
                        "ref_id": null
                    },
                    {
                        "start": 918,
                        "end": 936,
                        "text": "[Fan et al., 2022]",
                        "ref_id": null
                    },
                    {
                        "start": 1348,
                        "end": 1368,
                        "text": "[Hsieh et al., 2021]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introdution",
                "sec_num": "1"
            },
            {
                "text": "Divergences Between Distributions With Possible Different Supports.Another type of generative Wasserstein gradient flow model employs divergences that are welldefined for distributions with possible different supports.This includes free energy fuctionals [Mokrov et al., 2021 ; Bunne et al., 2022] , the kernel-based metrics such as the Maximum-Mean/Sobolev Discrepancy [Mroueh et al., 2019; Mroueh and Rigotti, 2020] and sliced-Wasserstein distance [Liutkus et al., 2019; Du et al., 2023] .As these divergences can be efficiently approximated with samples, neural networks are typically used to directly model the transport maps used in the inference procedure.In Wasserstein gradient flow methods, input convex neural networks (ICNNs, [Amos et al., 2017] ) are commonly used to approximate the transport map.However, recently, several works [Korotin et al., 2021] discuss the poor expressiveness of ICNNs architecture and show that it would result in poor performance in high-dimension applications.Besides, the Maximum-Mean/Sobolev discrepancy Wasserstein gradient flow models are usually hard to train and are easy to trapped in poor local optima in practice [Arbel et al., 2019] , since the kernel-based divergences are highly sensitive to the parameters of the kernel function [Li et al., 2017; Wang et al., 2018] . [Liutkus et al., 2019; Du et al., 2023] consider sliced-Wasserstein WGF to build nonparametric generative Models which do not achieve high generation quality, it is an interesting work on how to combine sliced-Wasserstein WGF and neural network methods.",
                "cite_spans": [
                    {
                        "start": 255,
                        "end": 275,
                        "text": "[Mokrov et al., 2021",
                        "ref_id": null
                    },
                    {
                        "start": 278,
                        "end": 297,
                        "text": "Bunne et al., 2022]",
                        "ref_id": null
                    },
                    {
                        "start": 370,
                        "end": 391,
                        "text": "[Mroueh et al., 2019;",
                        "ref_id": null
                    },
                    {
                        "start": 392,
                        "end": 417,
                        "text": "Mroueh and Rigotti, 2020]",
                        "ref_id": null
                    },
                    {
                        "start": 450,
                        "end": 472,
                        "text": "[Liutkus et al., 2019;",
                        "ref_id": null
                    },
                    {
                        "start": 473,
                        "end": 489,
                        "text": "Du et al., 2023]",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 737,
                        "end": 756,
                        "text": "[Amos et al., 2017]",
                        "ref_id": null
                    },
                    {
                        "start": 843,
                        "end": 865,
                        "text": "[Korotin et al., 2021]",
                        "ref_id": null
                    },
                    {
                        "start": 1163,
                        "end": 1183,
                        "text": "[Arbel et al., 2019]",
                        "ref_id": null
                    },
                    {
                        "start": 1283,
                        "end": 1300,
                        "text": "[Li et al., 2017;",
                        "ref_id": null
                    },
                    {
                        "start": 1301,
                        "end": 1319,
                        "text": "Wang et al., 2018]",
                        "ref_id": null
                    },
                    {
                        "start": 1322,
                        "end": 1344,
                        "text": "[Liutkus et al., 2019;",
                        "ref_id": null
                    },
                    {
                        "start": 1345,
                        "end": 1361,
                        "text": "Du et al., 2023]",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introdution",
                "sec_num": "1"
            },
            {
                "text": "Contribution.In this paper, we investigate the Wasserstein gradient flow with respect to the Sinkhorn divergence, which is categorized under the second type of divergence and does not necessitate any kernel functions.We introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Sinkhorn Wasserstein gradient flow from a specified source distribution.The NSGF employs a velocity field matching scheme that demands only samples from the target distribution to calculate empirical velocity field approximations.Our theoretical analyses show that as the sample size approaches infinity, the mean-field limit of the empirical approximation converges to the true velocity field of the Sinkhorn Wasserstein gradient flow.Given distinct source and target data samples, our NSGF can be harnessed across a wide range of machine learning applications, including unconditional/conditional image generation, style transfer, and audiotext translation.To further enhance model efficiency on highdimensional image datasets, a two-phase NSGF++ model is devised, which first follows the Sinkhorn flow to approach the image manifold quickly (\u2264 5 NFEs) and then refine the samples along a simple straight flow.A novel phase-transition time predictor is proposed to transfer between the two phases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introdution",
                "sec_num": "1"
            },
            {
                "text": "We empirically validate NSGF on low-dimensional 2D data and NSGF++ on benchmark images (MNIST, CIFAR-10).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introdution",
                "sec_num": "1"
            },
            {
                "text": "Our findings indicate that our models can be trained to yield commendable results in terms of generation cost and sample quality, surpassing the performance of the neural Wasserstein gradient flow methods previously tested on CIFAR-10, to the best of our knowledge.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introdution",
                "sec_num": "1"
            },
            {
                "text": "Sinkhorn Divergence in Machine Learning.Originally introduced in the domain of optimal transport, the Sinkhorn divergence emerged as a more computationally tractable alternative to the classical Wasserstein distance [Cuturi, 2013; Peyr\u00e9 et al., 2017; Feydy et al., 2019] .Since its inception, Sinkhorn divergence has found applications across a range of machine learning tasks, including domain adaptation [Alaya et al., 2019; Komatsu et al., 2021] , Sinkhorn barycenter [Luise et al., 2019; Shen et al., 2020] and color transfer [Pai et al., 2021] .Indeed, it has already been extended to singlestep generative modeling methods, such as the Sinkhorn GAN and VAE [Genevay et al., 2018; Deja et al., 2020; Patrini et al., 2020] .However, to the best of our knowledge, it has yet to be employed in developing efficient generative Wasserstein gradient flow models.",
                "cite_spans": [
                    {
                        "start": 216,
                        "end": 230,
                        "text": "[Cuturi, 2013;",
                        "ref_id": null
                    },
                    {
                        "start": 231,
                        "end": 250,
                        "text": "Peyr\u00e9 et al., 2017;",
                        "ref_id": null
                    },
                    {
                        "start": 251,
                        "end": 270,
                        "text": "Feydy et al., 2019]",
                        "ref_id": null
                    },
                    {
                        "start": 406,
                        "end": 426,
                        "text": "[Alaya et al., 2019;",
                        "ref_id": null
                    },
                    {
                        "start": 427,
                        "end": 448,
                        "text": "Komatsu et al., 2021]",
                        "ref_id": null
                    },
                    {
                        "start": 471,
                        "end": 491,
                        "text": "[Luise et al., 2019;",
                        "ref_id": "BIBREF77"
                    },
                    {
                        "start": 492,
                        "end": 510,
                        "text": "Shen et al., 2020]",
                        "ref_id": null
                    },
                    {
                        "start": 530,
                        "end": 548,
                        "text": "[Pai et al., 2021]",
                        "ref_id": "BIBREF87"
                    },
                    {
                        "start": 663,
                        "end": 685,
                        "text": "[Genevay et al., 2018;",
                        "ref_id": null
                    },
                    {
                        "start": 686,
                        "end": 704,
                        "text": "Deja et al., 2020;",
                        "ref_id": null
                    },
                    {
                        "start": 705,
                        "end": 726,
                        "text": "Patrini et al., 2020]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "2"
            },
            {
                "text": "Neural ODE/SDE Based Diffusion Models.Recently, diffusion models, as a class of Neural ODE/SDE Based generative methods have achieved unprecedented success, which also transforms a simple density to the target distribution, iteratively [Song and Ermon, 2019; Ho et al., 2020; Song et al., 2021] .Typically, each step of diffusion models only progresses a little by denoising a simple Gaussian noise, while each step in WGF models follows the most informative direction (in a certain sense).Hence, diffusion models usually have a long inference trajectory.In recent research undertakings, there has been a growing interest in exploring more informative steps within diffusion models.Specifically, flow matching methods [Lipman et al., 2023; Liu et al., 2023; Albergo and Vanden-Eijnden, 2023] establish correspondence between the source and target via optimal transport, subsequently crafting a probability path by directly linking data points from both ends.Notably, when the source and target are both Gaussians, their path is actually a Wasserstein gradient flow.However, this property does not consistently hold for general data probabilities.Moreover, [Tong et al., 2023; Pooladian et al., 2023] consider calculating the minibatch op-timal transport map to guide data points connecting.Besides, [Das et al., 2023] consider the shortest forward diffusion path for the Fisher metric and [Shaul et al., 2023] explore the conditional Gaussian probability path based on the principle of minimizing the Kinetic Energy.Nonetheless, a commonality among many of these methods is their reliance on Gaussian paths for theoretical substantiation, thereby constraining the broader applicability of these techniques within real-world generative modeling.",
                "cite_spans": [
                    {
                        "start": 236,
                        "end": 258,
                        "text": "[Song and Ermon, 2019;",
                        "ref_id": null
                    },
                    {
                        "start": 259,
                        "end": 275,
                        "text": "Ho et al., 2020;",
                        "ref_id": null
                    },
                    {
                        "start": 276,
                        "end": 294,
                        "text": "Song et al., 2021]",
                        "ref_id": null
                    },
                    {
                        "start": 718,
                        "end": 739,
                        "text": "[Lipman et al., 2023;",
                        "ref_id": null
                    },
                    {
                        "start": 740,
                        "end": 757,
                        "text": "Liu et al., 2023;",
                        "ref_id": null
                    },
                    {
                        "start": 758,
                        "end": 791,
                        "text": "Albergo and Vanden-Eijnden, 2023]",
                        "ref_id": null
                    },
                    {
                        "start": 1156,
                        "end": 1175,
                        "text": "[Tong et al., 2023;",
                        "ref_id": null
                    },
                    {
                        "start": 1176,
                        "end": 1199,
                        "text": "Pooladian et al., 2023]",
                        "ref_id": "BIBREF95"
                    },
                    {
                        "start": 1299,
                        "end": 1317,
                        "text": "[Das et al., 2023]",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 1389,
                        "end": 1409,
                        "text": "[Shaul et al., 2023]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "2"
            },
            {
                "text": "We denote x = (x 1 , \u2022 \u2022 \u2022 , x d ) \u2208 R d and X \u2282 R d as a vector and a compact ground set in R d , respectively.For a given point x \u2208 X , \u2225x\u2225 p := ( i x p i )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Notations",
                "sec_num": "3.1"
            },
            {
                "text": "1 p denotes the p-norm on euclidean space, and \u03b4 x stands for the Dirac (unit mass) distribution at point x \u2208 X .P 2 (X ) denotes the set of probability measures on X with finite second moment and C(X ) denotes the space of continuous functions on X .For a given functional F(\u2022) : P 2 (X ) \u2192 R, \u03b4F (\u00b5t) \u03b4\u00b5 (\u2022) : R d \u2192 R denotes its first variation at \u00b5 = \u00b5 t .Besides, we use \u2207 and \u2207 \u2022 () to denote the gradient and the divergence operator, respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Notations",
                "sec_num": "3.1"
            },
            {
                "text": "We first introduce the background of Wasserstein distance.Given two probability measures \u00b5, \u03bd \u2208 P 2 (X ), the p-Wasserstein distance W p (\u00b5, \u03bd) : P 2 (X ) \u00d7 P 2 (X ) \u2192 R + is defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "W p (\u00b5, \u03bd) = inf \u03c0\u2208\u03a0(\u00b5,\u03bd) X \u00d7X \u2225x -y\u2225 p d\u03c0(x, y) 1 p ,",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "where \u03a0(\u00b5, \u03bd) denotes the set of all probability couplings \u03c0 with marginals \u00b5 and \u03bd.The W p distance aims to find a coupling \u03c0 so as to minimize the cost function \u2225x -y\u2225 p of moving a probability mass from \u00b5 to \u03bd.It has been demonstrated that the p-Wasserstein distance is a valid metric on P 2 (X ), and (P 2 (X ), W p ) is referred to as the Wasserstein probability space [Villani and others, 2009] .",
                "cite_spans": [
                    {
                        "start": 374,
                        "end": 400,
                        "text": "[Villani and others, 2009]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "Note that directly calculating W p is computationally expensive, especially for high dimensional problems [Santambrogio, 2015] .Consequently, the entropy-regularized Wasserstein distance [Cuturi, 2013] is proposed to approximate equation equation 1 by regularizing the original problem with an entropy term: Definition 1.The entropy-regularized Wasserstein distance is formally defined as:",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 126,
                        "text": "[Santambrogio, 2015]",
                        "ref_id": null
                    },
                    {
                        "start": 187,
                        "end": 201,
                        "text": "[Cuturi, 2013]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "Wp,\u03b5(\u00b5, \u03bd) = inf \u03c0\u2208\u03a0(\u00b5,\u03bd) X \u00d7X",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "\u2225x -y\u2225 p d\u03c0(x, y)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "1 p + \u03b5KL(\u03c0|\u00b5 \u2297 \u03bd) ,",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "where \u03b5 > 0 is a regularization coefficient, \u00b5 \u2297 \u03bd denotes the product measure, i.e., \u00b5 \u2297 \u03bd(x, y) = \u00b5(x)\u03bd(y), and KL(\u03c0|\u00b5\u2297\u03bd) denotes the KL-divergence between \u03c0 and \u00b5\u2297\u03bd.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "Generally, the computational cost of W p,\u03b5 is much lower than W p , and can be efficiently calculated with Sinkhorn algorithms [Cuturi, 2013] .Without loss of generality, we fix p = 2 and abbreviate W 2,\u03b5 := W \u03b5 for ease of notion in the whole paper.According to Fenchel-Rockafellar theorem, the entropy-regularized Wasserstein problem W \u03b5 equation 2 has an equivalent dual formulation, which is given as follows [Peyr\u00e9 et al., 2017] :",
                "cite_spans": [
                    {
                        "start": 127,
                        "end": 141,
                        "text": "[Cuturi, 2013]",
                        "ref_id": null
                    },
                    {
                        "start": 413,
                        "end": 433,
                        "text": "[Peyr\u00e9 et al., 2017]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "W \u03b5 (\u00b5, \u03bd) = max f,g\u2208C(X ) \u27e8\u00b5, f \u27e9 + \u27e8\u03bd, g\u27e9 -\u03b5 \u00b5 \u2297 \u03bd, exp 1 \u03b5 (f \u2295 g -C) -1 ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "where C is the cost function in equation 2 and f \u2295 g is the tensor sum: (x, y) \u2208 X 2 \u2192 f (x) + g(y).The maximizers f \u00b5,\u03bd and g \u00b5,\u03bd of equation 3 are called the W \u03b5 -potentials of W \u03b5 (\u00b5, \u03bd).The following lemma states the optimality condition for the W \u03b5 -potentials: Lemma 1. (Optimality [Cuturi, 2013] ) The W \u03b5 -potentials (f \u00b5,\u03bd , g \u00b5,\u03bd ) exist and are unique (\u00b5, \u03bd)-a.e. up to an additive constant (i.e.\u2200K \u2208 R, (f \u00b5,\u03bd + K, g \u00b5,\u03bd -K) is optimal).Moreover,",
                "cite_spans": [
                    {
                        "start": 288,
                        "end": 302,
                        "text": "[Cuturi, 2013]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "W \u03b5 (\u00b5, \u03bd) = \u27e8\u00b5, f \u00b5,\u03bd \u27e9 + \u27e8\u03bd, g \u00b5,\u03bd \u27e9. (",
                        "eq_num": "4"
                    }
                ],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "We describe such the method in Appendix A for completeness.Note that, although computationally more efficient than the W p distance, the W \u03b5 distance is not a true metric, as there exists \u00b5 \u2208 P 2 (X ) such that W \u03b5 (\u00b5, \u00b5) \u0338 = 0 when \u03b5 \u0338 = 0, which restricts the applicability of W \u03b5 .As a result, the following Sinkhorn divergence S \u03b5 (\u00b5, \u03bd) : P 2 (X )\u00d7P 2 (X ) \u2192 R is proposed [Peyr\u00e9 et al., 2017] : Definition 2. Sinkhorn divergence:",
                "cite_spans": [
                    {
                        "start": 378,
                        "end": 398,
                        "text": "[Peyr\u00e9 et al., 2017]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "S \u03b5 (\u00b5, \u03bd) = W \u03b5 (\u00b5, \u03bd) - 1 2 (W \u03b5 (\u00b5, \u00b5) + W \u03b5 (\u03bd, \u03bd)) .",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "S \u03b5 (\u00b5, \u03bd) is nonnegative, bi-convex thus a valid metric on P 2 (X ) and metricize the convergence in law.Actually S \u03b5 (\u00b5, \u03bd) interpolates the Wasserstein distance (\u03f5 \u2192 0) and the Maximum Mean Discrepancy (\u03f5 \u2192 \u221e) [Feydy et al., 2019] .",
                "cite_spans": [
                    {
                        "start": 213,
                        "end": 233,
                        "text": "[Feydy et al., 2019]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Wasserstein distance and Sinkhorn divergence",
                "sec_num": "3.2"
            },
            {
                "text": "Consider an optimization problem over P 2 (X ):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gradient flows",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "min \u00b5\u2208P2(X ) F(\u00b5) := D(\u00b5|\u00b5 * ). (",
                        "eq_num": "6"
                    }
                ],
                "section": "Gradient flows",
                "sec_num": "3.3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gradient flows",
                "sec_num": "3.3"
            },
            {
                "text": "where \u00b5 * is the target distribution, D is the divergence we choose.We consider now the problem of transporting mass from an initial distribution \u00b5 0 to a target distribution \u00b5 * , by finding a continuous probability path \u00b5 t starting from \u00b5 0 = \u00b5 that converges to \u00b5 * while decreasing F(\u00b5 t ).To solve this optimization problem, one can consider a descent flow of F(\u00b5) in the Wasserstein space, which transports any initial distribution \u00b5 0 towards the target distribution \u00b5 * .Specifically, the descent flow of F(\u00b5) is described by the following continuity equation [Ambrosio et al., 2005; Villani and others, 2009; Santambrogio, 2017] :",
                "cite_spans": [
                    {
                        "start": 569,
                        "end": 592,
                        "text": "[Ambrosio et al., 2005;",
                        "ref_id": null
                    },
                    {
                        "start": 593,
                        "end": 618,
                        "text": "Villani and others, 2009;",
                        "ref_id": null
                    },
                    {
                        "start": 619,
                        "end": 638,
                        "text": "Santambrogio, 2017]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gradient flows",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u2202\u00b5 t (x) \u2202t = -\u2207 \u2022 (\u00b5 t (x)v t (x)).",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Gradient flows",
                "sec_num": "3.3"
            },
            {
                "text": "where v \u00b5t : X \u2192 X is a velocity field that defines the direction of position transportation.To ensure a descent of F(\u00b5 t ) over time t, the velocity field v \u00b5t should satisfy the following inequality ( [Ambrosio et al., 2005] ):",
                "cite_spans": [
                    {
                        "start": 203,
                        "end": 226,
                        "text": "[Ambrosio et al., 2005]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gradient flows",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "dF(\u00b5 t ) dt = \u27e8\u2207 \u03b4F(\u00b5 t ) \u03b4\u00b5 , v t \u27e9d\u00b5 t \u2264 0.",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Gradient flows",
                "sec_num": "3.3"
            },
            {
                "text": "A straightforward choice of v t is v t = -\u2207 \u03b4F (\u00b5t) \u03b4\u00b5 , which is actually the steepest descent direction of F(\u00b5 t ).When we select this v t , we refer to the aforementioned continuous equation as the Wasserstein gradient flow of F. We give the definition of the first variation in the appendix for the sake of completeness of the article.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gradient flows",
                "sec_num": "3.3"
            },
            {
                "text": "In this section, we first introduce the Sinkhorn Wasserstein gradient flow and investigate its convergence properties.Then, we develop our Neural Sinkhorn Gradient Flow model, which consists of a velocity field matching training procedure and a velocity field guided inference procedure.Moreover, we theoretically show that the mean-field limit of the empirical approximation used in the training procedure converges to the true velocity field of the Sinkhorn Wasserstein gradient flow.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "4"
            },
            {
                "text": "Based on the definition of the Sinkhorn divergence, we construct our Sinkhorn objective F \u03b5 (\u2022) = S \u03b5 (\u2022, \u00b5 * ), where \u00b5 * denotes the target distribution.The following theorem gives the first variation of the Sinkhorn objective.Theorem 1. (First variation of the Sinkhorn objective [Luise et al., 2019] ",
                "cite_spans": [
                    {
                        "start": 283,
                        "end": 303,
                        "text": "[Luise et al., 2019]",
                        "ref_id": "BIBREF77"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sinkhorn Wasserstein gradient flow",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": ") Let \u03b5 > 0. Let (f \u00b5,\u00b5 * , g \u00b5,\u00b5 * ) be the W \u03b5 -potentials of W \u03b5 (\u00b5, \u00b5 * ) and (f \u00b5,\u00b5 , g \u00b5,\u00b5 ) be the W \u03b5 - potentials of W \u03b5 (\u00b5, \u00b5). The first variation of the Sinkhorn objective F \u03b5 is \u03b4F \u03b5 \u03b4\u00b5 = f \u00b5,\u00b5 * -f \u00b5,\u00b5 .",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Sinkhorn Wasserstein gradient flow",
                "sec_num": "4.1"
            },
            {
                "text": "According to Theorem 1, we can construct the Sinkhorn Wasserstein gradient flow by setting the velocity field v t in the continuity equation equation 7 as v F\u03b5 \u00b5t = -\u2207 \u03b4F\u03b5(\u00b5t) \u03b4\u00b5t = \u2207f \u00b5t,\u00b5t -\u2207f \u00b5t,\u00b5 * .Proposition 1.Consider the Sinkhorn Wasserstein gradient flow described by the following continuity equation:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sinkhorn Wasserstein gradient flow",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u2202\u00b5 t (x) \u2202t = -\u2207 \u2022 (\u00b5 t (x)(\u2207f \u00b5t,\u00b5t (x) -\u2207f \u00b5t,\u00b5 * (x))). (",
                        "eq_num": "10"
                    }
                ],
                "section": "Sinkhorn Wasserstein gradient flow",
                "sec_num": "4.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sinkhorn Wasserstein gradient flow",
                "sec_num": "4.1"
            },
            {
                "text": "The following local descending property of F \u03b5 holds:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sinkhorn Wasserstein gradient flow",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "dF \u03b5 (\u00b5 t ) dt = -\u2225\u2207f \u00b5t,\u00b5t (x) -\u2207f \u00b5t,\u00b5 * (x)\u2225 2 d\u00b5 t ,",
                        "eq_num": "(11)"
                    }
                ],
                "section": "Sinkhorn Wasserstein gradient flow",
                "sec_num": "4.1"
            },
            {
                "text": "where the r.h.s.equals 0 if and only if \u00b5 t = \u00b5 * .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sinkhorn Wasserstein gradient flow",
                "sec_num": "4.1"
            },
            {
                "text": "We now present our NSGF method, the core of which lies in training a neural network to approximate the time-varying velocity field v F\u03b5 \u00b5t induced by Sinkhorn Wasserstein gradient flow.Given a target probability density path \u00b5 t (x) and it's corresponding velocity field v S\u03b5 \u00b5t , which generates \u00b5 t (x), we define the velocity field matching objective as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "min \u03b8 E t\u223c[0,T ],x\u223c\u00b5t v \u03b8 (x, t) -v S\u03b5 \u00b5t (x) 2 . (",
                        "eq_num": "12"
                    }
                ],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "To construct our algorithm, we utilize independently and identically distributed (i.i.d) samples denoted as {Y i } n i=1 \u2208 R d , which are drawn from an unknown target distribution \u00b5 * a common practice in the field of generative modeling.Given the current set of samples { Xt i } n i=1 \u223c \u00b5 t , our method calculates the velocity field using the W \u03b5 -potentials (Lemma 1) f \u03bct,\u03bc * and f \u03bct,\u03bct based on samples.Here, \u03bct and \u03bc * represent discrete Dirac distributions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "Remark 1.In the discrete case, W \u03b5 -potentials equation 1 can be computed by a standard method in [Genevay et al., 2016] .In practice, we use the efficient implementation of the Sinkhorn algorithm with GPU acceleration from the Geom-Loss package [Feydy et al., 2019] .",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 120,
                        "text": "[Genevay et al., 2016]",
                        "ref_id": null
                    },
                    {
                        "start": 246,
                        "end": 266,
                        "text": "[Feydy et al., 2019]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "The corresponding finite sample velocity field approximation can be computed as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "vF\u03b5 \u03bct ( Xt i ) = \u2207 Xt i f \u03bct,\u03bct ( Xt i ) -\u2207 Xt i f \u03bct,\u03bc * ( Xt i ).",
                        "eq_num": "(13)"
                    }
                ],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "Subsequently, we derive the particle formulation corresponding to the flow formulation equation 10.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "d Xt i = vF\u03f5 \u03bct Xt i dt, i = 1, 2, \u2022 \u2022 \u2022 n.",
                        "eq_num": "(14)"
                    }
                ],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "In the following proposition, we investigate the mean-field limit of the particle set",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "{ Xt i } i=1,\u2022\u2022\u2022 ,M . Theorem 2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "(Mean-field limits.)Suppose the empirical distribution \u03bc0 of M particles weakly converges to a distribution \u00b5 0 when M \u2192 \u221e.Then, the path of equation equation 14 starting from \u03bc0 weakly converges to a solution of the following partial differential equation starting from \u00b5 0 when M \u2192 \u221e:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u2202\u00b5 t (x) \u2202t = -\u2207 \u2022 (\u00b5 t (x)\u2207 \u03b4F \u03b5 (\u00b5 t ) \u03b4\u00b5 t ). (",
                        "eq_num": "15"
                    }
                ],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "which is actually the gradient flow of Sinkhorn divergence F \u03b5 in the Wasserstein space.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "The following proposition shows that the goal of the velocity field matching objective equation 12 can be regarded as approximating the steepest local descent direction with neural networks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "Proposition 2. (Steepest local descent direction.)Consider the infinitesimal transport T (x) = x + \u03bb\u03d5.The Fr\u00e9chet derivative under this particular perturbation,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "d d\u03bb F \u03b5 (T # \u00b5)| \u03bb=0 = lim \u03bb\u21920 F \u03b5 (T # \u00b5) -F \u03b5 (\u00b5) \u03bb = X \u2207f \u00b5,\u00b5 * (x)\u03d5(x)d\u00b5 - X \u2207f \u00b5,\u00b5 (x)\u03d5(x)d\u00b5,",
                        "eq_num": "(16)"
                    }
                ],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "and the steepest local descent direction is \u03d5",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "= \u2207f \u00b5,\u00b5 * (x)-f\u00b5,\u00b5(x) \u2225\u2207f \u00b5,\u00b5 * (x)-f\u00b5,\u00b5(x)\u2225 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "/ X0 i \u223c \u00b5 0 , \u1ef8i \u223c \u00b5 * , i = 1, 2, \u2022 \u2022 \u2022 n. for t = 0, 1, \u2022 \u2022 \u2022 T do calculatef \u03bct,\u03bct Xt i , f \u03bct,\u03bc * Xt i . vF\u03f5 \u00b5t Xt i = \u2207f \u03bct,\u03bct Xt i -\u2207f \u03bct,\u03bc * Xt i . Xt+1 i = Xt i + \u03b7 vF\u03f5 t Xt i . store all Xt i , vF\u03f5 t Xt i pair into the pool, i = 1, 2, \u2022 \u2022 \u2022 n.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "/ * velocity field matching * / while Not convergence do from trajectory pool sample pair Xt i , vF\u03f5",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "t Xt i . L(\u03b8) = v \u03b8 ( Xt i , t) -vF\u03b5 \u00b5t Xt i 2 , \u03b8 \u2190 \u03b8 -\u03b3\u2207 \u03b8 L (\u03b8",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "i \u223c \u03bc0 for t = 0, 1, \u2022 \u2022 \u2022 T do Xt+1 i = Xt i + \u03b7v \u03b8 Xt i , t , i = 1, 2, \u2022 \u2022 \u2022 n. Output: XT",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "i as the results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Velocity-fields Matching",
                "sec_num": "4.2"
            },
            {
                "text": "According to Theorem 2, we construct our NSGF method based on minibatches.We utilize a set of discrete targets, denoted as {Y i } n i=1 , where n represents the batch size, to construct the Sinkhorn Gradient Flow starting from random Gaussian noise or other initial distributions.As indicated by Theorem 2, the mean-field limit converges to the true Sinkhorn Gradient flow when the batch size approaches \u221e.Note that in practice, we only use a moderate batch size for computation efficiency, and the experimental results demonstrate that this works well for practical generative tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Minibatch Sinkhorn Gradient Flow and Experience Replay",
                "sec_num": "4.3"
            },
            {
                "text": "Considering the balance between expensive training costs and training quality, we opted to first build a trajectory pool of Sinkhorn gradient flow and then sample from it to construct the velocity field matching algorithm.Our method draws inspiration from experience replay, a common technique in reinforcement learning, adapting it to enhance our model's effectiveness [Mnih et al., 2013; Silver et al., 2016 ].Once we calculate the time-varying velocity field vs \u00b5t ( Xt i ), we can parameterize the velocity field using a straightforward regression method.The velocity field matching training procedure is outlined in Algorithm 1. Once obtained a feasible velocity field approximation v \u03b8 , one can generate new samples by iteratively employing the explicit Euler discretization of the Equation equation 14 to drive the samples to the target.Note that various other numerical schemes, such as the implicit Euler method [Platen and Bruti-Liberati, 2010] and Runge-Kutta methods [Butcher, 1964] , can be employed.In this study, we opt for the firstorder explicit Euler discretization method [S\u00fcli and Mayers, 2003] due to its simplicity and ease of implementation.We leave the exploration of higher-order algorithms for future research.",
                "cite_spans": [
                    {
                        "start": 370,
                        "end": 389,
                        "text": "[Mnih et al., 2013;",
                        "ref_id": null
                    },
                    {
                        "start": 390,
                        "end": 409,
                        "text": "Silver et al., 2016",
                        "ref_id": null
                    },
                    {
                        "start": 922,
                        "end": 955,
                        "text": "[Platen and Bruti-Liberati, 2010]",
                        "ref_id": null
                    },
                    {
                        "start": 980,
                        "end": 995,
                        "text": "[Butcher, 1964]",
                        "ref_id": null
                    },
                    {
                        "start": 1092,
                        "end": 1115,
                        "text": "[S\u00fcli and Mayers, 2003]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Minibatch Sinkhorn Gradient Flow and Experience Replay",
                "sec_num": "4.3"
            },
            {
                "text": "In this subsection, we propose an approach to enhance the performance of NSGF on high-dimensional datasets.As a trajectory pool should be first constructed in the velocity field matching training procedure of NSGF, the storage and computation costs would greatly hinter the usage of NSGF in large-scale tasks.To tackle this problem, we propose a twophase NSGF++ algorithm, which first follows the Sinkhorn gradient flow to approach the image manifold quickly and then refine the samples along a simple straight flow.Specifically, our NSGF++ model consists of three components, (1) a NSGF model trained on T \u2264 5 time steps, (2) a Neural Straight Flow (NSF) model trained via velocity field matching on a straight flow X t \u223c (1 -t)P 0 + tP 1 , t \u2208 [0, 1], which has also been used in existing FM models, (3) a phasetransition time predictor to transfer from NSGF to the NSF.Here, we train the time predictor t \u03d5 : X \u2192 [0, 1] with the following regression objective:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NSGF++",
                "sec_num": "4.4"
            },
            {
                "text": "L(\u03d5) = E t\u2208U (0,1),Xt\u223cPt ||t -t \u03d5 (X t )|| 2 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NSGF++",
                "sec_num": "4.4"
            },
            {
                "text": "Note that the training of the straight NSF model and the time predictor is simulated free and need no extra storage.As a result, the training cost of NSFG++ is similar to existing FM models, since the computation cost of NSF is nearly the same as FM, and the 5-step NSGF and the time predictor is easy to train.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NSGF++",
                "sec_num": "4.4"
            },
            {
                "text": "In the inference of NSGF++, we first follow the NSGF with less than 5 NFEs form X 0 \u223c P 0 to obtain Xt , then transfer it with the time predictor t \u03d5 , and obtain our final output by refining the transferred sample with the NSF model from t \u03d5 ( Xt ), as shown in Figure 5 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 270,
                        "end": 271,
                        "text": "5",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "NSGF++",
                "sec_num": "4.4"
            },
            {
                "text": "We conduct an empirical investigation of the NSGF-based generative models (the standard NSGF and its two-phase variant NSGF++) across a range of experiments.Initially, we demonstrate how NSGF guides the evolution and convergence of particles from the initial distribution toward the target distribution in 2D simulation experiments.Subsequently, our attention turns to real-world image benchmarks, such as MNIST and CIFAR-10.To improve the efficiency in those high-dimensional tasks, we adopt the two-phase variant NSGF++ instead of the standard NSGF.Our method's adapt-ability to high-dimensional spaces is exemplified through experiments conducted on these datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "We assess the performance of various generative modeling models in low dimensions.Specifically, we conduct a comparative analysis between our method, NSGF, and several neural ODE-based diffusion models, including Flow Matching (FM; [Lipman et al., 2023] ), Rectified Flow (1,2,3-RF; [Liu et al., 2023] ), Optimal Transport Condition Flow Matching (OT-CFM; [Tong et al., 2023; Pooladian et al., 2023] ), Stochastic Interpolant (SI; [Albergo and Vanden-Eijnden, 2023] ), and neural gradient-flow-based models such as JKO-Flow [Fan et al., 2022] and EPT [Gao et al., 2022] .Our evaluation involves learning 2D distributions adapted from [Grathwohl et al., 2018] , which include multiple modes.",
                "cite_spans": [
                    {
                        "start": 232,
                        "end": 253,
                        "text": "[Lipman et al., 2023]",
                        "ref_id": null
                    },
                    {
                        "start": 283,
                        "end": 301,
                        "text": "[Liu et al., 2023]",
                        "ref_id": null
                    },
                    {
                        "start": 356,
                        "end": 375,
                        "text": "[Tong et al., 2023;",
                        "ref_id": null
                    },
                    {
                        "start": 376,
                        "end": 399,
                        "text": "Pooladian et al., 2023]",
                        "ref_id": "BIBREF95"
                    },
                    {
                        "start": 431,
                        "end": 465,
                        "text": "[Albergo and Vanden-Eijnden, 2023]",
                        "ref_id": null
                    },
                    {
                        "start": 524,
                        "end": 542,
                        "text": "[Fan et al., 2022]",
                        "ref_id": null
                    },
                    {
                        "start": 551,
                        "end": 569,
                        "text": "[Gao et al., 2022]",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 634,
                        "end": 658,
                        "text": "[Grathwohl et al., 2018]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2D simulation data",
                "sec_num": "5.1"
            },
            {
                "text": "Table 1 provides a comprehensive overview of our 2D experimental results, clearly illustrating the generalization capabilities of NSGF.Even when employing fewer steps.It is evident that neural gradient-flow-based models consistently outperform neural ODE-based diffusion models, particularly in low-step settings.This observation suggests that neural gradient-flow-based models generate more informative paths, enabling effective generation with a reduced number of steps.Furthermore, our results showcase the best performances among neural gradient-flow-based models, indicating that we have successfully introduced a lower error in approximating Wasserstein gradient flows.More complete details of the experiment can be found in the appendix E. In the absence of specific additional assertions, we adopted Euler steps as the inference steps.We present additional comparisons between neural ODEbased diffusion models and neural gradient-flow-based models, represented by NSGF and EPT, in Figure 3 , 4, which illustrates the flow at different steps from 0 to T .Our observations reveal that the velocity field induced by NSGF exhibits notably high-speed values right from the outset.This is attributed to the fact that NSGF follows the steepest descent direction within the probability space.In contrast, neural ODEbased diffusion models, particularly those based on stochastic interpolation, do not follow the steepest descent path in 2D experiments.Even with the proposed rectified flow method by [Liu et al., 2023] to straighten the path, these methods still necessitate more steps to reach the desired outcome.",
                "cite_spans": [
                    {
                        "start": 1499,
                        "end": 1517,
                        "text": "[Liu et al., 2023]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 996,
                        "end": 997,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "2D simulation data",
                "sec_num": "5.1"
            },
            {
                "text": "In this section, we illustrate the scalability of our algorithm to the high-dimensional setting by applying our methods to real image datasets.Notably, we leverage the two-phase variant (NSGF++) instead of the standard NSGF to enhance efficiency in high-dimensional spaces.It is worth mentioning that we achieve this improvement by constructing a significantly smaller training pool compared with the standard NSGF pool (10% of the usual size), thus requiring only 5% of the typical training duration.We evaluate NSGF++ on MNIST and CIFAR10 to show our generating ability.Due to the limit of the space, we defer the generative images and comparison results of MNIST in appendix 3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Image benchmark data",
                "sec_num": "5.2"
            },
            {
                "text": "We report sample quality using the standard Fr\u00e9chet Inception Distance (FID) [Heusel et al., 2017] , Inception Score (IS) [Salimans et al., 2016] and compute cost using the number of function evaluations (NFE).These are all standard metrics throughout the literature.",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 98,
                        "text": "[Heusel et al., 2017]",
                        "ref_id": null
                    },
                    {
                        "start": 122,
                        "end": 145,
                        "text": "[Salimans et al., 2016]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Image benchmark data",
                "sec_num": "5.2"
            },
            {
                "text": "Table 2 presents the results, including the Fr\u00e9chet Inception Distance (FID), Inception Score (IS), and the number of function evaluations (NFE), comparing the empirical distribution generated by each algorithm with the target distribution.While our current implementation may not yet rival state-of-the-art methods, it demonstrates promising outcomes, particularly in terms of generating quality (FID), outperforming neural gradient-flow-based models (EPT, [Gao et al., 2022] ; JKO-Flow, [Fan et al., 2022] ; DGGF,(LSIF-X 2 ) [Heng et al., 2022] ) with fewer steps.It's essential to emphasize that this work represents an initial exploration of this particular model category and has not undergone optimization",
                "cite_spans": [
                    {
                        "start": 458,
                        "end": 476,
                        "text": "[Gao et al., 2022]",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 489,
                        "end": 507,
                        "text": "[Fan et al., 2022]",
                        "ref_id": null
                    },
                    {
                        "start": 527,
                        "end": 546,
                        "text": "[Heng et al., 2022]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Image benchmark data",
                "sec_num": "5.2"
            },
            {
                "text": "This paper delves into the realm of Wasserstein gradient flow w.r.t. the Sinkhorn divergence as an alternative to kernel methods.Our main investigation revolves around the Neural Sinkhorn Gradient Flow (NSGF) model, which introduces a parameterized velocity field that evolves over time in the Sinkhorn gradient flow.One noteworthy aspect of the NSGF is its efficient velocity field matching, which relies solely on samples from the target distribution for empirical approximations.The combination of rigorous theoretical foundations and empirical observations demonstrates that our approximations of the velocity field converge toward their true counterparts as the sample sizes grow.To further enhance model efficiency on high-dimensional tasks, a two-phase NSGF++ model is devised, which first follows the Sinkhorn flow to approach the image manifold quickly and then refine the samples along a simple straight flow.Through extensive empirical experiments on well-known datasets like MNIST and CIFAR-10, we validate the effectiveness of the proposed methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "The W \u03b5 -potentials is the cornerstone to conduct NSGF.Hence, a key component of our method is to efficiently compute this quantity. [Genevay et al., 2016] provided an efficient method when both \u00b5 and \u03bd are discrete measures so that we can calculate W \u03b5 -potential in terms of samples.In particular, when \u00b5 is discrete, f can be simply represented by a finitedimensional vector since only its values on supp(\u00b5) matter.",
                "cite_spans": [
                    {
                        "start": 133,
                        "end": 155,
                        "text": "[Genevay et al., 2016]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "To more clearly explain the relationship between the calculation of W \u03b5 -protentials and the composition of our algorithm, we provide the following explanation: In practice, we actually calculate the W \u03b5 -potentials for the empirical distribution of discrete minibatches and construct Sinkhorn WGF based on this.Therefore, in fact, the \u00b5 and \u03bd in the subsequent text refer to ( Xt i ) n i=1 and ( \u1ef8 t i ) n i=1 in the Algorithm 1.We first introduce another property of the entropy-regularized optimal transport problem.Lemma 2. Define the Sinkhorn mapping:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "A : C(X ) \u00d7 M + 1 (X ) \u2192 C(X ) A(f, \u00b5)(y) = -\u03b5 log X exp((f (x) -c(x, y))/\u03b5)d\u00b5(x).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "(17) The pair (f \u00b5,\u03bd , g \u00b5,\u03bd ) are the W \u03b5 -potentials of the entropyregularized optimal transport problem 2 if they satisfy:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f \u00b5,\u03bd = A(g \u00b5,\u03bd , \u03bd), \u00b5 -a.e. and g \u00b5,\u03bd = A(f \u00b5,\u03bd , \u00b5), \u03bd -a.e.,",
                        "eq_num": "(18)"
                    }
                ],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "or equivalently",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "X h(x, y)d\u03bd(y) = 1, \u00b5 -a.e. , X h(x, y)d\u00b5(x) = 1, \u03bd -a.e. ,",
                        "eq_num": "(19)"
                    }
                ],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "where h(x, y) := exp 1 \u03b3 (f (x) + g(x) -c(x, y)).To be more precise, by plugging in the optimality condition on g \u00b5,\u03bd in 1, the dual problem 2 becomes:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "OT \u03b5 (\u00b5, \u03bd) = max f \u2208C \u27e8f, \u00b5\u27e9 + \u27e8A(f, \u00b5), \u03bd\u27e9",
                        "eq_num": "(20)"
                    }
                ],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "Viewing the discrete measure \u00b5 as a weight vector w \u00b5 on supp(\u00b5), we have:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "OT \u03b5 (\u00b5, \u03bd) = max f \u2208R d F (f ) := f \u22a4 w \u00b5 + E y\u223c\u03bd [A(f , \u00b5)(y)] ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "(21) that is, we get a standard concave stochastic optimization problem, where the randomness of the problem comes from \u03bd [Genevay et al., 2016] .Hence, the problem can be solved using stochastic gradient descent (SGD).In our methods, we can treat the computation of W \u03b5 -potentials as a Blackbox.In practice, we use the efficient implementation of the Sinkhorn algorithm with GPU acceleration from the GeomLoss package [Feydy et al., 2019] .",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 144,
                        "text": "[Genevay et al., 2016]",
                        "ref_id": null
                    },
                    {
                        "start": 420,
                        "end": 440,
                        "text": "[Feydy et al., 2019]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Computation of W \u03b5 -potentials",
                "sec_num": null
            },
            {
                "text": "Definition 3. (First variation of Functionals over Probability).Given a functional F : P(X ) \u2192 R + , we shell perturb measure \u00b5 with a perturbation \u03c7 so that \u00b5 + t\u03c7 belongs to P(X ) for small t ( d\u03c7 = 0).We treat F(\u00b5), as a functional over probability in its second argument and compute its first variation as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "d dt F (\u00b5 + t\u03c7) t=0 = lim t\u21920 F (\u00b5 + t\u03c7) -F (\u00b5) t := \u03b4F \u03b4\u00b5 (\u00b5) d\u03c7.",
                        "eq_num": "(22)"
                    }
                ],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "B.1 Proof of Theorem 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "Proof.According to definition 3, given",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "F \u03b5 (\u2022) = S \u03b5 (\u2022, \u00b5 * )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "and t in a neighborhood of 0, we define",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "\u00b5 t = \u00b5 + t\u03b4\u00b5 lim t\u21920 1 t (F \u03b5 (\u00b5 t ) -F \u03b5 (\u00b5)) = lim t\u21920 1 t (W \u03b5 (\u00b5 t , \u00b5 * ) -W \u03b5 (\u00b5, \u00b5 * )) \u2206 first part t -lim t\u21920 1 2t (W \u03b5 (\u00b5 t , \u00b5 t ) -W \u03b5 (\u00b5, \u00b5)) \u2206 second part t We first analysis \u2206 first part t := lim t\u21920 1 t (W \u03b5 (\u00b5 t , \u00b5 * ) -W \u03b5 (\u00b5, \u00b5 * )).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "First, let us remark that (f, g) is the a suboptimal pair of dual potentials W \u03b5,\u00b5 * (\u00b5) for short.Recall 3,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "W \u03b5 \u2265 \u27e8\u00b5 t , f \u27e9 + \u27e8\u00b5 * , g\u27e9 -\u03b5 \u00b5 t \u2297 \u00b5 * , exp 1 \u03b5 (f \u2295 g -C) -1 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "and thus, since",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "W \u03b5 \u2265 \u27e8\u00b5, f \u27e9 + \u27e8\u00b5 * , g\u27e9 -\u03b5 \u00b5 \u2297 \u00b5 * , exp 1 \u03b5 (f \u2295 g -C) -1 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "one has",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "\u2206 first part t \u2265 \u27e8\u03b4\u00b5, f \u27e9 -\u03b5 \u03b4\u00b5 \u2297 \u00b5 * , exp 1 \u03b5 (f \u2295 g -C) + o(1) \u2265 \u27e8\u03b4\u00b5, f -\u03b5\u27e9 + o(1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "Conversely, let us denote by (f t , g t ) the optimal pair of potentials for W \u03b5 (\u00b5 t , \u00b5 * ) satisfying g t (x o ) = 0 for some arbitrary anchor point x o \u2208 X .As (f t , g t ) are suboptimal potentials for W \u03b5 (\u00b5, \u00b5 * ) we get that",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "W \u03b5 \u2265 \u27e8\u00b5, f t \u27e9 + \u27e8\u00b5 * , g t \u27e9 -\u03b5 \u00b5 t \u2297 \u00b5 * , exp 1 \u03b5 (f \u2295 g -C) -1 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "and thus, since",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "W \u03b5 \u2265 \u27e8\u00b5 t , f t \u27e9 + \u27e8\u00b5 * , g t \u27e9 -\u03b5 \u00b5 t \u2297 \u00b5 * , exp 1 \u03b5 (f t \u2295 g -C) -1 , one has \u2206 first part t \u2265 \u27e8\u03b4\u00b5, ft\u27e9 -\u03b5 \u03b4\u00b5 \u2297 \u00b5 * t , exp 1 \u03b5 (ft \u2295 g -C) + o(1) \u2265 \u27e8\u03b4\u00b5, ft -\u03b5\u27e9 + o(1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "Now, let us remark that as t goes to 0, \u00b5 + t\u03b4\u00b5 \u21c0 \u00b5. f t and g t converge uniformly towards f and g according to Proposition 13 [Feydy et al., 2019] .we get",
                "cite_spans": [
                    {
                        "start": 128,
                        "end": 148,
                        "text": "[Feydy et al., 2019]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "\u2206 first part t = \u27e8\u03b4\u00b5, f \u27e9 Simular to analysis \u2206 first part t := lim t\u21920 1 t (W \u03b5 (\u00b5 t , \u00b5 * ) -W \u03b5 (\u00b5, \u00b5 * ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "we define",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "\u2206 second part t := lim t\u21920 1 2t (W \u03b5 (\u00b5 t , \u00b5 t ) -W \u03b5 (\u00b5, \u00b5))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": ", we have:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "\u2206 second part t = \u27e8\u03b4\u00b5, f \u2032 \u27e9 to be more clearly, we denote f = f \u00b5,\u00b5 * and f \u2032 = f \u00b5,\u00b5 thus, lim t\u21920 1 t (F \u03b5 (\u00b5 t ) -F \u03b5 (\u00b5)) = \u27e8\u03b4\u00b5, f \u00b5,\u00b5 * -f \u00b5,\u00b5 \u27e9.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "So the first variation of F \u03b5 is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "\u03b4F \u03b5 \u03b4\u00b5 = f \u00b5,\u00b5 * -f \u00b5,\u00b5 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Theory of Sinkhorn Wasserstein gradient flow",
                "sec_num": null
            },
            {
                "text": "Following the lines of our proof in Theorem 1, we give the following proof.Lemma 3. (Fr\u00e9chet derivative of entropy-regularized Wasserstein distance) Let \u03b5 > 0. We shall fix in the following a measure \u00b5 * and let (f \u00b5,\u00b5 * , g \u00b5,\u00b5 * ) be the W \u03b5 potentials of W \u03b5 (\u00b5, \u00b5 * ) according to lemma 1.Consider the infinitesimal transport T (x) = x + \u03bb\u03d5.We have the Fr\u00e9chet derivative under this particular perturbation:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "d d\u03bb W \u03b5 (T # \u00b5, \u00b5 * )| \u03bb=0 = lim \u03bb\u21920 W \u03b5 (T # \u00b5, \u00b5 * ) -W \u03b5 (\u00b5, \u00b5 * ) \u03bb = X \u2207f \u00b5,\u00b5 * (x)\u03d5(x)d\u00b5(x).",
                        "eq_num": "(23)"
                    }
                ],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "Proof.let f = f \u00b5,\u00b5 * and g = g \u00b5,\u00b5 * be the W \u03b5 -potentials 1 to W \u03b5 (\u00b5, \u00b5 * ) for short.By 3 and the optimality of (f, g), we have follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "W \u03b5 (\u00b5, \u00b5 * ) = \u27e8f, \u00b5\u27e9 + \u27e8g, \u00b5 * \u27e9.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "However, (f, g) are not necessarily the optimal dual variables for W \u03b5 (T # \u00b5, \u00b5 * ), recall the lemma 2:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "W \u03b5 (T # \u00b5, \u00b5 * ) \u2265 \u27e8f, T # \u00b5\u27e9 + \u27e8g, \u00b5 * \u27e9 -\u03b5\u27e8h -1, T # \u00b5 \u2297 \u00b5 * \u27e9,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "where X h(x, y)d\u00b5 * (y) = 1 and hence \u27e8h-1, T # \u00b5\u2297\u00b5 * \u27e9 = 0. Thus:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "W \u03b5 (T # \u00b5, \u00b5 * ) -W \u03b5 (\u00b5, \u00b5 * ) \u2265 \u27e8f, T # \u00b5 -\u00b5\u27e9.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "Use the change-of-variables formula of the push-forward measure to obtain:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "1 \u03bb \u27e8f, T # \u00b5 -\u00b5\u27e9 = 1 \u03bb X ((f \u2022 T )(x) -f (x))d\u00b5(x) = X \u2207f (x + \u03bb \u2032 \u03d5(x))\u03d5(x)d\u00b5(x),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "where \u03bb \u2032 \u2208 [0, \u03bb] is from the mean value theorem.Here we assume \u2207f is Lipschitz continuous follow Proposition 12 in [Feydy et al., 2019] and Lemma A.4 form [Shen et al., 2020] .",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 137,
                        "text": "[Feydy et al., 2019]",
                        "ref_id": null
                    },
                    {
                        "start": 157,
                        "end": 176,
                        "text": "[Shen et al., 2020]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "We have:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "lim \u03bb\u21920 1 \u03bb \u27e8f, T # \u00b5 -\u00b5\u27e9 = X \u2207f (x)\u03d5(x)d\u00b5(x).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "Hence:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "lim \u03bb\u21920 1 \u03bb (W \u03b5 (T # \u00b5, \u00b5 * ) -W \u03b5 (\u00b5, \u00b5 * )) \u2265 X \u2207f (x)\u03d5(x)d\u00b5(x).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "Similarly, let f \u2032 and g \u2032 be the W \u03b5 potentials to W \u03b5 (T # \u00b5, \u00b5 * ), we have:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "W \u03b5 (\u00b5, \u00b5 * ) \u2265 \u27e8f \u2032 , \u00b5\u27e9 + \u27e8g, \u00b5 * \u27e9 -\u03b5\u27e8h -1, \u00b5 \u2297 \u00b5 * \u27e9,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "where X h(x, y)d\u00b5 * (y) = 1 and hence \u27e8h-1, \u00b5\u2297\u00b5 * \u27e9 = 0. Thus:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "W \u03b5 (T # \u00b5, \u00b5 * ) -W \u03b5 (\u00b5, \u00b5 * ) \u2264 \u27e8f \u2032 , T # \u00b5 -\u00b5\u27e9.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "Same as above, use the change-of-variables formula and the mean value theorem:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "1 \u03bb \u27e8f \u2032 , T # \u00b5 -\u00b5 * \u27e9 = X \u2207f \u2032 (x + \u03bb \u2032 \u03d5(x))\u03d5(x)d\u00b5(x),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "Thus:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "lim \u03bb\u21920 1 \u03bb (W \u03b5 (T # \u00b5, \u00b5 * ) -W \u03b5 (\u00b5, \u00b5 * )) \u2264 X lim \u03bb\u21920 \u2207f \u2032 (x + \u03bb \u2032 \u03d5(x))\u03d5(x)d\u00b5(x).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "Assume that \u2207f \u2032 is Lipschitz continuous and f \u2032 \u2192 f as \u03bb \u2192 0. Consequently we have lim \u03bb\u21920 \u2207f \u2032 (x + \u03bb \u2032 \u03d5(x)) and hence:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "lim \u03bb\u21920 1 \u03bb (W \u03b5 (T # \u00b5, \u00b5 * ) -W \u03b5 (\u00b5, \u00b5 * )) = X \u2207f (x)\u03d5(x)d\u00b5(x).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "According to lemma 3, we have:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "d d\u03bb F\u03b5(T # \u00b5) \u03bb=0 = X \u2207f\u00b5,\u00b5 * (x)\u03d5(x)d\u00b5(x) - 1 2 \u2022 X 2\u2207f\u00b5,\u00b5(x)\u03d5(x)d\u00b5(x) = X \u2207f\u00b5,\u00b5 * (x)\u03d5(x)d\u00b5 - X \u2207f\u00b5,\u00b5(x)\u03d5(x)d\u00b5.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Proof of Proposition 2",
                "sec_num": null
            },
            {
                "text": "Proof.First, we define \u03a8(\u00b5) = hd\u00b5 where h : R d \u2192 R is an arbitrary bounded and continuous function and \u03b4\u03a8(\u00b5) \u03b4\u00b5 (x) denotes the first variation of functional \u03a8 at \u00b5 satisfying:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "\u03b4\u03a8(\u00b5) \u03b4\u00b5 (x)\u03be(x)dx = lim \u03f5\u21920 \u03a8(\u00b5 + \u03f5\u03be) -\u03a8(\u00b5) \u03f5",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "for all signed measure \u03be(x)dx = 0. We also have the following:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "\u03b4\u03a8(\u00b5) \u03b4\u00b5 (\u2022) = \u03b4 hd\u00b5 \u03b4\u00b5 (\u2022) = h(\u2022)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "Assume \u00b5 t is a flow satisfies the following:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "\u2202 t \u03a8[\u00b5 t ] = (L\u03a8)[\u00b5 t ],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "where,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L\u03a8[\u00b5 t ] = -\u27e8\u2207 \u03b4F \u03b5 (\u00b5 t ) \u03b4\u00b5 (x), \u2207 x \u03b4\u03a8(\u00b5 t ) \u03b4\u00b5 (x)\u27e9\u00b5 t (x)dx",
                        "eq_num": "(24)"
                    }
                ],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "Notably, \u00b5 t is a solution of equation 2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "Next, let \u03bcM t be the distribution produced by the equation 14 at time t.Under mild assumption of \u03bcM 0 \u21c0 \u00b5 0 , we want to show that the mean-field limit of \u03bcM t as M \u2192 \u221e is \u00b5 t by showing that lim M \u2192\u221e \u03a8(\u00b5 M t ) = \u03a8(\u00b5 t ) [Folland, 1999] .For the measure valued flow \u03bcM t equation 14, the infinitesimal generator of \u03a8 w.r.t.\u03bcM t is defined as follows:",
                "cite_spans": [
                    {
                        "start": 222,
                        "end": 237,
                        "text": "[Folland, 1999]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "(L\u03a8)[\u03bc M t ] := lim \u03f5\u21920 + \u03a8(\u03bc M t+\u03f5 ) -\u03a8(\u03bc M t ) \u03f5 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "According to the definition of first variation, it can be calculated that",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "(L\u03a8)[\u03bc M t ] = lim \u03f5\u21920 + \u03a8[ M i=1 1 M \u03b4 x i t+\u03f5 ] -\u03a8( M i=1 1 M \u03b4 x i t ) \u03f5 = \u03b4\u03a8(\u03bc M t ) \u03b4\u00b5 (x) M i=1 1 M \u2202t\u03c1(x i t )dx",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "Then we adopt the Induction over the Continuum to prove lim n\u2192\u221e \u03a8(\u03bc M t ) = \u03a8(\u00b5 t ) for all t > 0.Here t \u2208 R + satisfy the requirement of well ordering and the existence of a greatest lower bound for non-empty subsets, so Induction over the Continuum is reasonable [Kalantari, 2007] .",
                "cite_spans": [
                    {
                        "start": 265,
                        "end": 282,
                        "text": "[Kalantari, 2007]",
                        "ref_id": "BIBREF64"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "1.As for t = 0, our assumption of \u03bcM 0 \u21c0 \u00b5 0 suffice.2. For the case of t = t * , we first hypothesis that for t < t * , \u03bcM t \u21c0 \u00b5 t as M \u2192 \u221e.Then for t < t * we have: 1) and (2), we can reach to the conclusion that lim M \u2192\u221e \u03a8(\u00b5 M t ) = \u03a8(\u00b5 t ) for all t. which indicates that \u03bcM t \u21c0 \u00b5 t if \u03bcM 0 \u21c0 \u00b5 0 .Since \u00b5 t solves the partial differential equation 10, we conclude that the path of equation 14 starting from \u03bcM 0 weakly converges to a solution of the partial differential equation equation 10 starting from \u00b5 0 as M \u2192 \u221e.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "lim M \u2192\u221e (L\u03a8)[\u03bc M t ] = lim M \u2192\u221e \u03b4\u03a8(\u03bc M t ) \u03b4\u00b5 (x) M i=1 1 M \u2202t\u03c1(x i t )dx = -lim M \u2192\u221e \u27e8\u2207 \u03b4F\u03b5(\u00b5t) \u03b4\u00b5 (x), \u2207x \u03b4\u03a8(\u03bc M t ) \u03b4\u00b5 (x)\u27e9\u03bc M t (x)dx = -\u27e8\u2207 \u03b4F\u03b5(\u00b5t) \u03b4\u00b5 (x), \u2207x \u03b4\u03a8(\u00b5t) \u03b4\u00b5 (x)\u27e9\u00b5t(x)dx. Because lim M \u2192\u221e \u03a8(\u03bc M 0 ) = \u03a8(\u00b5 0 ) at t = 0 and lim M \u2192\u221e (\u2202 t \u03a8)[\u03bc M t ] = (\u2202 t \u03a8)[\u00b5 t ] for all t < t * , we have lim M \u2192\u221e \u03a8(\u03bc M t * ) = \u03a8(\u00b5 t * ). Combining (",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Proof of theorem 2",
                "sec_num": null
            },
            {
                "text": "Proposition 3. Consider the Sinkhorn gradient flow 10, the differentiation of F \u03b5 (\u00b5 t ) with respect to the time t satisfies:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Descending property",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "dF \u03b5 (\u00b5 t ) dt = - \u2207 \u03b4F \u03b5 (\u00b5 t ) \u03b4\u00b5 t 2 d\u00b5 t \u2264 0",
                        "eq_num": "(25)"
                    }
                ],
                "section": "B.4 Descending property",
                "sec_num": null
            },
            {
                "text": "Proof.By substituting \u03a8(\u2022) = F \u03b5 (\u2022) in equation 24, we directly reach to the above equality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Descending property",
                "sec_num": null
            },
            {
                "text": "For large datasets, the computation and storage of the optimal transport plan can be challenging due to OT's cubic time and quadratic memory complexities relative to the number of samples [Cuturi, 2013; Genevay et al., 2016; Peyr\u00e9 et al., 2017] .The minibatch approximation offers a viable solution for enhancing calculation efficiency.Theoretical analysis of using the minibatch approximation for transportation plans is provided by [Fatras et al., 2019; Fatras et al., 2021b] .Although minibatch OT introduces some errors compared to the exact OT solution, its efficiency in computing approximate OT is clear, and it has seen successful applications in domains like domain adaptation [Damodaran et al., 2018; Fatras et al., 2021a] and generative modeling [Genevay et al., 2018] .More recently, [Pooladian et al., 2023; Tong et al., 2023 ] introduced OT-CFM and empirically demonstrated that using minibatch approximation of optimal transport in flow matching methods [Liu et al., 2023; Lipman et al., 2023] can straighten the flow's trajectory and yield more consistent samples.OT-CFM specifically focuses on minibatch initial and target samples, continuing to use random linear interpolation paths.In contrast, NSGF leverages minibatch W \u03b5 -potentials to construct Sinkhorn gradient flows in minibatches.Our method also involves performing velocity field matching on the flow's discretized form, marking a separate and innovative direction in the field.",
                "cite_spans": [
                    {
                        "start": 188,
                        "end": 202,
                        "text": "[Cuturi, 2013;",
                        "ref_id": null
                    },
                    {
                        "start": 203,
                        "end": 224,
                        "text": "Genevay et al., 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 225,
                        "end": 244,
                        "text": "Peyr\u00e9 et al., 2017]",
                        "ref_id": null
                    },
                    {
                        "start": 434,
                        "end": 455,
                        "text": "[Fatras et al., 2019;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 456,
                        "end": 477,
                        "text": "Fatras et al., 2021b]",
                        "ref_id": null
                    },
                    {
                        "start": 686,
                        "end": 710,
                        "text": "[Damodaran et al., 2018;",
                        "ref_id": null
                    },
                    {
                        "start": 711,
                        "end": 732,
                        "text": "Fatras et al., 2021a]",
                        "ref_id": null
                    },
                    {
                        "start": 757,
                        "end": 779,
                        "text": "[Genevay et al., 2018]",
                        "ref_id": null
                    },
                    {
                        "start": 796,
                        "end": 820,
                        "text": "[Pooladian et al., 2023;",
                        "ref_id": "BIBREF95"
                    },
                    {
                        "start": 821,
                        "end": 838,
                        "text": "Tong et al., 2023",
                        "ref_id": null
                    },
                    {
                        "start": 969,
                        "end": 987,
                        "text": "[Liu et al., 2023;",
                        "ref_id": null
                    },
                    {
                        "start": 988,
                        "end": 1008,
                        "text": "Lipman et al., 2023]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Minibatch Optimal Transport",
                "sec_num": null
            },
            {
                "text": "We introduce a two-phase NSGF++ algorithm that initially employs the Sinkhorn gradient flow for rapid approximation to the image manifold, followed by sample refinement using a straightforward straight flow.The NSGF++ model comprises three key components:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D NSGF++",
                "sec_num": null
            },
            {
                "text": "\u2022 An NSGF model trained for T \u2264 5 time steps.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D NSGF++",
                "sec_num": null
            },
            {
                "text": "\u2022 A phase-transition time predictor, denoted as t \u03d5 : X \u2192 [0, 1], which facilitates the transition from NSGF to NSF.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D NSGF++",
                "sec_num": null
            },
            {
                "text": "\u2022 A Neural Straight Flow (NSF) model, trained through velocity field matching on a linear interpolation straight flow X t \u223c (1 -t)P 0 + tP 1 , t \u2208 [0, 1]. the detailed algorithm is outlined in 3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D NSGF++",
                "sec_num": null
            },
            {
                "text": "In the inference process of NSGF++, we initially apply the NSGF with fewer than 5 NFE, starting from X 0 \u223c P 0 , to obtain an intermediate output XT .This output is then processed using the time predictor t \u03d5 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D NSGF++",
                "sec_num": null
            },
            {
                "text": "i \u223c \u00b5 0 , \u1ef8i \u223c \u00b5 * , i = 1, 2, \u2022 \u2022 \u2022 n. t \u223c U(0, 1). X t = t \u1ef8i + (1 -t) X0 i L(\u03d5) = E t\u2208U (0,1),Xt\u223cPt ||t -t \u03d5 (X t )|| 2 . \u03d5 \u2190 \u03d5 -\u03b3 \u2032 \u2207 \u03d5 L (\u03d5) . / * NSF model * / while Training do X0 i \u223c \u00b5 0 , \u1ef8i \u223c \u00b5 * , i = 1, 2, \u2022 \u2022 \u2022 n. t \u223c U(0, 1). X t = t \u1ef8i + (1 -t) X0 i . L NSF (\u03b4) \u2190 u \u03b4 (t, X t ) -\u1ef8i -X0 i 2 . \u03b4 \u2190 \u03b4 -\u03b3 \u2032\u2032 \u2207 \u03b4 L NSF (\u03b4).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D NSGF++",
                "sec_num": null
            },
            {
                "text": "Output: v \u03b8 parameterize the time-varying velocity field of NSGF, t \u03d5 parameterize the phase trainsition time predictor, u \u03b4 parameterize the NSF model from the state t \u03d5 ( Xt ).The detailed algorithm is outlined in 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D NSGF++",
                "sec_num": null
            },
            {
                "text": "For the 2D experiments, we closely follow [Tong et al., 2023] and the released code at https: //github.com/atong01/conditional-flow-matching(code released under MIT license), and use the same synthetic datasets and the 2-Wasserstein distance between the test set and samples simulated using NSGF as the evaluation metric.We use 1024 samples in the test set since we find the We use a simple MLP with 3 hidden layers and 256 hidden units to parameterize the velocity matching networks.We use batch size 256 and 10/100 steps with a uniform schedule at sampling time.For both Nerual gradient-flow-based models and Nerual ODE-based Models, we train for 20000 steps in total.Note that FM cannot be used for the 8gaussians-moons task since it requires a Gaussian source, but we still conducted experiments with the algorithm and found competitive experimental results.We believe that this is because FM is essentially very close to 1-RF in its algorithmic design, and that the Gaussian source condition can be meaningfully relaxed in practice, as confirmed in [Tong et al., 2023] .The experiments are run using one 3090 GPU and take approximately less than 60 minutes (for both training and testing).",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 61,
                        "text": "[Tong et al., 2023]",
                        "ref_id": null
                    },
                    {
                        "start": 1054,
                        "end": 1073,
                        "text": "[Tong et al., 2023]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Experimrnts E.1 2D simulated data",
                "sec_num": null
            },
            {
                "text": "For the neural gradient-flow-based models, we solely implemented the EPT without the outer loop, as the outer loop can be likened to a GAN-like distillation approach [Goodfellow et al., 2014] .Notably, the original EPT [Gao et al., 2022] recommends iterating for 20, 000 rounds with an exceedingly",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 191,
                        "text": "[Goodfellow et al., 2014]",
                        "ref_id": null
                    },
                    {
                        "start": 219,
                        "end": 237,
                        "text": "[Gao et al., 2022]",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Experimrnts E.1 2D simulated data",
                "sec_num": null
            },
            {
                "text": "i = Xt i + \u03b7v \u03b8 Xt i , t , i = 1, 2, \u2022 \u2022 \u2022 n. / * phase trainsition time predict * / t = t \u03d5 ( XT i ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Experimrnts E.1 2D simulated data",
                "sec_num": null
            },
            {
                "text": "/ * NSF refine phase * / T = (1 -t)/\u03c9.X = ODEsolver( XT i , u \u03b4 , t, 1, T ).Output: X as the results.small step size; however, to ensure a fair comparison, we employed the same number of steps as the other methods while adapting the step size accordingly.It's worth mentioning that for the JKO-Flow, we used the recommended parameter setting of 10 steps, as suggested in [Fan et al., 2022] , but we also provide results for 100 steps for comparative purposes.All the results for Neural Gradient flow-based models were trained and sampled following the standard procedures outlined in their respective papers.",
                "cite_spans": [
                    {
                        "start": 371,
                        "end": 389,
                        "text": "[Fan et al., 2022]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Experimrnts E.1 2D simulated data",
                "sec_num": null
            },
            {
                "text": "For the MNIST/CIFAR-10 experiments, we summarize the setup of our NSGF++ model here, where the exact parameter choices can be seen in the source code.For the calculation of W \u03b5 -potentials, we use the GeomLoss package [Feydy et al., 2019] with blur = 0.5, 1.0 or 2.0, scaling = 0.80, 0.85 or 0.95 depends on learning rate of Sinkhorn gradient flow.We find using an incremental lr scheme will improve training performance.More detailed experiments we will leave for future work.We used the Adam optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.999 and no weight decay.Here we list different part of our NSGF++ model separately.First, we use the UNet architecture from [Dhariwal and Nichol, 2021] .For MNIST, we use channels = 32, depth = 1, channels multiple = [1, 2, 2], heads = 1, attention resolution = 16, dropout = 0.0.For CIFAR-10, we use channels = 128, depth = 2, channels multiple = [1, 2, 2, 2], heads = 4, heads channels = 64, attention resolution = 16, dropout = 0.0.We use the same UNet architecture in our neural straight flow model.",
                "cite_spans": [
                    {
                        "start": 218,
                        "end": 238,
                        "text": "[Feydy et al., 2019]",
                        "ref_id": null
                    },
                    {
                        "start": 652,
                        "end": 679,
                        "text": "[Dhariwal and Nichol, 2021]",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.2 Image benchmark data",
                "sec_num": null
            },
            {
                "text": "For the phase transition time predictor, we employed an efficiently designed convolutional neural network (CNN) capable of achieving satisfactory results while optimizing training time.The CNN used in our study consists of a structured architecture featuring four convolutional layers with filter depths of 32, 64, 128, and 256.Each layer uses a 3x3 kernel, a stride of 1, and padding of 1, coupled with ReLU activation and 2x2 average pooling for effective feature downsampling.The network culminates in a fully connected layer that transforms the flattened features into a single value, further For the MNIST/CIFAR-10 experiments, a considerable amount of storage space is required when establishing the trajectory pool during the first phase of the algorithm.For MNIST dataset, setting the batch size to 256 and saving 1500 batches 5-step minibatch Sinkhorn gradient flow trajectories requires less than 20GB of storage space and with CIFAR-10, setting the batch size to 128 and saving 2500 batches 5-step minibatch Sinkhorn gradient flow trajectories requires about 45GB of storage space.In situations where storage space is limited, we suggest dynamically adding and removing trajectories in the trajectory pool to meet the training requirements.Identifying a more effective trade-off between training time and storage space utilization is a direction for future improvement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.2 Image benchmark data",
                "sec_num": null
            },
            {
                "text": "In our supplementary materials, we present additional results from 2D simulated data to demonstrate the efficiency of the NSGF++ model in Figure 6 and Figure 7 .These results indicate that NSGF++ achieves competitive performance with a more direct path and fewer steps compared to other neural Wasserstein gradient flow and flow-matching methods, highlighting its effectiveness and computational efficiency.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 145,
                        "end": 146,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    },
                    {
                        "start": 158,
                        "end": 159,
                        "text": "7",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "2D simulated data",
                "sec_num": null
            },
            {
                "text": "In our study, we include results from the MNIST dataset to showcase the efficiency of the NSGF++ model.As detailed in Table 3 , NSGF++ achieves competitive Fr\u00e9chet Inception Distances (FIDs) while utilizing only 60% of the Number of Function Evaluations (NFEs) typically required.This underscores the model's effectiveness in balancing performance with computational efficiency.To evaluate our results, we use the Fr\u00e9chet inception distance (FID) between 10K generated samples and the test dataset.Here, a smaller FID value indicates a higher similarity between generated and test samples.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 124,
                        "end": 125,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "MNIST",
                "sec_num": null
            },
            {
                "text": "In our work, we present further results of the NSGF++ model on the CIFAR-10 dataset, illustrated in Figures 9 and 11 .These experimental findings demonstrate that NSGF++ attains competitive performance in generation tasks, highlighting its efficacy.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 108,
                        "end": 109,
                        "text": "9",
                        "ref_id": null
                    },
                    {
                        "start": 114,
                        "end": 116,
                        "text": "11",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "CIFAR-10",
                "sec_num": null
            },
            {
                "text": "MNIST FID(\u2193) NFE(\u2193) NSGF++(ours)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Algorithm",
                "sec_num": null
            },
            {
                "text": "3.8 60 SWGF [ 2019 ] 225.1 500 SIG [ 2020 ] 4.5 / FM [ 2023 ] 3.4 100 OT-CFM [ 2023 ] 3.3 100 RF [ 2023 ] 3.1 100 Training set 2.27 /",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 20,
                        "text": "[ 2019 ]",
                        "ref_id": null
                    },
                    {
                        "start": 35,
                        "end": 43,
                        "text": "[ 2020 ]",
                        "ref_id": null
                    },
                    {
                        "start": 53,
                        "end": 61,
                        "text": "[ 2023 ]",
                        "ref_id": null
                    },
                    {
                        "start": 77,
                        "end": 85,
                        "text": "[ 2023 ]",
                        "ref_id": null
                    },
                    {
                        "start": 97,
                        "end": 105,
                        "text": "[ 2023 ]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Algorithm",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Screening sinkhorn algorithm for regularized optimal transport",
                "authors": [
                    {
                        "first": "Maxime",
                        "middle": [],
                        "last": "Mokhtar Z Alaya",
                        "suffix": ""
                    },
                    {
                        "first": "Gilles",
                        "middle": [],
                        "last": "Berar",
                        "suffix": ""
                    },
                    {
                        "first": "Alain",
                        "middle": [],
                        "last": "Gasso",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rakotomamonjy",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NeurIPS",
                "volume": "32",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mokhtar Z Alaya, Maxime Berar, Gilles Gasso, and Alain Rakotomamonjy. Screening sinkhorn algorithm for regularized optimal transport. NeurIPS, 32, 2019.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Building normalizing flows with stochastic interpolants",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Samuel",
                        "suffix": ""
                    },
                    {
                        "first": "Albergo",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Vanden-Eijnden",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "The Eleventh ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh ICLR, 2023.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Optimizing functionals on the space of probabilities with input convex neural networks",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Alvarez-Melis",
                        "suffix": ""
                    },
                    {
                        "first": "Yair",
                        "middle": [],
                        "last": "Schiff",
                        "suffix": ""
                    },
                    {
                        "first": "Youssef",
                        "middle": [],
                        "last": "Mroueh",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Transactions on Machine Learning Research",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Alvarez-Melis, Yair Schiff, and Youssef Mroueh. Optimizing functionals on the space of proba- bilities with input convex neural networks. Transactions on Ma- chine Learning Research, 2022.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Gradient Flows of Probability Measures",
                "authors": [
                    {
                        "first": "Luigi",
                        "middle": [],
                        "last": "Ambrosio",
                        "suffix": ""
                    },
                    {
                        "first": "Giuseppe",
                        "middle": [],
                        "last": "Savar\u00e9",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Handbook of Differential Equations: Evolutionary Equations",
                "volume": "",
                "issue": "",
                "pages": "1--136",
                "other_ids": {
                    "DOI": [
                        "10.1016/s1874-5717(07)80004-1"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00e9. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Input convex neural networks",
                "authors": [
                    {
                        "first": "Brandon",
                        "middle": [],
                        "last": "Amos",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "J Zico",
                        "middle": [],
                        "last": "Kolter",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "146--155",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Brandon Amos, Lei Xu, and J Zico Kolter. In- put convex neural networks. In ICML, pages 146-155. PMLR, 2017.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Figure 2: Forest plot demonstrating the incidence of dental trauma in Arab children\u2019s permanent teeth (Hamdan & Rock, 1995; Marcenes et al., 1999; Al-Majed, Murray & Maguire, 2001; Al-Jundi, 2002; Artun et al., 2005; Sgan-Cohen, Yassin & Livny, 2008; Al-Malik, 2009; Noori & Al-Obaidi, 2009; Navabazam & Farahani, 2010; Ajlouni, Jaradat & Rihani, 2010; ElKarmi et al., 2015; El-Kenany, Awad & Hegazy, 2016; Muhamad, Nezar & Azzaldeen, 2016; Rajab & Abu Al Huda, 2019; Al-Ansari & Nazir, 2020; Arheiam et al., 2020; Shehri et al., 2021; Basha et al., 2021; Alshammary et al., 2022; Hashim et al., 2022).",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ansari",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.7717/peerj.18366/fig-2"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ansari et al., 2021]",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A Characteristic Function Approach to Deep Implicit Generative Modeling",
                "authors": [
                    {
                        "first": "Abdul",
                        "middle": [],
                        "last": "Fatir Ansari",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Scarlett",
                        "suffix": ""
                    },
                    {
                        "first": "Harold",
                        "middle": [],
                        "last": "Soh",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": "",
                "issue": "",
                "pages": "7476--7484",
                "other_ids": {
                    "DOI": [
                        "10.1109/cvpr42600.2020.00750"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Abdul Fatir Ansari, Ming Liang Ang, and Harold Soh. Refining deep generative models via discriminator gradient flow. In ICLR, 2021.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Maximum mean discrepancy gradient flow",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Arbel",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Korba",
                        "suffix": ""
                    },
                    {
                        "first": "Adil",
                        "middle": [],
                        "last": "Salim",
                        "suffix": ""
                    },
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Gretton",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NeurIPS",
                "volume": "32",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient flow. NeurIPS, 32, 2019.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Proximal optimal transport modeling of population dynamics",
                "authors": [
                    {
                        "first": "Charlotte",
                        "middle": [],
                        "last": "Bunne",
                        "suffix": ""
                    },
                    {
                        "first": "Laetitia",
                        "middle": [],
                        "last": "Papaxanthos",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Krause",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Cuturi",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "International Conference on Artificial Intelligence and Statistics",
                "volume": "",
                "issue": "",
                "pages": "6511--6528",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Charlotte Bunne, Laetitia Papaxanthos, An- dreas Krause, and Marco Cuturi. Proximal optimal transport modeling of population dynamics. In International Confer- ence on Artificial Intelligence and Statistics, pages 6511-6528. PMLR, 2022.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Implicit runge-kutta processes",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "John",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Butcher",
                        "suffix": ""
                    }
                ],
                "year": 1964,
                "venue": "Mathematics of computation",
                "volume": "18",
                "issue": "85",
                "pages": "50--64",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John C Butcher. Implicit runge-kutta processes. Mathematics of computation, 18(85):50-64, 1964.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Density ratio estimation via infinitesimal classification",
                "authors": [
                    {
                        "first": "Kristy",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Chenlin",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Ermon",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "International Conference on Artificial Intelligence and Statistics",
                "volume": "",
                "issue": "",
                "pages": "2552--2573",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kristy Choi, Chenlin Meng, Yang Song, and Stefano Ermon. Density ratio estimation via infinitesimal clas- sification. In International Conference on Artificial Intelligence and Statistics, pages 2552-2573. PMLR, 2022.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
                "authors": [
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Cuturi",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "NeurIPS",
                "volume": "26",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marco Cuturi. Sinkhorn distances: Lightspeed com- putation of optimal transport. NeurIPS, 26, 2013.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Sliced iterative normalizing flows",
                "authors": [
                    {
                        "first": "Seljak",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2007.00674"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dai and Seljak, 2020] Biwei Dai and Uros Seljak. Sliced iterative normalizing flows. arXiv preprint arXiv:2007.00674, 2020.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Bharath Bhushan Damodaran",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e9mi",
                        "middle": [],
                        "last": "Kellenberger",
                        "suffix": ""
                    },
                    {
                        "first": "Devis",
                        "middle": [],
                        "last": "Flamary",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolas",
                        "middle": [],
                        "last": "Tuia",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Courty",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the European conference on computer vision (ECCV)",
                "volume": "",
                "issue": "",
                "pages": "447--463",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bharath Bhushan Damodaran, Benjamin Kellenberger, R\u00e9mi Flamary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsuper- vised domain adaptation. In Proceedings of the European con- ference on computer vision (ECCV), pages 447-463, 2018.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Figure 11: Visual representation of comparison with existing studies (Genovese et al., 2021; Das & Meher, 2021; Khan Tusar et al., 2024; Bhuvaneswari et al., 2023; Najjar et al., 2023; Keerthivasan & Saranya, 2023; Dangore et al., 2024).",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.7717/peerj-cs.2997/fig-11"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Das et al., 2023]",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Image generation with shortest path diffusion",
                "authors": [
                    {
                        "first": "Ayan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Stathi",
                        "middle": [],
                        "last": "Fotiadis",
                        "suffix": ""
                    },
                    {
                        "first": "Anil",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Farhang",
                        "middle": [],
                        "last": "Nabiei",
                        "suffix": ""
                    },
                    {
                        "first": "Fengting",
                        "middle": [],
                        "last": "Liao",
                        "suffix": ""
                    },
                    {
                        "first": "Sattar",
                        "middle": [],
                        "last": "Vakili",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Da-Shan",
                        "suffix": ""
                    },
                    {
                        "first": "Alberto",
                        "middle": [],
                        "last": "Shiu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bernacchia",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2306.00501"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ayan Das, Stathi Fotiadis, Anil Batra, Farhang Nabiei, FengTing Liao, Sattar Vakili, Da-shan Shiu, and Alberto Bernacchia. Image generation with shortest path diffusion. arXiv preprint arXiv:2306.00501, 2023.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Endto-end sinkhorn autoencoder with noise generator",
                "authors": [
                    {
                        "first": "Kamil",
                        "middle": [],
                        "last": "Deja",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Dubi\u0144ski",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Nowak",
                        "suffix": ""
                    },
                    {
                        "first": "Sandro",
                        "middle": [],
                        "last": "Wenzel",
                        "suffix": ""
                    },
                    {
                        "first": "Przemys\u0142aw",
                        "middle": [],
                        "last": "Spurek",
                        "suffix": ""
                    },
                    {
                        "first": "Tomasz",
                        "middle": [],
                        "last": "Trzcinski",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "IEEE Access",
                "volume": "9",
                "issue": "",
                "pages": "7211--7219",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kamil Deja, Jan Dubi\u0144ski, Piotr Nowak, San- dro Wenzel, Przemys\u0142aw Spurek, and Tomasz Trzcinski. End- to-end sinkhorn autoencoder with noise generator. IEEE Access, 9:7211-7219, 2020.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Diffusion models beat gans on image synthesis",
                "authors": [
                    {
                        "first": "Nichol",
                        "middle": [
                            ";"
                        ],
                        "last": "Dhariwal",
                        "suffix": ""
                    },
                    {
                        "first": "Prafulla",
                        "middle": [],
                        "last": "Dhariwal",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Nichol",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Advances in neural information processing systems",
                "volume": "34",
                "issue": "",
                "pages": "8780--8794",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dhariwal and Nichol, 2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Ad- vances in neural information processing systems, 34:8780-8794, 2021.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Figure 4: Forest plot showing impact of paternal HBV infection on clinical pregnancy rate (per cycle) (Chen et al., 2013; Cito et al., 2021; Du et al., 2014; He et al., 2018; Lam et al., 2010; Lee et al., 2010; Ma et al., 2023; Oger et al., 2011; Sun et al., 2024; Zhou et al., 2011).",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.7717/peerj.19824/fig-4"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Du et al., 2023]",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Nonparametric generative modeling with conditional sliced-wasserstein flows",
                "authors": [
                    {
                        "first": "Chao",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Tianbo",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Yan Shuicheng",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chao Du, Tianbo Li, Tianyu Pang, YAN Shuicheng, and Min Lin. Nonparametric generative modeling with conditional sliced-wasserstein flows. 2023.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Variational wasserstein gradient flow",
                "authors": [
                    {
                        "first": "Jiaojiao",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Qinsheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Amirhossein",
                        "middle": [],
                        "last": "Taghvaei",
                        "suffix": ""
                    },
                    {
                        "first": "Yongxin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "6185--6215",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiaojiao Fan, Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin Chen. Variational wasserstein gradient flow. In ICML, pages 6185-6215. PMLR, 2022.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "L'Institut dans la presse locale et r\u00e9gionale",
                "authors": [
                    {
                        "first": "Andr\u00e9",
                        "middle": [],
                        "last": "Fatras",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Cahiers de sociologie \u00e9conomique et culturelle",
                "volume": "1",
                "issue": "1",
                "pages": "109--114",
                "other_ids": {
                    "DOI": [
                        "10.3406/casec.1999.2294"
                    ],
                    "ISSN": [
                        "0761-9871"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fatras et al., 2019]",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Learning with minibatch wasserstein: asymptotic and gradient properties",
                "authors": [
                    {
                        "first": "Kilian",
                        "middle": [],
                        "last": "Fatras",
                        "suffix": ""
                    },
                    {
                        "first": "Younes",
                        "middle": [],
                        "last": "Zine",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e9mi",
                        "middle": [],
                        "last": "Flamary",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e9mi",
                        "middle": [],
                        "last": "Gribonval",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolas",
                        "middle": [],
                        "last": "Courty",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1910.04091"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kilian Fatras, Younes Zine, R\u00e9mi Flamary, R\u00e9mi Gribonval, and Nicolas Courty. Learning with minibatch wasserstein: asymptotic and gradient properties. arXiv preprint arXiv:1910.04091, 2019.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Unbalanced minibatch optimal transport; applications to domain adaptation",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Fatras",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "3186--3197",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fatras et al., 2021a] Kilian Fatras, Thibault S\u00e9journ\u00e9, R\u00e9mi Fla- mary, and Nicolas Courty. Unbalanced minibatch optimal trans- port; applications to domain adaptation. In ICML, pages 3186- 3197. PMLR, 2021.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Minibatch optimal transport distances; analysis and applications",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Fatras",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2101.01792"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fatras et al., 2021b] Kilian Fatras, Younes Zine, Szymon Majew- ski, R\u00e9mi Flamary, R\u00e9mi Gribonval, and Nicolas Courty. Mini- batch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Interpolating between optimal transport and mmd using sinkhorn divergences",
                "authors": [
                    {
                        "first": "Jean",
                        "middle": [],
                        "last": "Feydy",
                        "suffix": ""
                    },
                    {
                        "first": "Thibault",
                        "middle": [],
                        "last": "S\u00e9journ\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Franc \u00b8ois-Xavier",
                        "suffix": ""
                    },
                    {
                        "first": "Shun-Ichi",
                        "middle": [],
                        "last": "Vialard",
                        "suffix": ""
                    },
                    {
                        "first": "Alain",
                        "middle": [],
                        "last": "Amari",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Trouv\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Peyr\u00e9",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "The 22nd International Conference on Artificial Intelligence and Statistics",
                "volume": "",
                "issue": "",
                "pages": "2681--2690",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jean Feydy, Thibault S\u00e9journ\u00e9, Franc \u00b8ois- Xavier Vialard, Shun-ichi Amari, Alain Trouv\u00e9, and Gabriel Peyr\u00e9. Interpolating between optimal transport and mmd using sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2681-2690. PMLR, 2019.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Management of Atrial Fibrillation: Out-of-Hospital Approach",
                "authors": [
                    {
                        "first": "Edward",
                        "middle": [
                            "D"
                        ],
                        "last": "Folland",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Journal of Thrombosis and Thrombolysis",
                "volume": "7",
                "issue": "2",
                "pages": "131--135",
                "other_ids": {
                    "DOI": [
                        "10.1023/a:1008877302411"
                    ],
                    "ISSN": [
                        "0929-5305"
                    ],
                    "ISSNe": [
                        "1573-742X"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Folland, 1999]",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Real analysis: modern techniques and their applications",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gerald B Folland",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "40",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gerald B Folland. Real analysis: modern tech- niques and their applications, volume 40. John Wiley & Sons, 1999.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Review of Gao et al.",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.5194/amt-2019-67-rc1"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gao et al., 2019]",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Deep generative learning via variational gradient flow",
                "authors": [
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Yuling",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yao",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Can",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Shunkang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "2093--2101",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, and Shunkang Zhang. Deep generative learning via variational gradient flow. In ICML, pages 2093-2101. PMLR, 2019.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Figure 2: Summary graph of risk of bias of research (Gao et al., 2022; Turrado et al., 2021; Feyzio\u011flu et al., 2020; Basha et al., 2022; Mohammad & Ahmad, 2018; Villumsen et al., 2019; Zhang et al., 2022).",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.7717/peerj.18701/fig-2"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gao et al., 2022]",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Deep generative learning via euler particle transport",
                "authors": [
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuling",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "Jin",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiliang",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhijian",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Mathematical and Scientific Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "336--368",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuan Gao, Jian Huang, Yuling Jiao, Jin Liu, Xil- iang Lu, and Zhijian Yang. Deep generative learning via eu- ler particle transport. In Mathematical and Scientific Machine Learning, pages 336-368. PMLR, 2022.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Stochastic optimization for large-scale optimal transport",
                "authors": [
                    {
                        "first": "Aude",
                        "middle": [],
                        "last": "Genevay",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Cuturi",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Peyr\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Francis",
                        "middle": [],
                        "last": "Bach",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "NeurIPS",
                "volume": "29",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aude Genevay, Marco Cuturi, Gabriel Peyr\u00e9, and Francis Bach. Stochastic optimization for large-scale optimal transport. NeurIPS, 29, 2016.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Learning generative models with sinkhorn divergences",
                "authors": [
                    {
                        "first": "Aude",
                        "middle": [],
                        "last": "Genevay",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Peyr\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Cuturi",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "International Conference on Artificial Intelligence and Statistics",
                "volume": "",
                "issue": "",
                "pages": "1608--1617",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aude Genevay, Gabriel Peyr\u00e9, and Marco Cuturi. Learning generative models with sinkhorn divergences. In International Conference on Artificial Intelligence and Statis- tics, pages 1608-1617. PMLR, 2018.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Generative adversarial nets",
                "authors": [
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [],
                        "last": "Pouget-Abadie",
                        "suffix": ""
                    },
                    {
                        "first": "Mehdi",
                        "middle": [],
                        "last": "Mirza",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Warde-Farley",
                        "suffix": ""
                    },
                    {
                        "first": "Sherjil",
                        "middle": [],
                        "last": "Ozair",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "NeurIPS",
                "volume": "27",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 27, 2014.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Ffjord: Freeform continuous dynamics for scalable reversible generative models",
                "authors": [
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Grathwohl",
                        "suffix": ""
                    },
                    {
                        "first": "Ricky Tq",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Bettencourt",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Duvenaud",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.01367"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free- form continuous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367, 2018.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Deep generative wasserstein gradient flows",
                "authors": [
                    {
                        "first": "Alvin",
                        "middle": [],
                        "last": "Heng",
                        "suffix": ""
                    },
                    {
                        "first": "Abdul",
                        "middle": [],
                        "last": "Fatir Ansari",
                        "suffix": ""
                    },
                    {
                        "first": "Harold",
                        "middle": [],
                        "last": "Soh",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alvin Heng, Abdul Fatir Ansari, and Harold Soh. Deep generative wasserstein gradient flows. 2022.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
                "authors": [
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Heusel",
                        "suffix": ""
                    },
                    {
                        "first": "Hubert",
                        "middle": [],
                        "last": "Ramsauer",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Unterthiner",
                        "suffix": ""
                    },
                    {
                        "first": "Bernhard",
                        "middle": [],
                        "last": "Nessler",
                        "suffix": ""
                    },
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "NeurIPS",
                "volume": "30",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30, 2017.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Denoising diffusion probabilistic models",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "Ajay",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "Pieter",
                        "middle": [],
                        "last": "Abbeel",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "NeurIPS",
                "volume": "33",
                "issue": "",
                "pages": "6840--6851",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. De- noising diffusion probabilistic models. NeurIPS, 33:6840-6851, 2020.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "The limits of min-max optimization algorithms: Convergence to spurious non-critical sets",
                "authors": [
                    {
                        "first": "Ya-Ping",
                        "middle": [],
                        "last": "Hsieh",
                        "suffix": ""
                    },
                    {
                        "first": "Panayotis",
                        "middle": [],
                        "last": "Mertikopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "Volkan",
                        "middle": [],
                        "last": "Cevher",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "4337--4348",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization algorithms: Convergence to spurious non-critical sets. In ICML, pages 4337- 4348. PMLR, 2021.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "The variational formulation of the fokker-planck equation",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Kinderlehrer",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Otto",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "SIAM journal on mathematical analysis",
                "volume": "29",
                "issue": "1",
                "pages": "1--17",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-planck equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Iraj Kalantari. Induction over the Continuum",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kalantari",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "145--154",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kalantari, 2007] Iraj Kalantari. Induction over the Continuum, pages 145-154. Springer Netherlands, Dordrecht, 2007.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "Multi-source domain adaptation with sinkhorn barycenter",
                "authors": [
                    {
                        "first": "Tatsuya",
                        "middle": [],
                        "last": "Komatsu",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoko",
                        "middle": [],
                        "last": "Matsui",
                        "suffix": ""
                    },
                    {
                        "first": "Junbin",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "2021 29th European Signal Processing Conference (EU-SIPCO)",
                "volume": "",
                "issue": "",
                "pages": "1371--1375",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tatsuya Komatsu, Tomoko Matsui, and Jun- bin Gao. Multi-source domain adaptation with sinkhorn barycen- ter. In 2021 29th European Signal Processing Conference (EU- SIPCO), pages 1371-1375. IEEE, 2021.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Korotin",
                        "suffix": ""
                    },
                    {
                        "first": "Lingxiao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Aude",
                        "middle": [],
                        "last": "Genevay",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [
                            "M"
                        ],
                        "last": "Solomon",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Filippov",
                        "suffix": ""
                    },
                    {
                        "first": "Evgeny",
                        "middle": [],
                        "last": "Burnaev",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "NeurIPS",
                "volume": "34",
                "issue": "",
                "pages": "14593--14605",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, Alexander Filippov, and Evgeny Burnaev. Do neural optimal transport solvers work? a continu- ous wasserstein-2 benchmark. NeurIPS, 34:14593-14605, 2021.",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "Mmd gan: Towards deeper understanding of moment matching network",
                "authors": [
                    {
                        "first": "Chun-Liang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Cheng",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Barnab\u00e1s",
                        "middle": [],
                        "last": "P\u00f3czos",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "NeurIPS",
                "volume": "30",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab\u00e1s P\u00f3czos. Mmd gan: Towards deeper understanding of moment matching network. NeurIPS, 30, 2017.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Flow matching for generative modeling",
                "authors": [
                    {
                        "first": "Yaron",
                        "middle": [],
                        "last": "Lipman",
                        "suffix": ""
                    },
                    {
                        "first": "Ricky",
                        "middle": [
                            "T Q"
                        ],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Heli",
                        "middle": [],
                        "last": "Ben-Hamu",
                        "suffix": ""
                    },
                    {
                        "first": "Maximilian",
                        "middle": [],
                        "last": "Nickel",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "The Eleventh ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yaron Lipman, Ricky T. Q. Chen, Heli Ben- Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh ICLR, 2023.",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "Flow straight and fast: Learning to generate and transfer data with rectified flow",
                "authors": [
                    {
                        "first": "Xingchao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Chengyue",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "The Eleventh ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh ICLR, 2023.",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "Slicedwasserstein flows: Nonparametric generative modeling via optimal transport and diffusions",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Liutkus",
                        "suffix": ""
                    },
                    {
                        "first": "Umut",
                        "middle": [],
                        "last": "Simsekli",
                        "suffix": ""
                    },
                    {
                        "first": "Szymon",
                        "middle": [],
                        "last": "Majewski",
                        "suffix": ""
                    },
                    {
                        "first": "Alain",
                        "middle": [],
                        "last": "Durmus",
                        "suffix": ""
                    },
                    {
                        "first": "Fabian-Robert",
                        "middle": [],
                        "last": "St\u00f6ter",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "4104--4113",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert St\u00f6ter. Sliced- wasserstein flows: Nonparametric generative modeling via opti- mal transport and diffusions. In ICML, pages 4104-4113. PMLR, 2019.",
                "links": null
            },
            "BIBREF77": {
                "ref_id": "b77",
                "title": "Figure 3: Risk of bias summary (Abreu et al., 2017; Afshar et al., 2010; Ai, 2020; Bolasco et al., 2011; Cai et al., 2022; Chen, Zhao & Huang, 2019; Dai & Ma, 2021; Deng, 2011; Dong et al., 2011; Fakhrpour et al., 2020; Fang et al., 2023; Feng et al., 2020; Frih et al., 2017; Hristea et al., 2016; Jeong et al., 2019; Kozlowska et al., 2023; Leng, 2012; Li et al., 2008; Li & Feng, 2020; Liao et al., 2016; Limwannata et al., 2021; Lu, 2022; Martin-Alema\u00f1y et al., 2020, 2016, 2022; Sezer et al., 2014; Shi et al., 2021; Su et al., 2022; Sun, Sun & Yang, 2022a; Tabibi et al., 2023; Tan et al., 2015; Tayebi, Ramezani & Kashef, 2018; Vijaya et al., 2019; Wang & Liu, 2021; Wang, 2018; Wang et al., 2019; Wang, 2019; Wang et al., 2023; Wei, 2020; Wen et al., 2022; Wilund et al., 2010; Xu et al., 2022; Xu & Fang, 2016; Yan, Zhao & Peng, 2022; Yang et al., 2021; Yao et al., 2020; Yu & Cao, 2018; Zeng et al., 2020; Zhou, 2020; Zhou et al., 2016; Zhu et al., 2020).",
                "authors": [
                    {
                        "first": "Luise",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.7717/peerj.19053/fig-3"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Luise et al., 2019]",
                "links": null
            },
            "BIBREF78": {
                "ref_id": "b78",
                "title": "Sinkhorn barycenters with free support via frank-wolfe algorithm",
                "authors": [
                    {
                        "first": "Giulia",
                        "middle": [],
                        "last": "Luise",
                        "suffix": ""
                    },
                    {
                        "first": "Saverio",
                        "middle": [],
                        "last": "Salzo",
                        "suffix": ""
                    },
                    {
                        "first": "Massimiliano",
                        "middle": [],
                        "last": "Pontil",
                        "suffix": ""
                    },
                    {
                        "first": "Carlo",
                        "middle": [],
                        "last": "Ciliberto",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NeurIPS",
                "volume": "32",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Giulia Luise, Saverio Salzo, Massimiliano Pon- til, and Carlo Ciliberto. Sinkhorn barycenters with free support via frank-wolfe algorithm. NeurIPS, 32, 2019.",
                "links": null
            },
            "BIBREF80": {
                "ref_id": "b80",
                "title": "Human-level control through deep reinforcement learning",
                "authors": [
                    {
                        "first": "Volodymyr",
                        "middle": [],
                        "last": "Mnih",
                        "suffix": ""
                    },
                    {
                        "first": "Koray",
                        "middle": [],
                        "last": "Kavukcuoglu",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Silver",
                        "suffix": ""
                    },
                    {
                        "first": "Andrei",
                        "middle": [
                            "A"
                        ],
                        "last": "Rusu",
                        "suffix": ""
                    },
                    {
                        "first": "Joel",
                        "middle": [],
                        "last": "Veness",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [
                            "G"
                        ],
                        "last": "Bellemare",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Graves",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Riedmiller",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [
                            "K"
                        ],
                        "last": "Fidjeland",
                        "suffix": ""
                    },
                    {
                        "first": "Georg",
                        "middle": [],
                        "last": "Ostrovski",
                        "suffix": ""
                    },
                    {
                        "first": "Stig",
                        "middle": [],
                        "last": "Petersen",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [],
                        "last": "Beattie",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Sadik",
                        "suffix": ""
                    },
                    {
                        "first": "Ioannis",
                        "middle": [],
                        "last": "Antonoglou",
                        "suffix": ""
                    },
                    {
                        "first": "Helen",
                        "middle": [],
                        "last": "King",
                        "suffix": ""
                    },
                    {
                        "first": "Dharshan",
                        "middle": [],
                        "last": "Kumaran",
                        "suffix": ""
                    },
                    {
                        "first": "Daan",
                        "middle": [],
                        "last": "Wierstra",
                        "suffix": ""
                    },
                    {
                        "first": "Shane",
                        "middle": [],
                        "last": "Legg",
                        "suffix": ""
                    },
                    {
                        "first": "Demis",
                        "middle": [],
                        "last": "Hassabis",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Nature",
                "volume": "518",
                "issue": "7540",
                "pages": "529--533",
                "other_ids": {
                    "DOI": [
                        "10.1038/nature14236"
                    ],
                    "arXiv": [
                        "arXiv:1312.5602"
                    ],
                    "ISSN": [
                        "0028-0836"
                    ],
                    "ISSNe": [
                        "1476-4687"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learn- ing. arXiv preprint arXiv:1312.5602, 2013.",
                "links": null
            },
            "BIBREF82": {
                "ref_id": "b82",
                "title": "Large-scale wasserstein gradient flows",
                "authors": [
                    {
                        "first": "Petr",
                        "middle": [],
                        "last": "Mokrov",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Korotin",
                        "suffix": ""
                    },
                    {
                        "first": "Lingxiao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Aude",
                        "middle": [],
                        "last": "Genevay",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [
                            "M"
                        ],
                        "last": "Solomon",
                        "suffix": ""
                    },
                    {
                        "first": "Evgeny",
                        "middle": [],
                        "last": "Burnaev",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "NeurIPS",
                "volume": "34",
                "issue": "",
                "pages": "15243--15256",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Petr Mokrov, Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, and Evgeny Burnaev. Large-scale wasserstein gradient flows. NeurIPS, 34:15243- 15256, 2021.",
                "links": null
            },
            "BIBREF84": {
                "ref_id": "b84",
                "title": "Unbalanced sobolev descent",
                "authors": [
                    {
                        "first": "Youssef",
                        "middle": [],
                        "last": "Mroueh",
                        "suffix": ""
                    },
                    {
                        "first": "Mattia",
                        "middle": [],
                        "last": "Rigotti",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "NeurIPS",
                "volume": "33",
                "issue": "",
                "pages": "17034--17043",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Youssef Mroueh and Mattia Rigotti. Unbalanced sobolev descent. NeurIPS, 33:17034-17043, 2020.",
                "links": null
            },
            "BIBREF86": {
                "ref_id": "b86",
                "title": "Sobolev descent",
                "authors": [
                    {
                        "first": "Youssef",
                        "middle": [],
                        "last": "Mroueh",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Sercu",
                        "suffix": ""
                    },
                    {
                        "first": "Anant",
                        "middle": [],
                        "last": "Raj",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "The 22nd International Conference on Artificial Intelligence and Statistics",
                "volume": "",
                "issue": "",
                "pages": "2976--2985",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Youssef Mroueh, Tom Sercu, and Anant Raj. Sobolev descent. In The 22nd International Conference on Arti- ficial Intelligence and Statistics, pages 2976-2985. PMLR, 2019.",
                "links": null
            },
            "BIBREF87": {
                "ref_id": "b87",
                "title": "Review of Pai et al. (2019)",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pai",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.5194/acp-2019-331-rc2"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pai et al., 2021]",
                "links": null
            },
            "BIBREF88": {
                "ref_id": "b88",
                "title": "Fast sinkhorn filters: Using matrix scaling for non-rigid shape correspondence with functional maps",
                "authors": [
                    {
                        "first": "Gautam",
                        "middle": [],
                        "last": "Pai",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Simone",
                        "middle": [],
                        "last": "Melzi",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Wonka",
                        "suffix": ""
                    },
                    {
                        "first": "Maks",
                        "middle": [],
                        "last": "Ovsjanikov",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "384--393",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gautam Pai, Jing Ren, Simone Melzi, Peter Wonka, and Maks Ovsjanikov. Fast sinkhorn filters: Using ma- trix scaling for non-rigid shape correspondence with functional maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 384-393, 2021.",
                "links": null
            },
            "BIBREF90": {
                "ref_id": "b90",
                "title": "Sinkhorn autoencoders",
                "authors": [
                    {
                        "first": "Giorgio",
                        "middle": [],
                        "last": "Patrini",
                        "suffix": ""
                    },
                    {
                        "first": "Rianne",
                        "middle": [],
                        "last": "Van Den",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Berg",
                        "suffix": ""
                    },
                    {
                        "first": "Marcello",
                        "middle": [],
                        "last": "Forre",
                        "suffix": ""
                    },
                    {
                        "first": "Samarth",
                        "middle": [],
                        "last": "Carioni",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Bhargav",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    },
                    {
                        "first": "Frank",
                        "middle": [],
                        "last": "Genewein",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nielsen",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Uncertainty in Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "733--743",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Giorgio Patrini, Rianne Van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Un- certainty in Artificial Intelligence, pages 733-743. PMLR, 2020.",
                "links": null
            },
            "BIBREF92": {
                "ref_id": "b92",
                "title": "Computational optimal transport",
                "authors": [
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Peyr\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Cuturi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Center for Research in Economics and Statistics Working Papers",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gabriel Peyr\u00e9, Marco Cuturi, et al. Computa- tional optimal transport. Center for Research in Economics and Statistics Working Papers, (2017-86), 2017.",
                "links": null
            },
            "BIBREF94": {
                "ref_id": "b94",
                "title": "Numerical solution of stochastic differential equations with jumps in finance",
                "authors": [
                    {
                        "first": "Eckhard",
                        "middle": [],
                        "last": "Platen",
                        "suffix": ""
                    },
                    {
                        "first": "Nicola",
                        "middle": [],
                        "last": "Bruti-Liberati",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "64",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eckhard Platen and Nicola Bruti- Liberati. Numerical solution of stochastic differential equations with jumps in finance, volume 64. Springer Science & Business Media, 2010.",
                "links": null
            },
            "BIBREF95": {
                "ref_id": "b95",
                "title": "Multisample flow matching: Straightening flows with minibatch couplings",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pooladian",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2304.14772"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pooladian et al., 2023] Aram-Alexandre Pooladian, Heli Ben- Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lip- man, and Ricky Chen. Multisample flow matching: Straight- ening flows with minibatch couplings. arXiv preprint arXiv:2304.14772, 2023.",
                "links": null
            },
            "BIBREF97": {
                "ref_id": "b97",
                "title": "Improved techniques for training gans",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Salimans",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Zaremba",
                        "suffix": ""
                    },
                    {
                        "first": "Vicki",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "NeurIPS",
                "volume": "29",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29, 2016.",
                "links": null
            },
            "BIBREF99": {
                "ref_id": "b99",
                "title": "Optimal transport for applied mathematicians",
                "authors": [
                    {
                        "first": "Filippo",
                        "middle": [],
                        "last": "Santambrogio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Birk\u00e4user",
                "volume": "55",
                "issue": "58-63",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Filippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58-63):94, 2015.",
                "links": null
            },
            "BIBREF101": {
                "ref_id": "b101",
                "title": "{Euclidean, metric, and Wasserstein} gradient flows: an overview",
                "authors": [
                    {
                        "first": "Filippo",
                        "middle": [],
                        "last": "Santambrogio",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Bulletin of Mathematical Sciences",
                "volume": "7",
                "issue": "1",
                "pages": "87--154",
                "other_ids": {
                    "DOI": [
                        "10.1007/s13373-017-0101-1"
                    ],
                    "ISSN": [
                        "1664-3607"
                    ],
                    "ISSNe": [
                        "1664-3615"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. Bulletin of Math- ematical Sciences, 7:87-154, 2017.",
                "links": null
            },
            "BIBREF103": {
                "ref_id": "b103",
                "title": "On kinetic optimal probability paths for generative models",
                "authors": [
                    {
                        "first": "Neta",
                        "middle": [],
                        "last": "Shaul",
                        "suffix": ""
                    },
                    {
                        "first": "Ricky Tq",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Maximilian",
                        "middle": [],
                        "last": "Nickel",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Yaron",
                        "middle": [],
                        "last": "Lipman",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "30883--30907",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Neta Shaul, Ricky TQ Chen, Maximilian Nickel, Matthew Le, and Yaron Lipman. On kinetic optimal probability paths for generative models. In ICML, pages 30883- 30907. PMLR, 2023.",
                "links": null
            },
            "BIBREF105": {
                "ref_id": "b105",
                "title": "Sinkhorn barycenter via functional gradient descent",
                "authors": [
                    {
                        "first": "Zebang",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenfu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Alejandro",
                        "middle": [],
                        "last": "Ribeiro",
                        "suffix": ""
                    },
                    {
                        "first": "Hamed",
                        "middle": [],
                        "last": "Hassani",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "NeurIPS",
                "volume": "33",
                "issue": "",
                "pages": "986--996",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zebang Shen, Zhenfu Wang, Alejandro Ribeiro, and Hamed Hassani. Sinkhorn barycenter via functional gradient descent. NeurIPS, 33:986-996, 2020.",
                "links": null
            },
            "BIBREF107": {
                "ref_id": "b107",
                "title": "Mastering the game of go with deep neural networks and tree search",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Silver",
                        "suffix": ""
                    },
                    {
                        "first": "Aja",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [
                            "J"
                        ],
                        "last": "Maddison",
                        "suffix": ""
                    },
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Guez",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Sifre",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Van Den",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Driessche",
                        "suffix": ""
                    },
                    {
                        "first": "Ioannis",
                        "middle": [],
                        "last": "Schrittwieser",
                        "suffix": ""
                    },
                    {
                        "first": "Veda",
                        "middle": [],
                        "last": "Antonoglou",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Panneershelvam",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lanctot",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "nature",
                "volume": "529",
                "issue": "7587",
                "pages": "484--489",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural net- works and tree search. nature, 529(7587):484-489, 2016.",
                "links": null
            },
            "BIBREF109": {
                "ref_id": "b109",
                "title": "Generative modeling by estimating gradients of the data distribution",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Ermon",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NeurIPS",
                "volume": "32",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang Song and Stefano Ermon. Gener- ative modeling by estimating gradients of the data distribution. NeurIPS, 32, 2019.",
                "links": null
            },
            "BIBREF111": {
                "ref_id": "b111",
                "title": "Score-based generative modeling through stochastic differential equations",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Jascha",
                        "middle": [],
                        "last": "Sohl-Dickstein",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Abhishek",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Ermon",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Poole",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021.",
                "links": null
            },
            "BIBREF113": {
                "ref_id": "b113",
                "title": "An introduction to numerical analysis",
                "authors": [
                    {
                        "first": "Endre",
                        "middle": [],
                        "last": "S\u00fcli",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "F"
                        ],
                        "last": "Mayers",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Endre S\u00fcli and David F Mayers. An intro- duction to numerical analysis. Cambridge university press, 2003.",
                "links": null
            },
            "BIBREF115": {
                "ref_id": "b115",
                "title": "Improving and generalizing flowbased generative models with minibatch optimal transport",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Tong",
                        "suffix": ""
                    },
                    {
                        "first": "Nikolay",
                        "middle": [],
                        "last": "Malkin",
                        "suffix": ""
                    },
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Huguet",
                        "suffix": ""
                    },
                    {
                        "first": "Yanlei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jarrid",
                        "middle": [],
                        "last": "Rector-Brooks",
                        "suffix": ""
                    },
                    {
                        "first": "Fatras",
                        "middle": [],
                        "last": "Kilian",
                        "suffix": ""
                    },
                    {
                        "first": "Guy",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian FATRAS, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow- based generative models with minibatch optimal transport. In ICML Workshop on New Frontiers in Learning, Control, and Dy- namical Systems, 2023.",
                "links": null
            },
            "BIBREF117": {
                "ref_id": "b117",
                "title": "Optimal transport: old and new",
                "authors": [
                    {
                        "first": "C\u00e9dric",
                        "middle": [],
                        "last": "Villani",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "338",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C\u00e9dric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.",
                "links": null
            },
            "BIBREF119": {
                "ref_id": "b119",
                "title": "Improving mmd-gan training with repulsive loss function",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Saman",
                        "middle": [],
                        "last": "Halgamuge",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1812.09916"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wei Wang, Yuan Sun, and Saman Halgamuge. Improving mmd-gan training with repulsive loss function. arXiv preprint arXiv:1812.09916, 2018.",
                "links": null
            },
            "BIBREF121": {
                "ref_id": "b121",
                "title": "A mean-field games laboratory for generative modeling",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Benjamin",
                        "suffix": ""
                    },
                    {
                        "first": "Markos",
                        "middle": [
                            "A"
                        ],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Katsoulakis",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2304.13534"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Benjamin J Zhang and Markos A Katsoulakis. A mean-field games laboratory for generative mod- eling. arXiv preprint arXiv:2304.13534, 2023.",
                "links": null
            },
            "BIBREF122": {
                "ref_id": "b122",
                "title": "Dpvi: A dynamic-weight particle-based variational inference framework",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2112.00945"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhang et al., 2021a] Chao Zhang, Zhijian Li, Hui Qian, and Xin Du. Dpvi: A dynamic-weight particle-based variational inference framework. arXiv preprint arXiv:2112.00945, 2021.",
                "links": null
            },
            "BIBREF123": {
                "ref_id": "b123",
                "title": "Wasserstein flow meets replicator dynamics: A mean-field analysis of representation learning in actor-critic",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "NeurIPS",
                "volume": "34",
                "issue": "",
                "pages": "15993--16006",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhang et al., 2021b] Yufeng Zhang, Siyu Chen, Zhuoran Yang, Michael Jordan, and Zhaoran Wang. Wasserstein flow meets replicator dynamics: A mean-field analysis of representation learning in actor-critic. NeurIPS, 34:15993-16006, 2021.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "fig_num": "1",
                "uris": null,
                "type_str": "figure",
                "text": "Figure 1: Tajectories comparison between the Flow matching and the NSGF++ model in CIFAR-10 task.we can see NSGF++ model quickly recovers the target structure and progressively optimizes the details in subsequent steps",
                "num": null
            },
            "FIGREF1": {
                "fig_num": "1",
                "uris": null,
                "type_str": "figure",
                "text": "Velocity field matching training Input : number of time steps T , batch size n, gradient flow step size \u03b7 > 0, empirical or samplable distribution \u00b5 0 and \u00b5 * , neural network parameters \u03b8, optimizer step size \u03b3 > 0 / * Build trajectory pool * / while Building do / * Sample batches of size n i.i.d.from the datasets *",
                "num": null
            },
            "FIGREF2": {
                "fig_num": null,
                "uris": null,
                "type_str": "figure",
                "text": "Figure 2: NSGF++ framework",
                "num": null
            },
            "FIGREF3": {
                "fig_num": "34",
                "uris": null,
                "type_str": "figure",
                "text": "Figure 3: Visualization results for 2D generated paths.We show different methods that drive the particle from the prior distribution (black) to the target distribution (blue).The color change of the flow shows the different number of steps (from blue to red means from 0 to T ).",
                "num": null
            },
            "FIGREF4": {
                "fig_num": "5",
                "uris": null,
                "type_str": "figure",
                "text": "Figure 5: The inference result of our NSGF++ model.The first row shows the result after 5 NSGF steps and the second row shows the final results.",
                "num": null
            },
            "FIGREF5": {
                "fig_num": null,
                "uris": null,
                "type_str": "figure",
                "text": "Algorithm 4: NSGF++ Inference Input : number of NSGF time steps T \u2264 5, NSGF++ inference step size \u03b7, NSGF velocity field v \u03b8 , phase trainsition time predictor t \u03d5 , NSF inference step size \u03c9, NSF model u \u03b4 , prior samples X0 i \u223c \u03bc0 , ODEsolver(X, model, starttime, endtime, steps) / * NSGF phase * / for t = 0, 1, \u2022 \u2022 \u2022 T do Xt+1",
                "num": null
            },
            "FIGREF6": {
                "fig_num": "6",
                "uris": null,
                "type_str": "figure",
                "text": "Figure6: Visualization results for 2D generated paths.We show different methods that drive the particle from the prior distribution (black) to the target distribution (blue).The color change of the flow shows the different number of steps (from blue to red means from 0 to T ).We can see NSGF using fewer steps than OT-CFM refined through a sigmoid activation function for regression tasks targeting time value outputs.This architecture is tailored for processing image inputs to predict continuous time values within a specific range.The training parameters are as follows: Batch size: 128 Learning rate: 10 -4 .For sampling, a 5-step Euler integration is applied in the NSGF phase on MNIST and CIFAR-10 datasets.Training the phase transition time predictor is efficient and methodically streamlined.Utilizing a well-structured CNN as its backbone, the model reaches peak performance in merely 20 minutes, covering 40,000 iterations.This training efficiency is a significant advantage, especially for applications that demand rapid model adaptation.For the MNIST/CIFAR-10 experiments, a considerable amount of storage space is required when establishing the trajectory pool during the first phase of the algorithm.For MNIST dataset, setting the batch size to 256 and saving 1500 batches 5-step minibatch Sinkhorn gradient flow trajectories requires less than 20GB of storage space and with CIFAR-10, setting the batch size to 128 and saving 2500 batches 5-step minibatch Sinkhorn gradient flow trajectories requires about 45GB of storage space.In situations where storage space is limited, we suggest dynamically adding and removing trajectories in the trajectory pool to meet the training requirements.Identifying a more effective trade-off between training time and storage space utilization is a direction for future improvement.",
                "num": null
            },
            "FIGREF7": {
                "fig_num": "7",
                "uris": null,
                "type_str": "figure",
                "text": "Figure 7: 2-Wasserstein Distance of the generated process utilizing neural ODE-based diffusion models and NSGF.The FM/SI methods reduce noise roughly linearly, while NSGF quickly recovers the target structure and progressively optimizes the details in subsequent steps.",
                "num": null
            },
            "FIGREF8": {
                "fig_num": "89",
                "uris": null,
                "type_str": "figure",
                "text": "Figure 8: Uncurated samples on MNIST and L2-nearest neighbors from the training set (top: Samples, bottom: real)We observe that they are significantly different.Hence, our method generates really new samples and is not just reproducing the samples from the training set",
                "num": null
            },
            "TABREF1": {
                "html": null,
                "type_str": "table",
                "text": "Comparison of neural gradient-flow-based methods and neural ODE-based diffusion models over five data sets with 10/100 Euler steps.The principle of steps in JKO-flow means backward Eulerian method steps (JKO steps).",
                "num": null,
                "content": "<table coords=\"6,60.78,57.82,492.44,95.54\"><tr><td>Algorithm</td><td colspan=\"10\">2-Wasserstein distance (10 steps) 8gaussians 8gaussians-moons moons scurve checkerboard 8gaussians 8gaussians-moons moons scurve checkerboard 2-Wasserstein distance (100 steps)</td></tr><tr><td>NSGF (ours)</td><td>0.285</td><td>0.144</td><td>0.077</td><td>0.117</td><td>0.252</td><td>0.278</td><td>0.144</td><td>0.067</td><td>0.110</td><td>0.147</td></tr><tr><td>JKO-Flow</td><td>0.290</td><td>0.177</td><td>0.085</td><td>0.135</td><td>0.269</td><td>0.274</td><td>0.167</td><td>0.085</td><td>0.123</td><td>0.160</td></tr><tr><td>EPT</td><td>0.295</td><td>0.180</td><td>0.082</td><td>0.138</td><td>0.277</td><td>0.289</td><td>0.176</td><td>0.080</td><td>0.118</td><td>0.163</td></tr><tr><td>OT-CFM</td><td>0.289</td><td>0.173</td><td>0.088</td><td>0.149</td><td>0.253</td><td>0.269</td><td>0.165</td><td>0.078</td><td>0.127</td><td>0.159</td></tr><tr><td>1-RF</td><td>0.427</td><td>0.294</td><td>0.107</td><td>0.169</td><td>0.396</td><td>0.415</td><td>0.293</td><td>0.099</td><td>0.136</td><td>0.166</td></tr><tr><td>2-RF</td><td>0.428</td><td>0.311</td><td>0.125</td><td>0.171</td><td>0.421</td><td>0.430</td><td>0.311</td><td>0.121</td><td>0.136</td><td>0.170</td></tr><tr><td>3-RF</td><td>0.421</td><td>0.298</td><td>0.110</td><td>0.170</td><td>0.413</td><td>0.414</td><td>0.297</td><td>0.103</td><td>0.140</td><td>0.170</td></tr><tr><td>SI</td><td>0.435</td><td>0.324</td><td>0.134</td><td>0.187</td><td>0.427</td><td>0.411</td><td>0.294</td><td>0.096</td><td>0.139</td><td>0.166</td></tr><tr><td>FM</td><td>0.423</td><td>0.292</td><td>0.111</td><td>0.171</td><td>0.417</td><td>0.415</td><td>0.290</td><td>0.097</td><td>0.135</td><td>0.165</td></tr></table>"
            },
            "TABREF2": {
                "html": null,
                "type_str": "table",
                "text": "Comparison of Neural Wasserstein gradient flow methods and Neural ODE-based diffusion models over CIFAR-10",
                "num": null,
                "content": "<table coords=\"7,83.78,59.22,185.92,109.01\"><tr><td>Algorithm</td><td colspan=\"3\">CIFAR 10 IS(\u2191) FID(\u2193) NFE(\u2193)</td></tr><tr><td>NSGF++ (ours)</td><td>8.86</td><td>5.55</td><td>59</td></tr><tr><td>EPT[2022]</td><td>/</td><td>46.63</td><td>10k</td></tr><tr><td colspan=\"2\">JKO-Flow[2022] 7.48</td><td>23.7</td><td>&gt;150</td></tr><tr><td>DGGF[2022]</td><td>/</td><td>28.12</td><td>110</td></tr><tr><td>OT-CFM[2023]</td><td>/</td><td>11.14</td><td>100</td></tr><tr><td>FM[2023]</td><td>/</td><td>6.35</td><td>142</td></tr><tr><td>RF[2023]</td><td>9.20</td><td>4.88</td><td>100</td></tr><tr><td>SI[2023]</td><td>/</td><td>10.27</td><td>/</td></tr></table>"
            },
            "TABREF3": {
                "html": null,
                "type_str": "table",
                "text": "The final output is achieved by refining this intermediate result with the NSF model, starting Algorithm 3: NSGF++ Training Input : number of time steps T , batch size n, gradient flow step size \u03b7 > 0, empirical or samplable distribution \u00b5 0 and \u00b5 * , neural network parameters \u03b8, optimizer step size \u03b3 > 0",
                "num": null,
                "content": "<table coords=\"13,63.96,95.41,476.45,240.70\"><tr><td colspan=\"2\">/ * NSGF model</td><td/><td/><td>* /</td></tr><tr><td colspan=\"5\">/ * Build trajectory pool</td><td>* /</td></tr><tr><td colspan=\"2\">while Building do</td><td/><td/></tr><tr><td colspan=\"5\">/ * Sample batches of size n i.i.d. from the datasets</td><td>* /</td></tr><tr><td colspan=\"5\">X0 i \u223c \u00b5 0 , \u1ef8i \u223c \u00b5  *  , i = 1, 2, \u2022 \u2022 \u2022 n.</td></tr><tr><td colspan=\"3\">for t = 0, 1, \u2022 \u2022 \u2022 T do</td><td/></tr><tr><td colspan=\"2\">calculatef \u03bct,\u03bct</td><td colspan=\"3\">Xt i , f \u03bct,\u03bc  *  Xt i .</td></tr><tr><td>vF\u03f5 \u00b5t</td><td colspan=\"3\">Xt i = \u2207f \u03bct,\u03bct</td><td>Xt i -\u2207f \u03bct,\u03bc  *  Xt i .</td></tr><tr><td>Xt+1 i</td><td colspan=\"2\">= Xt i + \u03b7 vF\u03f5 t</td><td colspan=\"2\">Xt i .</td></tr><tr><td colspan=\"3\">store all Xt i , vF\u03f5 t</td><td>Xt i</td><td>pair into the pool, i = 1, 2, \u2022 \u2022 \u2022 n.</td></tr><tr><td colspan=\"5\">/ * velocity field matching</td><td>* /</td></tr><tr><td colspan=\"3\">while Not convergence do</td><td/></tr><tr><td colspan=\"5\">from trajectory pool sample pair Xt i , vF\u03f5 t</td><td>Xt i</td><td>.</td></tr><tr><td colspan=\"4\">L(\u03b8) = v \u03b8 ( Xt i , t) -vF\u03b5 \u00b5t</td><td>Xt i</td><td>2</td><td>,</td></tr><tr><td colspan=\"3\">\u03b8 \u2190 \u03b8 -\u03b3\u2207 \u03b8 L (\u03b8) .</td><td/></tr><tr><td colspan=\"5\">/ * phase trainsition time predictor</td><td>* /</td></tr><tr><td colspan=\"2\">while Training do</td><td/><td/></tr><tr><td>X0</td><td/><td/><td/></tr></table>"
            },
            "TABREF4": {
                "html": null,
                "type_str": "table",
                "text": "Comparison of NSGF++ and other methods over MNIST, The last row states statistics of the FID scores between 10k training examples and 10k test examples",
                "num": null,
                "content": "<table/>"
            }
        }
    }
}